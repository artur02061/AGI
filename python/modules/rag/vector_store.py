"""
–ö—Ä–∏—Å—Ç–∏–Ω–∞ 6.0 ‚Äî –í–µ–∫—Ç–æ—Ä–Ω–∞—è –ø–∞–º—è—Ç—å

–ò–ó–ú–ï–ù–ï–ù–ò–Ø v6.0:
- ‚úÖ PersistentClient –≤–º–µ—Å—Ç–æ in-memory Client (–¥–∞–Ω–Ω—ã–µ –ù–ï —Ç–µ—Ä—è—é—Ç—Å—è –ø—Ä–∏ —Ä–µ—Å—Ç–∞—Ä—Ç–µ!)
- ‚úÖ JSON –∫—ç—à –≤–º–µ—Å—Ç–æ pickle
- ‚úÖ Async-safe embedding —á–µ—Ä–µ–∑ ollama.AsyncClient
- ‚úÖ Graceful fallback –µ—Å–ª–∏ ChromaDB –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
- ‚úÖ –£–±—Ä–∞–Ω –¥—É–±–ª–∏–∫–∞—Ç modules/rag/memory.py
"""

import hashlib
import json
import math
import re
from collections import Counter
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional

import ollama

try:
    import orjson

    def _json_load(f):
        return orjson.loads(f.read())

    def _json_dump(obj, f):
        f.write(orjson.dumps(obj))
except ImportError:
    import json

    def _json_load(f):
        return json.load(f)

    def _json_dump(obj, f):
        json.dump(obj, f)

from utils.logging import get_logger
import config

logger = get_logger("vector_store")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#                   CHROMADB –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def _init_chromadb(persist_dir: str):
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç ChromaDB —Å PersistentClient.
    Fallback –Ω–∞ in-memory –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫.
    """
    try:
        import chromadb
        from chromadb.config import Settings

        # v6.0: PersistentClient ‚Äî –¥–∞–Ω–Ω—ã–µ –Ω–∞ –¥–∏—Å–∫–µ!
        client = chromadb.PersistentClient(
            path=persist_dir,
            settings=Settings(anonymized_telemetry=False),
        )
        logger.info(f"‚úÖ ChromaDB PersistentClient: {persist_dir}")
        return client

    except TypeError:
        # –°—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è ChromaDB –±–µ–∑ PersistentClient
        import chromadb
        from chromadb.config import Settings

        client = chromadb.Client(Settings(
            persist_directory=persist_dir,
            chroma_db_impl="duckdb+parquet",
            anonymized_telemetry=False,
        ))
        logger.warning("‚ö†Ô∏è ChromaDB —Å—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º legacy persist")
        return client

    except ImportError:
        logger.error("‚ùå ChromaDB –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω! pip install chromadb")
        return None

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ChromaDB: {e}")
        return None


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#                     VECTOR MEMORY
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class VectorMemory:
    """–í–µ–∫—Ç–æ—Ä–Ω–∞—è –ø–∞–º—è—Ç—å —Å –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ–º"""

    def __init__(self, persist_dir: str = None, shared_embedding_cache=None):
        persist_dir = persist_dir or str(config.VECTOR_DB_DIR)
        self._persist_dir = persist_dir
        Path(persist_dir).mkdir(parents=True, exist_ok=True)

        self.client = _init_chromadb(persist_dir)
        self.collection = None

        if self.client is not None:
            try:
                self.collection = self.client.get_or_create_collection(
                    name="kristina_memory",
                )
                logger.info("‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è kristina_memory –≥–æ—Ç–æ–≤–∞")
            except (KeyError, Exception) as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ ({e}), –ø–µ—Ä–µ—Å–æ–∑–¥–∞—é...")
                try:
                    import shutil
                    self.client = None
                    shutil.rmtree(persist_dir, ignore_errors=True)
                    Path(persist_dir).mkdir(parents=True, exist_ok=True)
                    self.client = _init_chromadb(persist_dir)
                    if self.client:
                        self.collection = self.client.get_or_create_collection(
                            name="kristina_memory",
                        )
                        logger.info("‚úÖ ChromaDB –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∞ —Å –Ω—É–ª—è")
                except Exception as e2:
                    logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å ChromaDB: {e2}")

        # –û–¥–∏–Ω async-–∫–ª–∏–µ–Ω—Ç –¥–ª—è –≤—Å–µ—Ö embedding-–∑–∞–ø—Ä–æ—Å–æ–≤ (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —É—Ç–µ—á–∫—É —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–æ–≤)
        self._async_client: Optional[ollama.AsyncClient] = None

        # Embedding –∫—ç—à ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–π –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω, –∏–Ω–∞—á–µ —Å–≤–æ–π
        self._shared_cache = shared_embedding_cache
        self.embedding_cache: Dict[str, List[float]] = {}
        self._cache_path = Path(config.DATA_DIR) / "embedding_cache.json"

        if self._shared_cache is None and config.EMBEDDING_CACHE_ENABLED:
            self._load_embedding_cache()

        # –°—á—ë—Ç—á–∏–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.doc_counter = 0
        if self.collection is not None:
            try:
                self.doc_counter = self.collection.count()
            except Exception:
                self.doc_counter = 0

        logger.info(f"üìä –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {self.doc_counter} | –ö—ç—à: {len(self.embedding_cache)}")

    # ‚îÄ‚îÄ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ ‚îÄ‚îÄ

    def add_dialogue(
        self,
        user_input: str,
        assistant_response: str,
        importance: int = 1,
        metadata: Optional[Dict] = None,
    ):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∏–∞–ª–æ–≥ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ø–∞–º—è—Ç—å"""
        if self.collection is None:
            logger.warning("‚ö†Ô∏è ChromaDB –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –¥–∏–∞–ª–æ–≥ –Ω–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω")
            return

        text = f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {user_input}\n–ö—Ä–∏—Å—Ç–∏–Ω–∞: {assistant_response}"
        now = datetime.now()

        meta = {
            "type": "dialogue",
            "timestamp": now.isoformat(),
            "date": now.strftime("%Y-%m-%d"),
            "month": now.strftime("%Y-%m"),
            "time": now.strftime("%H:%M"),
            "importance": importance,
            "user_input": user_input[:200],
            "response_length": len(assistant_response),
            "keywords": json.dumps(self._extract_keywords(text), ensure_ascii=False),
            "category": self._classify_category(user_input),
        }
        if metadata:
            # ChromaDB –Ω–µ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –≤–ª–æ–∂–µ–Ω–Ω—ã–µ dict ‚Äî —Ç–æ–ª—å–∫–æ str/int/float
            for k, v in metadata.items():
                if isinstance(v, (str, int, float, bool)):
                    meta[k] = v
                else:
                    meta[k] = str(v)

        embedding = self._get_embedding(text)

        doc_id = f"dialogue_{now.strftime('%Y%m%d_%H%M%S')}_{self.doc_counter}"
        self.doc_counter += 1

        try:
            self.collection.add(
                ids=[doc_id],
                embeddings=[embedding],
                documents=[text],
                metadatas=[meta],
            )
            logger.debug(f"üíæ –î–∏–∞–ª–æ–≥ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {doc_id}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞: {e}")

    # ‚îÄ‚îÄ –ü–æ–∏—Å–∫ ‚îÄ‚îÄ

    def search(
        self,
        query: str,
        n_results: int = None,
        filter_metadata: Optional[Dict] = None,
        date_range: Optional[tuple] = None,
    ) -> List[Dict]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏"""
        if self.collection is None:
            return []

        n_results = n_results or config.VECTOR_SEARCH_RESULTS

        query_embedding = self._get_embedding(query)

        where_filter = {}
        if filter_metadata:
            where_filter.update(filter_metadata)

        try:
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=min(n_results * 2, max(self.doc_counter, 1)),
                where=where_filter if where_filter else None,
            )
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []

        formatted = []
        if results["ids"] and results["ids"][0]:
            for i in range(len(results["ids"][0])):
                meta = results["metadatas"][0][i]
                result_date = meta.get("date", "")

                if date_range:
                    from_date, to_date = date_range
                    if not (from_date <= result_date <= to_date):
                        continue

                formatted.append({
                    "id": results["ids"][0][i],
                    "text": results["documents"][0][i],
                    "metadata": meta,
                    "distance": results["distances"][0][i] if "distances" in results else None,
                })

        # v7.4: Reranking —Å temporal decay + keyword overlap + importance
        formatted = self._rerank(formatted, query)

        return formatted[:n_results]

    async def search_async(
        self,
        query: str,
        n_results: int = None,
        filter_metadata: Optional[Dict] = None,
        date_range: Optional[tuple] = None,
    ) -> List[Dict]:
        """Async —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ ‚Äî –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç event loop"""
        if self.collection is None:
            return []

        n_results = n_results or config.VECTOR_SEARCH_RESULTS

        query_embedding = await self._get_embedding_async(query)

        where_filter = {}
        if filter_metadata:
            where_filter.update(filter_metadata)

        try:
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=min(n_results * 2, max(self.doc_counter, 1)),
                where=where_filter if where_filter else None,
            )
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []

        formatted = []
        if results["ids"] and results["ids"][0]:
            for i in range(len(results["ids"][0])):
                meta = results["metadatas"][0][i]
                result_date = meta.get("date", "")

                if date_range:
                    from_date, to_date = date_range
                    if not (from_date <= result_date <= to_date):
                        continue

                formatted.append({
                    "id": results["ids"][0][i],
                    "text": results["documents"][0][i],
                    "metadata": meta,
                    "distance": results["distances"][0][i] if "distances" in results else None,
                })

        # v7.4: Reranking —Å temporal decay + keyword overlap + importance
        formatted = self._rerank(formatted, query)

        return formatted[:n_results]

    async def add_dialogue_async(
        self,
        user_input: str,
        assistant_response: str,
        importance: int = 1,
        metadata: Optional[Dict] = None,
    ):
        """Async —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∏–∞–ª–æ–≥–∞ ‚Äî –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç event loop"""
        if self.collection is None:
            logger.warning("‚ö†Ô∏è ChromaDB –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –¥–∏–∞–ª–æ–≥ –Ω–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω")
            return

        text = f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {user_input}\n–ö—Ä–∏—Å—Ç–∏–Ω–∞: {assistant_response}"
        now = datetime.now()

        meta = {
            "type": "dialogue",
            "timestamp": now.isoformat(),
            "date": now.strftime("%Y-%m-%d"),
            "month": now.strftime("%Y-%m"),
            "time": now.strftime("%H:%M"),
            "importance": importance,
            "user_input": user_input[:200],
            "response_length": len(assistant_response),
            "keywords": self._extract_keywords(text),
            "category": self._classify_category(user_input),
        }
        if metadata:
            for k, v in metadata.items():
                if isinstance(v, (str, int, float, bool)):
                    meta[k] = v
                else:
                    meta[k] = str(v)

        # keywords needs to be a string for ChromaDB
        if isinstance(meta["keywords"], list):
            meta["keywords"] = json.dumps(meta["keywords"], ensure_ascii=False)

        embedding = await self._get_embedding_async(text)

        doc_id = f"dialogue_{now.strftime('%Y%m%d_%H%M%S')}_{self.doc_counter}"
        self.doc_counter += 1

        try:
            self.collection.add(
                ids=[doc_id],
                embeddings=[embedding],
                documents=[text],
                metadatas=[meta],
            )
            logger.debug(f"üíæ –î–∏–∞–ª–æ–≥ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {doc_id}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞: {e}")

    def search_by_timeframe(
        self,
        query: str,
        timeframe: str,
        n_results: int = None,
    ) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–æ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏"""
        now = datetime.now()
        timeframes = {
            "today": (now.strftime("%Y-%m-%d"), now.strftime("%Y-%m-%d")),
            "yesterday": (
                (now - timedelta(days=1)).strftime("%Y-%m-%d"),
                (now - timedelta(days=1)).strftime("%Y-%m-%d"),
            ),
            "this_week": (
                (now - timedelta(days=now.weekday())).strftime("%Y-%m-%d"),
                now.strftime("%Y-%m-%d"),
            ),
            "this_month": (
                now.replace(day=1).strftime("%Y-%m-%d"),
                now.strftime("%Y-%m-%d"),
            ),
        }
        date_range = timeframes.get(timeframe)
        return self.search(query, n_results=n_results, date_range=date_range)

    def get_recent_dialogues(self, n: int = 10) -> List[Dict]:
        """–ü–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–∏–∞–ª–æ–≥–æ–≤"""
        if self.collection is None:
            return []
        try:
            all_items = self.collection.get(
                where={"type": "dialogue"},
                include=["documents", "metadatas"],
            )
        except Exception:
            return []

        if not all_items["ids"]:
            return []

        items = []
        for i in range(len(all_items["ids"])):
            items.append({
                "id": all_items["ids"][i],
                "text": all_items["documents"][i],
                "metadata": all_items["metadatas"][i],
            })

        items.sort(key=lambda x: x["metadata"].get("timestamp", ""), reverse=True)
        return items[:n]

    # ‚îÄ‚îÄ Reranking (v7.4) ‚îÄ‚îÄ

    def _rerank(self, results: List[Dict], query: str) -> List[Dict]:
        """
        v7.4: –¢—Ä—ë—Ö—Ñ–∞–∫—Ç–æ—Ä–Ω–æ–µ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ RAG.

        –§–∏–Ω–∞–ª—å–Ω—ã–π score = w1*semantic + w2*temporal + w3*keyword + w4*importance

        1. Semantic score ‚Äî cosine distance –∏–∑ ChromaDB (–∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è)
        2. Temporal decay ‚Äî —Å–≤–µ–∂–∏–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è –ø–æ–ª—É—á–∞—é—Ç –±–æ–Ω—É—Å (–ø–æ–ª—É—Ä–∞—Å–ø–∞–¥ 7 –¥–Ω–µ–π)
        3. Keyword overlap ‚Äî —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        4. Importance ‚Äî –≤–∞–∂–Ω–æ—Å—Ç—å –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        """
        if not results:
            return results

        query_keywords = set(self._extract_keywords(query))
        now = datetime.now()

        for item in results:
            meta = item["metadata"]
            distance = item.get("distance")

            # 1. Semantic: distance ‚Üí similarity (cosine distance: 0=–∏–¥–µ–Ω—Ç–∏—á–Ω—ã, 2=–ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã)
            semantic = 1.0 - (distance / 2.0) if distance is not None else 0.5

            # 2. Temporal decay: exp(-lambda * age_days), half-life = 7 –¥–Ω–µ–π
            temporal = 0.5
            ts = meta.get("timestamp", "")
            if ts:
                try:
                    doc_time = datetime.fromisoformat(ts)
                    age_days = max((now - doc_time).total_seconds() / 86400, 0)
                    half_life = 7.0
                    temporal = math.exp(-0.693 * age_days / half_life)  # ln(2) ‚âà 0.693
                except (ValueError, TypeError):
                    pass

            # 3. Keyword overlap: Jaccard-like
            doc_keywords = set()
            kw_raw = meta.get("keywords", "")
            if isinstance(kw_raw, str):
                try:
                    doc_keywords = set(json.loads(kw_raw))
                except (ValueError, TypeError):
                    doc_keywords = set(kw_raw.split())
            elif isinstance(kw_raw, list):
                doc_keywords = set(kw_raw)

            if query_keywords and doc_keywords:
                overlap = len(query_keywords & doc_keywords)
                union = len(query_keywords | doc_keywords)
                keyword_score = overlap / union if union > 0 else 0.0
            else:
                keyword_score = 0.0

            # 4. Importance
            importance = meta.get("importance", 1)
            importance_score = min(importance / 3.0, 1.0)

            # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞
            final_score = (
                0.50 * semantic +
                0.20 * temporal +
                0.15 * keyword_score +
                0.15 * importance_score
            )

            item["_rerank_score"] = final_score

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º—É score (—É–±—ã–≤–∞–Ω–∏–µ)
        results.sort(key=lambda x: x.get("_rerank_score", 0), reverse=True)
        return results

    # ‚îÄ‚îÄ Embeddings ‚îÄ‚îÄ

    def _get_embedding(self, text: str) -> List[float]:
        """–ü–æ–ª—É—á–∞–µ—Ç embedding —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º (—á–µ—Ä–µ–∑ shared –∏–ª–∏ local cache)"""
        # –ï—Å–ª–∏ –µ—Å—Ç—å shared cache (EmbeddingCacheAdapter) ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if self._shared_cache is not None:
            cached = self._shared_cache.get(text)
            if cached is not None:
                return cached

            try:
                response = ollama.embeddings(
                    model=config.EMBEDDING_MODEL,
                    prompt=text,
                )
                embedding = response["embedding"]
                self._shared_cache.put(text, embedding)
                return embedding
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ embedding: {e}")
                return [0.0] * config.EMBEDDING_DIM

        # Fallback: –ª–æ–∫–∞–ª—å–Ω—ã–π cache
        text_hash = hashlib.md5(text.encode()).hexdigest()

        if config.EMBEDDING_CACHE_ENABLED and text_hash in self.embedding_cache:
            return self.embedding_cache[text_hash]

        try:
            response = ollama.embeddings(
                model=config.EMBEDDING_MODEL,
                prompt=text,
            )
            embedding = response["embedding"]

            if config.EMBEDDING_CACHE_ENABLED:
                self.embedding_cache[text_hash] = embedding
                if len(self.embedding_cache) % 100 == 0:
                    self._save_embedding_cache()

            return embedding

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ embedding: {e}")
            return [0.0] * config.EMBEDDING_DIM

    def _get_async_client(self) -> ollama.AsyncClient:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π AsyncClient (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —É—Ç–µ—á–∫—É —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–æ–≤)"""
        if self._async_client is None:
            self._async_client = ollama.AsyncClient()
        return self._async_client

    async def _get_embedding_async(self, text: str) -> List[float]:
        """Async embedding —á–µ—Ä–µ–∑ ollama.AsyncClient ‚Äî –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç event loop"""
        client = self._get_async_client()

        # Shared cache (Rust EmbeddingCacheAdapter)
        if self._shared_cache is not None:
            cached = self._shared_cache.get(text)
            if cached is not None:
                return cached

            try:
                response = await client.embeddings(
                    model=config.EMBEDDING_MODEL,
                    prompt=text,
                )
                embedding = response["embedding"]
                self._shared_cache.put(text, embedding)
                return embedding
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ async embedding: {e}")
                return [0.0] * config.EMBEDDING_DIM

        # Local cache fallback
        text_hash = hashlib.md5(text.encode()).hexdigest()

        if config.EMBEDDING_CACHE_ENABLED and text_hash in self.embedding_cache:
            return self.embedding_cache[text_hash]

        try:
            response = await client.embeddings(
                model=config.EMBEDDING_MODEL,
                prompt=text,
            )
            embedding = response["embedding"]

            if config.EMBEDDING_CACHE_ENABLED:
                self.embedding_cache[text_hash] = embedding
                if len(self.embedding_cache) % 100 == 0:
                    self._save_embedding_cache()

            return embedding

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ async embedding: {e}")
            return [0.0] * config.EMBEDDING_DIM

    def _load_embedding_cache(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫—ç—à (orjson ~5x –±—ã—Å—Ç—Ä–µ–µ stdlib json –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤)"""
        if self._cache_path.exists():
            try:
                with open(self._cache_path, "rb") as f:
                    self.embedding_cache = _json_load(f)
                logger.info(f"‚úÖ –ö—ç—à embeddings: {len(self.embedding_cache)} –∑–∞–ø–∏—Å–µ–π")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {e}")
                self.embedding_cache = {}

    def _save_embedding_cache(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫—ç—à (orjson ~5x –±—ã—Å—Ç—Ä–µ–µ stdlib json –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤)"""
        try:
            if len(self.embedding_cache) > config.EMBEDDING_CACHE_MAX_SIZE:
                items = list(self.embedding_cache.items())
                self.embedding_cache = dict(items[-config.EMBEDDING_CACHE_MAX_SIZE:])

            with open(self._cache_path, "wb") as f:
                _json_dump(self.embedding_cache, f)
            logger.debug(f"üíæ –ö—ç—à: {len(self.embedding_cache)} –∑–∞–ø–∏—Å–µ–π")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞: {e}")

    # ‚îÄ‚îÄ –£—Ç–∏–ª–∏—Ç—ã ‚îÄ‚îÄ

    @staticmethod
    def _extract_keywords(text: str) -> List[str]:
        stop_words = {
            "—è", "—Ç—ã", "–æ–Ω", "–æ–Ω–∞", "–º—ã", "–≤—ã", "–æ–Ω–∏",
            "–≤", "–Ω–∞", "–∏", "—Å", "–ø–æ", "–¥–ª—è", "–æ—Ç", "–∫",
            "the", "is", "are", "was", "were", "a", "an",
        }
        words = re.findall(r"\b\w+\b", text.lower())
        counter = Counter(w for w in words if len(w) > 3 and w not in stop_words)
        return [word for word, _ in counter.most_common(10)]

    @staticmethod
    def _classify_category(text: str) -> str:
        text_lower = text.lower()
        categories = {
            "code": ["–∫–æ–¥", "—Ñ—É–Ω–∫—Ü–∏—è", "–∫–ª–∞—Å—Å", "–æ—à–∏–±–∫–∞", "–ø—Ä–æ–≥—Ä–∞–º–º", "python"],
            "system": ["–∑–∞–ø—É—Å—Ç–∏", "–æ—Ç–∫—Ä–æ–π", "—Ñ–∞–π–ª", "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ", "–ø—Ä–æ—Ü–µ—Å—Å"],
            "web": ["–Ω–∞–π–¥–∏", "–ø–æ–∏—Å–∫", "–∏–Ω—Ç–µ—Ä–Ω–µ—Ç", "–Ω–æ–≤–æ—Å—Ç–∏", "–ø–æ–≥–æ–¥–∞"],
            "personal": ["–ø–æ–º–Ω–∏—à—å", "–≥–æ–≤–æ—Ä–∏–ª", "–æ–±—Å—É–∂–¥–∞–ª–∏", "–Ω–∞–ø–æ–º–Ω–∏"],
        }
        for category, keywords in categories.items():
            if any(kw in text_lower for kw in keywords):
                return category
        return "general"

    def get_stats(self) -> Dict:
        total = self.doc_counter
        return {
            "total": total,
            "dialogues": total,  # –ü–æ–∫–∞ –≤—Å—ë ‚Äî –¥–∏–∞–ª–æ–≥–∏
            "cache_size": len(self.embedding_cache),
            "persistent": self.client is not None,
        }

    async def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç async-–∫–ª–∏–µ–Ω—Ç Ollama (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç ResourceWarning)"""
        if self._async_client is not None:
            try:
                if hasattr(self._async_client, '_client') and self._async_client._client:
                    await self._async_client._client.aclose()
            except Exception:
                pass
            self._async_client = None

    def save_cache(self):
        """–Ø–≤–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—ç—à–∞ (–≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ shutdown)"""
        if config.EMBEDDING_CACHE_ENABLED:
            self._save_embedding_cache()
