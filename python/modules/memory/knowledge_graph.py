"""
–ö—Ä–∏—Å—Ç–∏–Ω–∞ 6.0 ‚Äî Knowledge Graph (–ì—Ä–∞—Ñ –ó–Ω–∞–Ω–∏–π)

–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å –Ω–∞ —Ç—Ä–æ–π–∫–∞—Ö (subject, predicate, object):
  "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å" ‚Üí "–ª—é–±–∏—Ç" ‚Üí "Python"
  "–ø—Ä–æ–µ–∫—Ç X" ‚Üí "–Ω–∞–ø–∏—Å–∞–Ω_–Ω–∞" ‚Üí "Rust"
  "–∫–æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è" ‚Üí "–∑–æ–≤—É—Ç" ‚Üí "–ú—É—Ä–∑–∏–∫"

–•—Ä–∞–Ω–µ–Ω–∏–µ: NetworkX DiGraph + JSON –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å.
–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤: LLM (gemma3:4b) –∏–ª–∏ regex-–ø–∞—Ç—Ç–µ—Ä–Ω—ã.

–ü–æ–∏—Å–∫: –ø–æ —Å—É—â–Ω–æ—Å—Ç–∏, –ø–æ –ø—Ä–µ–¥–∏–∫–∞—Ç—É, –ø–æ –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–∏ (BFS).
"""

import json
import re
from datetime import datetime, timezone
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Set
from collections import defaultdict

# ollama ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —á–µ—Ä–µ–∑ AsyncClient –≤–Ω—É—Ç—Ä–∏ _llm_extract

from utils.logging import get_logger
import config

logger = get_logger("knowledge_graph")

# NetworkX ‚Äî –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ (–¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ graph traversal)
try:
    import networkx as nx
    HAS_NETWORKX = True
except ImportError:
    HAS_NETWORKX = False
    logger.warning("‚ö†Ô∏è networkx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –≥—Ä–∞—Ñ")


class KnowledgeGraph:
    """–ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π ‚Äî —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å —á–µ—Ä–µ–∑ —Ç—Ä–æ–π–∫–∏"""

    def __init__(self):
        self._graph_file = config.config.knowledge_graph_dir / "graph.json"
        self._model = config.config.knowledge_graph_extractor_model
        self._max_nodes = config.config.knowledge_graph_max_nodes
        self._max_edges = config.config.knowledge_graph_max_edges

        # –ì—Ä–∞—Ñ
        if HAS_NETWORKX:
            self._graph = nx.DiGraph()
        else:
            self._graph = None

        # –ü—Ä–æ—Å—Ç–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ (–≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–Ω–æ, –¥–∞–∂–µ –±–µ–∑ networkx)
        self._triples: List[Dict] = []  # [{s, p, o, ts, importance}, ...]
        self._entity_index: Dict[str, Set[int]] = defaultdict(set)  # entity ‚Üí triple indices
        self._triple_keys: Set[tuple] = set()  # O(1) duplicate check: (s, p, o)

        self._load()

        logger.info(
            f"üß† Knowledge Graph: {len(self._triples)} —Ñ–∞–∫—Ç–æ–≤, "
            f"networkx={'–¥–∞' if HAS_NETWORKX else '–Ω–µ—Ç'}"
        )

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    #                    –î–û–ë–ê–í–õ–ï–ù–ò–ï –§–ê–ö–¢–û–í
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def add_triple(
        self,
        subject: str,
        predicate: str,
        obj: str,
        importance: int = 1,
        source: str = "auto",
    ):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Ç—Ä–æ–π–∫—É –≤ –≥—Ä–∞—Ñ"""
        subject = subject.strip().lower()
        predicate = predicate.strip().lower()
        obj = obj.strip().lower()

        if not subject or not predicate or not obj:
            return

        # O(1) –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–∞ —á–µ—Ä–µ–∑ set
        triple_key = (subject, predicate, obj)
        if triple_key in self._triple_keys:
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –∏ timestamp —É —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Ç—Ä–æ–π–∫–∏
            for t in self._triples:
                if t["s"] == subject and t["p"] == predicate and t["o"] == obj:
                    t["importance"] = max(t["importance"], importance)
                    t["ts"] = datetime.now(timezone.utc).isoformat()
                    break
            return

        # –õ–∏–º–∏—Ç
        if len(self._triples) >= self._max_edges:
            self._evict_least_important()

        idx = len(self._triples)
        triple = {
            "s": subject,
            "p": predicate,
            "o": obj,
            "ts": datetime.now(timezone.utc).isoformat(),
            "importance": importance,
            "source": source,
        }
        self._triples.append(triple)
        self._triple_keys.add(triple_key)

        # –ò–Ω–¥–µ–∫—Å—ã
        self._entity_index[subject].add(idx)
        self._entity_index[obj].add(idx)

        # NetworkX
        if self._graph is not None:
            self._graph.add_edge(subject, obj, predicate=predicate, importance=importance)

        logger.debug(f"‚ûï –§–∞–∫—Ç: {subject} ‚Üí {predicate} ‚Üí {obj}")

    async def extract_and_add(self, user_input: str, response: str, importance: int = 1):
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–∞–∫—Ç—ã –∏–∑ –¥–∏–∞–ª–æ–≥–∞ —á–µ—Ä–µ–∑ LLM –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –≤ –≥—Ä–∞—Ñ.
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –∫–∞–∂–¥–æ–º save_to_memory.
        """
        if not config.config.knowledge_graph_enabled:
            return

        if importance < config.config.knowledge_graph_min_importance:
            return

        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º regex-–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ (–±—ã—Å—Ç—Ä–æ, –±–µ—Å–ø–ª–∞—Ç–Ω–æ)
        regex_triples = self._regex_extract(user_input)
        for s, p, o in regex_triples:
            self.add_triple(s, p, o, importance, source="regex")

        # –ï—Å–ª–∏ –µ—Å—Ç—å —á—Ç–æ-—Ç–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–æ–µ ‚Äî LLM –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
        if len(user_input) > 30 and importance >= 2:
            llm_triples = await self._llm_extract(user_input, response)
            for s, p, o in llm_triples:
                self.add_triple(s, p, o, importance, source="llm")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    #                    –ü–û–ò–°–ö
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def query_entity(self, entity: str, max_results: int = 10) -> List[Dict]:
        """–í—Å–µ —Ñ–∞–∫—Ç—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —Å—É—â–Ω–æ—Å—Ç—å—é"""
        entity = entity.strip().lower()
        results = []

        indices = self._entity_index.get(entity, set())
        for idx in indices:
            if idx < len(self._triples):
                results.append(self._triples[idx])

        # –¢–∞–∫–∂–µ –∏—â–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        for key, idx_set in self._entity_index.items():
            if entity in key and key != entity:
                for idx in idx_set:
                    if idx < len(self._triples) and self._triples[idx] not in results:
                        results.append(self._triples[idx])

        results.sort(key=lambda x: x["importance"], reverse=True)
        return results[:max_results]

    def query_relation(self, subject: str = None, predicate: str = None, obj: str = None) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ —à–∞–±–ª–æ–Ω—É (s, p, o) ‚Äî None = –ª—é–±–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ"""
        results = []
        for t in self._triples:
            if subject and t["s"] != subject.lower():
                continue
            if predicate and t["p"] != predicate.lower():
                continue
            if obj and t["o"] != obj.lower():
                continue
            results.append(t)
        return results

    def get_context_for_query(self, query: str, max_facts: int = 5) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –≤ –ø—Ä–æ–º–ø—Ç.
        """
        query_words = set(query.lower().split())

        scored_triples = []
        for t in self._triples:
            text = f"{t['s']} {t['p']} {t['o']}"
            text_words = set(text.split())
            score = len(query_words & text_words) * t["importance"]
            if score > 0:
                scored_triples.append((score, t))

        scored_triples.sort(key=lambda x: x[0], reverse=True)

        if not scored_triples:
            return ""

        facts = []
        for _, t in scored_triples[:max_facts]:
            facts.append(f"‚Ä¢ {t['s']} {t['p']} {t['o']}")

        return "–ò–∑–≤–µ—Å—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã:\n" + "\n".join(facts)

    def get_neighbors(self, entity: str, depth: int = 1) -> Dict[str, List[Dict]]:
        """BFS –ø–æ –≥—Ä–∞—Ñ—É ‚Äî –Ω–∞—Ö–æ–¥–∏—Ç —Å–æ—Å–µ–¥–µ–π –Ω–∞ –≥–ª—É–±–∏–Ω—É depth"""
        entity = entity.strip().lower()

        if self._graph is not None and entity in self._graph:
            # NetworkX BFS
            neighbors = {}
            for d in range(1, depth + 1):
                level_nodes = set()
                try:
                    for node in nx.single_source_shortest_path_length(self._graph, entity, cutoff=d):
                        level_nodes.add(node)
                except nx.NetworkXError:
                    pass
                neighbors[f"depth_{d}"] = self.query_entity(entity)
            return neighbors

        # Fallback: –ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫
        return {"depth_1": self.query_entity(entity)}

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    #                    –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –§–ê–ö–¢–û–í
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _regex_extract(self, text: str) -> List[Tuple[str, str, str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤ regex-–ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏ (–±—ã—Å—Ç—Ä–æ)"""
        triples = []

        patterns = [
            # "–º–µ–Ω—è –∑–æ–≤—É—Ç –ò–º—è"
            (r"–º–µ–Ω—è –∑–æ–≤—É—Ç\s+(\w+)", "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å", "–∑–æ–≤—É—Ç", 1),
            # "—è —Ä–∞–±–æ—Ç–∞—é –≤ –ö–æ–º–ø–∞–Ω–∏—è" / "—è —Ä–∞–±–æ—Ç–∞—é –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–æ–º"
            (r"—è —Ä–∞–±–æ—Ç–∞—é\s+(?:–≤\s+)?(.+?)(?:\.|,|$)", "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å", "—Ä–∞–±–æ—Ç–∞–µ—Ç", 1),
            # "–º–æ–π –∫–æ—Ç/–ø—ë—Å/... –∑–æ–≤—É—Ç –ò–º—è"
            (r"–º–æ[–π—è—ë]\s+(\w+)\s+–∑–æ–≤—É—Ç\s+(\w+)", None, "–∑–æ–≤—É—Ç", None),
            # "—è –ª—é–±–ª—é X" / "—è –æ–±–æ–∂–∞—é X"
            (r"—è (?:–ª—é–±–ª—é|–æ–±–æ–∂–∞—é)\s+(.+?)(?:\.|,|!|$)", "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å", "–ª—é–±–∏—Ç", 1),
            # "—è –∂–∏–≤—É –≤ –ì–æ—Ä–æ–¥"
            (r"—è –∂–∏–≤—É\s+–≤\s+(.+?)(?:\.|,|$)", "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å", "–∂–∏–≤—ë—Ç_–≤", 1),
            # "–º–Ω–µ N –ª–µ—Ç"
            (r"–º–Ω–µ\s+(\d+)\s+(?:–ª–µ—Ç|–≥–æ–¥|–≥–æ–¥–∞)", "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å", "–≤–æ–∑—Ä–∞—Å—Ç", 1),
        ]

        for pattern_info in patterns:
            if len(pattern_info) == 4:
                pattern, subj, pred, obj_group = pattern_info
                match = re.search(pattern, text.lower())
                if match:
                    if subj is None:
                        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω —Å 2 –≥—Ä—É–ø–ø–∞–º–∏
                        if match.lastindex and match.lastindex >= 2:
                            triples.append((match.group(1), pred, match.group(2)))
                    else:
                        obj = match.group(obj_group).strip()
                        if obj:
                            triples.append((subj, pred, obj))

        return triples

    async def _llm_extract(self, user_input: str, response: str) -> List[Tuple[str, str, str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤ —á–µ—Ä–µ–∑ LLM (async)"""
        prompt = f"""–ò–∑–≤–ª–µ–∫–∏ —Ñ–∞–∫—Ç—ã –∏–∑ –¥–∏–∞–ª–æ–≥–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON –º–∞—Å—Å–∏–≤–∞ —Ç—Ä–æ–µ–∫.
–ö–∞–∂–¥–∞—è —Ç—Ä–æ–π–∫–∞: {{"s": "—Å—É–±—ä–µ–∫—Ç", "p": "–ø—Ä–µ–¥–∏–∫–∞—Ç", "o": "–æ–±—ä–µ–∫—Ç"}}

–ü—Ä–∏–º–µ—Ä—ã:
  "–Ø —Ä–∞–±–æ—Ç–∞—é –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–æ–º –≤ Google" ‚Üí [{{"s": "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å", "p": "—Ä–∞–±–æ—Ç–∞–µ—Ç", "o": "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç"}}, {{"s": "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å", "p": "—Ä–∞–±–æ—Ç–∞–µ—Ç_–≤", "o": "google"}}]
  "–ú–æ–π –ø—Ä–æ–µ–∫—Ç –Ω–∞–ø–∏—Å–∞–Ω –Ω–∞ Rust" ‚Üí [{{"s": "–ø—Ä–æ–µ–∫—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è", "p": "–Ω–∞–ø–∏—Å–∞–Ω_–Ω–∞", "o": "rust"}}]

–ï—Å–ª–∏ —Ñ–∞–∫—Ç–æ–≤ –Ω–µ—Ç ‚Äî –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤ [].

–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {user_input[:200]}
–û—Ç–≤–µ—Ç: {response[:200]}

JSON:"""

        try:
            from ollama import AsyncClient
            client = AsyncClient()
            resp = await client.chat(
                model=self._model,
                messages=[{"role": "user", "content": prompt}],
                options={"temperature": 0.0, "num_predict": 300},
            )

            text = resp["message"]["content"].strip()

            # –ü–∞—Ä—Å–∏–º JSON
            json_match = re.search(r'\[.*\]', text, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group())
                triples = []
                for item in data:
                    if isinstance(item, dict) and "s" in item and "p" in item and "o" in item:
                        triples.append((str(item["s"]), str(item["p"]), str(item["o"])))
                return triples

        except Exception as e:
            logger.debug(f"LLM extraction failed: {e}")

        return []

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    #                    –ü–ï–†–°–ò–°–¢–ï–ù–¢–ù–û–°–¢–¨
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def save(self):
        try:
            with open(self._graph_file, "w", encoding="utf-8") as f:
                json.dump(self._triples, f, ensure_ascii=False, indent=2)
            logger.debug(f"üíæ Knowledge Graph: {len(self._triples)} —Ñ–∞–∫—Ç–æ–≤")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∞: {e}")

    def _load(self):
        if not self._graph_file.exists():
            return
        try:
            with open(self._graph_file, "r", encoding="utf-8") as f:
                self._triples = json.load(f)

            # –ü–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –∏ set –∫–ª—é—á–µ–π
            self._entity_index.clear()
            self._triple_keys.clear()
            for i, t in enumerate(self._triples):
                self._entity_index[t["s"]].add(i)
                self._entity_index[t["o"]].add(i)
                self._triple_keys.add((t["s"], t["p"], t["o"]))

            # NetworkX
            if self._graph is not None:
                self._graph.clear()
                for t in self._triples:
                    self._graph.add_edge(
                        t["s"], t["o"],
                        predicate=t["p"],
                        importance=t.get("importance", 1),
                    )

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≥—Ä–∞—Ñ–∞: {e}")
            self._triples = []

    def _evict_least_important(self):
        """–£–¥–∞–ª—è–µ—Ç 10% –Ω–∞–∏–º–µ–Ω–µ–µ –≤–∞–∂–Ω—ã—Ö —Ç—Ä–æ–µ–∫"""
        count = max(1, len(self._triples) // 10)
        sorted_triples = sorted(
            enumerate(self._triples),
            key=lambda x: x[1].get("importance", 0),
        )
        indices = sorted([i for i, _ in sorted_triples[:count]], reverse=True)
        for idx in indices:
            t = self._triples.pop(idx)
            self._triple_keys.discard((t["s"], t["p"], t["o"]))

        # –ü–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
        self._entity_index.clear()
        for i, t in enumerate(self._triples):
            self._entity_index[t["s"]].add(i)
            self._entity_index[t["o"]].add(i)

    def get_stats(self) -> Dict:
        entities = set()
        for t in self._triples:
            entities.add(t["s"])
            entities.add(t["o"])

        predicates = set(t["p"] for t in self._triples)

        return {
            "triples": len(self._triples),
            "entities": len(entities),
            "predicates": len(predicates),
            "has_networkx": HAS_NETWORKX,
        }
