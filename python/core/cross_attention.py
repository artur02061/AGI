"""
–ö—Ä–∏—Å—Ç–∏–Ω–∞ 7.3 ‚Äî Cross-Attention —Å –ø–∞–º—è—Ç—å—é (Memory-Augmented Attention)

–ó–ê–ß–ï–ú:
  Self-Attention (WISH-004) —Å–º–æ—Ç—Ä–∏—Ç –≤–Ω—É—Ç—Ä–∏ —Ñ—Ä–∞–∑—ã.
  Cross-Attention –ø–æ–∑–≤–æ–ª—è–µ—Ç "—Å–º–æ—Ç—Ä–µ—Ç—å" –Ω–∞ –í–ù–ï–®–ù–Æ–Æ –ø–∞–º—è—Ç—å –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.

  Query = —Ç–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
  Key/Value = —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–∞–ø–∏—Å–∏ –∏–∑ ChromaDB

  –≠—Ç–æ RAG, –Ω–æ –í–ù–£–¢–†–ò –º–æ–¥–µ–ª–∏, –∞ –Ω–µ –∫–∞–∫ –ø–æ—Å—Ç-–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥.

–ö–ê–ö –†–ê–ë–û–¢–ê–ï–¢:
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ –¢–µ–∫—É—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)         ‚îÇ
  ‚îÇ X = [token_emb_1, token_emb_2, ...]           ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ Q = X @ Wq
                     ‚Üì
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ Cross-Attention                                ‚îÇ
  ‚îÇ                                                ‚îÇ
  ‚îÇ   Q (–∏–∑ —Ç–µ–∫—É—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)                    ‚îÇ
  ‚îÇ   K = memory_vectors @ Wk (–∏–∑ ChromaDB)        ‚îÇ
  ‚îÇ   V = memory_vectors @ Wv (–∏–∑ ChromaDB)        ‚îÇ
  ‚îÇ                                                ‚îÇ
  ‚îÇ   Attn = softmax(Q @ K.T / ‚àöd) @ V            ‚îÇ
  ‚îÇ                                                ‚îÇ
  ‚îÇ   ‚Üí –í–µ–∫—Ç–æ—Ä, –æ–±–æ–≥–∞—â—ë–Ω–Ω—ã–π –ø–∞–º—è—Ç—å—é                ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚Üì
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ Gate: Œ± * self_attn + (1-Œ±) * cross_attn       ‚îÇ
  ‚îÇ Œ± –æ–±—É—á–∞–µ–º—ã–π ‚Äî –º–æ–¥–µ–ª—å —Ä–µ—à–∞–µ—Ç —Å–∫–æ–ª—å–∫–æ "–ø–∞–º—è—Ç–∏"   ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

–ò–ù–¢–ï–ì–†–ê–¶–ò–Ø:
  - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ orchestrator –ø—Ä–∏ Tier 2-3
  - –û–±–æ–≥–∞—â–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –î–û –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞
  - –†–∞–±–æ—Ç–∞–µ—Ç —Å vector_memory (ChromaDB + bge-m3)
"""

import math
import json
import time
import sqlite3
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any

from utils.logging import get_logger
import config

logger = get_logger("cross_attention")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#               –õ–ò–ù–ï–ô–ù–ê–Ø –ê–õ–ì–ï–ë–†–ê (–º–∏–Ω–∏–º—É–º, —á–∏—Å—Ç—ã–π Python)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def _matmul_mv(M: List[List[float]], v: List[float]) -> List[float]:
    """–ú–∞—Ç—Ä–∏—Ü–∞ √ó –≤–µ–∫—Ç–æ—Ä: M[m√ón] @ v[n] ‚Üí r[m]"""
    return [sum(M[i][j] * v[j] for j in range(len(v))) for i in range(len(M))]


def _dot(a: List[float], b: List[float]) -> float:
    return sum(x * y for x, y in zip(a, b))


def _softmax(values: List[float]) -> List[float]:
    if not values:
        return []
    max_val = max(values)
    exps = [math.exp(min(v - max_val, 80)) for v in values]
    total = sum(exps) + 1e-10
    return [e / total for e in exps]


def _vec_add(a: List[float], b: List[float]) -> List[float]:
    return [x + y for x, y in zip(a, b)]


def _vec_scale(v: List[float], s: float) -> List[float]:
    return [x * s for x in v]


def _random_matrix(rows: int, cols: int) -> List[List[float]]:
    import random
    scale = math.sqrt(2.0 / (rows + cols))
    return [[random.gauss(0, scale) for _ in range(cols)] for _ in range(rows)]


def _layer_norm(x: List[float], eps: float = 1e-5) -> List[float]:
    n = len(x)
    mean = sum(x) / n
    var = sum((xi - mean) ** 2 for xi in x) / n
    inv_std = 1.0 / math.sqrt(var + eps)
    return [(xi - mean) * inv_std for xi in x]


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#               CROSS-ATTENTION HEAD
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


class CrossAttentionHead:
    """
    –û–¥–Ω–∞ –≥–æ–ª–æ–≤–∞ cross-attention:
      Q –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, K/V –∏–∑ –ø–∞–º—è—Ç–∏.

    d_model: —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    d_memory: —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø–∞–º—è—Ç–∏ (ChromaDB = 1024 –¥–ª—è bge-m3)
    d_head: —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ–µ–∫—Ü–∏–∏ (d_model // n_heads)
    """

    def __init__(self, d_model: int, d_memory: int, d_head: int):
        self.d_model = d_model
        self.d_memory = d_memory
        self.d_head = d_head

        # –ü—Ä–æ–µ–∫—Ü–∏–∏
        self.Wq = _random_matrix(d_model, d_head)    # Query: –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        self.Wk = _random_matrix(d_memory, d_head)    # Key: –∏–∑ –ø–∞–º—è—Ç–∏
        self.Wv = _random_matrix(d_memory, d_head)    # Value: –∏–∑ –ø–∞–º—è—Ç–∏

        self._scale = 1.0 / math.sqrt(d_head)

    def forward(
        self,
        query: List[float],
        memory_keys: List[List[float]],
        memory_values: List[List[float]],
    ) -> Tuple[List[float], List[float]]:
        """
        Cross-attention: –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –∫ N –∑–∞–ø–∏—Å—è–º –ø–∞–º—è—Ç–∏.

        Args:
            query: –≤–µ–∫—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞ [d_model]
            memory_keys: –º–∞—Ç—Ä–∏—Ü–∞ –∫–ª—é—á–µ–π [N √ó d_memory]
            memory_values: –º–∞—Ç—Ä–∏—Ü–∞ –∑–Ω–∞—á–µ–Ω–∏–π [N √ó d_memory]

        Returns:
            (output [d_head], attention_weights [N])
        """
        # Q = query @ Wq ‚Üí [d_head]
        q = _matmul_mv(list(zip(*self.Wq)), query) if self.Wq else query[:self.d_head]
        # –ë–æ–ª–µ–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ: q[j] = sum(query[i] * Wq[i][j])
        q = [sum(query[i] * self.Wq[i][j] for i in range(min(len(query), self.d_model)))
             for j in range(self.d_head)]

        # K = memory @ Wk ‚Üí [N √ó d_head]
        # V = memory @ Wv ‚Üí [N √ó d_head]
        n_mem = len(memory_keys)
        if n_mem == 0:
            return [0.0] * self.d_head, []

        keys = []
        vals = []
        for m_idx in range(n_mem):
            mk = memory_keys[m_idx]
            mv = memory_values[m_idx]
            # k[j] = sum(mk[i] * Wk[i][j])
            k = [sum(mk[i] * self.Wk[i][j]
                     for i in range(min(len(mk), self.d_memory)))
                 for j in range(self.d_head)]
            v = [sum(mv[i] * self.Wv[i][j]
                     for i in range(min(len(mv), self.d_memory)))
                 for j in range(self.d_head)]
            keys.append(k)
            vals.append(v)

        # Attention scores: Q ¬∑ K^T / ‚àöd
        scores = [_dot(q, k) * self._scale for k in keys]
        weights = _softmax(scores)

        # Output: weighted sum of values
        output = [0.0] * self.d_head
        for m_idx in range(n_mem):
            w = weights[m_idx]
            for j in range(self.d_head):
                output[j] += w * vals[m_idx][j]

        return output, weights


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#               MULTI-HEAD CROSS-ATTENTION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


class MultiHeadCrossAttention:
    """
    Multi-Head Cross-Attention —Å –≥–µ–π—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º.

    –ù–µ—Å–∫–æ–ª—å–∫–æ –≥–æ–ª–æ–≤ "—Å–º–æ—Ç—Ä—è—Ç" –Ω–∞ —Ä–∞–∑–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –ø–∞–º—è—Ç–∏,
    –∑–∞—Ç–µ–º gate —Ä–µ—à–∞–µ—Ç —Å–∫–æ–ª—å–∫–æ –ø–∞–º—è—Ç–∏ –ø–æ–¥–º–µ—à–∞—Ç—å.
    """

    def __init__(
        self,
        d_model: int = 128,
        d_memory: int = 1024,
        n_heads: int = 4,
    ):
        self.d_model = d_model
        self.d_memory = d_memory
        self.n_heads = n_heads
        self.d_head = d_model // n_heads

        # –ì–æ–ª–æ–≤—ã cross-attention
        self.heads = [
            CrossAttentionHead(d_model, d_memory, self.d_head)
            for _ in range(n_heads)
        ]

        # Output projection: concat(heads) ‚Üí d_model
        self.Wo = _random_matrix(d_model, d_model)

        # Gate: –æ–±—É—á–∞–µ–º—ã–π —Å–∫–∞–ª—è—Ä Œ± ‚àà [0, 1]
        # Œ± = sigmoid(gate_w ¬∑ [context; memory_attn] + gate_b)
        self.gate_w = [0.0] * (d_model * 2)  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω—É–ª—è–º–∏ ‚Üí gate ‚âà 0.5
        self.gate_b = 0.0

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self._total_queries = 0
        self._avg_gate = 0.5

    def forward(
        self,
        context_vec: List[float],
        memory_vectors: List[List[float]],
    ) -> Tuple[List[float], Dict]:
        """
        Cross-attention: –æ–±–æ–≥–∞—â–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–∞–º—è—Ç—å—é.

        Args:
            context_vec: –≤–µ–∫—Ç–æ—Ä —Ç–µ–∫—É—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ [d_model]
            memory_vectors: –∑–∞–ø–∏—Å–∏ –∏–∑ ChromaDB [N √ó d_memory]

        Returns:
            (enriched_vec [d_model], info dict)
        """
        if not memory_vectors:
            return context_vec, {"gate": 0.0, "n_memories": 0, "weights": []}

        self._total_queries += 1

        # Multi-head attention
        head_outputs = []
        all_weights = []

        for head in self.heads:
            out, weights = head.forward(
                query=context_vec,
                memory_keys=memory_vectors,
                memory_values=memory_vectors,
            )
            head_outputs.append(out)
            all_weights.append(weights)

        # Concat heads ‚Üí [d_model]
        concat = []
        for h_out in head_outputs:
            concat.extend(h_out)

        # Output projection
        attn_output = [
            sum(concat[i] * self.Wo[i][j]
                for i in range(min(len(concat), self.d_model)))
            for j in range(self.d_model)
        ]

        # Layer norm on attention output
        attn_output = _layer_norm(attn_output)

        # Gate: —Å–∫–æ–ª—å–∫–æ –ø–∞–º—è—Ç–∏ –ø–æ–¥–º–µ—à–∞—Ç—å
        gate_input = context_vec[:self.d_model] + attn_output[:self.d_model]
        gate_logit = sum(
            self.gate_w[i] * gate_input[i]
            for i in range(min(len(self.gate_w), len(gate_input)))
        ) + self.gate_b
        gate = 1.0 / (1.0 + math.exp(-max(-10, min(10, gate_logit))))  # sigmoid

        # Update running average
        self._avg_gate = 0.95 * self._avg_gate + 0.05 * gate

        # Blend: enriched = (1-gate)*context + gate*memory_attn
        enriched = [
            (1.0 - gate) * context_vec[i] + gate * attn_output[i]
            for i in range(min(len(context_vec), len(attn_output)))
        ]

        # –£—Å—Ä–µ–¥–Ω—è–µ–º –≤–µ—Å–∞ –ø–æ –≥–æ–ª–æ–≤–∞–º –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
        n_mem = len(memory_vectors)
        avg_weights = [0.0] * n_mem
        for head_w in all_weights:
            for i in range(min(len(head_w), n_mem)):
                avg_weights[i] += head_w[i] / self.n_heads

        info = {
            "gate": round(gate, 3),
            "n_memories": n_mem,
            "weights": [round(w, 3) for w in avg_weights[:10]],  # Top 10
            "avg_gate": round(self._avg_gate, 3),
        }

        return enriched, info


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#               MEMORY-AUGMENTED CONTEXT
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


class MemoryAugmentedContext:
    """
    –í—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –º–æ–¥—É–ª—å: –æ–±–æ–≥–∞—â–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –ø–∞–º—è—Ç—å—é.

    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
        mac = MemoryAugmentedContext(vector_memory, sentence_embeddings)

        # –ü—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞
        enriched = mac.enrich(
            user_input="–ö–∞–∫ —Å–æ–∑–¥–∞—Ç—å CSV-–ø–∞—Ä—Å–µ—Ä?",
            context_embedding=[...],  # –æ—Ç sentence_embeddings
        )

        if enriched:
            context_vec = enriched["context_vec"]  # –û–±–æ–≥–∞—â—ë–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
            memories = enriched["memories"]         # –ö–∞–∫–∏–µ –∑–∞–ø–∏—Å–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã
            gate = enriched["gate"]                 # –°–∫–æ–ª—å–∫–æ –ø–∞–º—è—Ç–∏ –ø–æ–¥–º–µ—à–∞–Ω–æ
    """

    def __init__(
        self,
        vector_memory=None,
        sentence_embeddings=None,
        d_model: int = 128,
        d_memory: int = 1024,
        n_heads: int = 4,
        max_memories: int = 5,
        db_path: Path = None,
    ):
        self._vector_memory = vector_memory
        self._sentence = sentence_embeddings
        self._max_memories = max_memories

        # Cross-Attention –º–æ–¥—É–ª—å
        self.cross_attention = MultiHeadCrossAttention(
            d_model=d_model,
            d_memory=d_memory,
            n_heads=n_heads,
        )

        # Persistence
        self._db_path = db_path or (config.config.data_dir / "cross_attention.db")
        self._db_path.parent.mkdir(parents=True, exist_ok=True)
        self._conn = sqlite3.connect(str(self._db_path))
        self._conn.row_factory = sqlite3.Row
        self._conn.execute("PRAGMA journal_mode=WAL")
        self._create_tables()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self._total_enrichments = 0
        self._useful_enrichments = 0  # gate > 0.3
        self._load_stats()

        logger.info(
            f"üîó CrossAttention: d_model={d_model}, d_memory={d_memory}, "
            f"heads={n_heads}, enrichments={self._total_enrichments}"
        )

    def _create_tables(self):
        self._conn.execute("""
            CREATE TABLE IF NOT EXISTS cross_attn_stats (
                key TEXT PRIMARY KEY,
                value TEXT NOT NULL
            )
        """)
        self._conn.execute("""
            CREATE TABLE IF NOT EXISTS cross_attn_log (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                query TEXT NOT NULL,
                n_memories INTEGER,
                gate REAL,
                top_memory TEXT,
                created_at REAL NOT NULL
            )
        """)
        self._conn.commit()

    def _load_stats(self):
        row = self._conn.execute(
            "SELECT value FROM cross_attn_stats WHERE key = 'total_enrichments'"
        ).fetchone()
        if row:
            self._total_enrichments = int(row["value"])
        row = self._conn.execute(
            "SELECT value FROM cross_attn_stats WHERE key = 'useful_enrichments'"
        ).fetchone()
        if row:
            self._useful_enrichments = int(row["value"])

    def _save_stats(self):
        for key, val in [
            ("total_enrichments", str(self._total_enrichments)),
            ("useful_enrichments", str(self._useful_enrichments)),
        ]:
            self._conn.execute("""
                INSERT INTO cross_attn_stats (key, value) VALUES (?, ?)
                ON CONFLICT(key) DO UPDATE SET value = ?
            """, (key, val, val))
        self._conn.commit()

    def enrich(
        self,
        user_input: str,
        context_embedding: List[float] = None,
        n_results: int = None,
    ) -> Optional[Dict]:
        """
        –û–±–æ–≥–∞—â–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –ø–∞–º—è—Ç—å—é.

        1. –ò—â–µ—Ç N –±–ª–∏–∂–∞–π—à–∏—Ö –∑–∞–ø–∏—Å–µ–π –≤ vector_memory (ChromaDB)
        2. –ü—Ä–æ–ø—É—Å–∫–∞–µ—Ç —á–µ—Ä–µ–∑ cross-attention
        3. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±–æ–≥–∞—â—ë–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä

        Returns:
            Dict —Å –ø–æ–ª—è–º–∏:
            - context_vec: –æ–±–æ–≥–∞—â—ë–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä [d_model]
            - memories: —Å–ø–∏—Å–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
            - gate: –¥–æ–ª—è –ø–∞–º—è—Ç–∏ (0=–Ω–µ—Ç, 1=—Ç–æ–ª—å–∫–æ –ø–∞–º—è—Ç—å)
            - weights: attention weights –Ω–∞ –∫–∞–∂–¥—É—é –∑–∞–ø–∏—Å—å
        """
        if not self._vector_memory:
            return None

        n = n_results or self._max_memories

        # 1. –ü–æ–ª—É—á–∞–µ–º context embedding
        if context_embedding is None and self._sentence:
            context_embedding = self._sentence.encode(user_input)

        if context_embedding is None:
            return None

        # –ü—Ä–æ–µ—Ü–∏—Ä—É–µ–º –¥–æ d_model –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        d_model = self.cross_attention.d_model
        if len(context_embedding) > d_model:
            # –ü—Ä–æ—Å—Ç–æ–π downsample: –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ d_model —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            # (–≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –Ω—É–∂–Ω–∞ –æ–±—É—á–∞–µ–º–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è)
            ctx_vec = context_embedding[:d_model]
        elif len(context_embedding) < d_model:
            ctx_vec = context_embedding + [0.0] * (d_model - len(context_embedding))
        else:
            ctx_vec = list(context_embedding)

        # 2. –ò—â–µ–º –≤ ChromaDB
        try:
            search_results = self._vector_memory.search(
                query=user_input,
                n_results=n,
            )
        except Exception as e:
            logger.debug(f"CrossAttention: memory search failed: {e}")
            return None

        if not search_results or not search_results.get("documents"):
            return None

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ–∫—Ç–æ—Ä–∞ –∑–∞–ø–∏—Å–µ–π
        memory_vectors = []
        memory_texts = []
        memory_metadatas = []

        documents = search_results.get("documents", [[]])[0]
        embeddings = search_results.get("embeddings", [[]])[0] if search_results.get("embeddings") else []
        metadatas = search_results.get("metadatas", [[]])[0]
        distances = search_results.get("distances", [[]])[0]

        for i, doc in enumerate(documents):
            if i < len(embeddings) and embeddings[i]:
                memory_vectors.append(embeddings[i])
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ ‚Äî –∫–æ–¥–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ sentence_embeddings
                if self._sentence:
                    emb = self._sentence.encode(doc)
                    if emb:
                        memory_vectors.append(emb)
                        continue
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –±–µ–∑ –≤–µ–∫—Ç–æ—Ä–∞

            memory_texts.append(doc)
            if i < len(metadatas):
                memory_metadatas.append(metadatas[i])

        if not memory_vectors:
            return None

        # 3. Cross-Attention
        enriched_vec, info = self.cross_attention.forward(
            context_vec=ctx_vec,
            memory_vectors=memory_vectors,
        )

        # 4. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self._total_enrichments += 1
        if info["gate"] > 0.3:
            self._useful_enrichments += 1

        # –õ–æ–≥–∏—Ä—É–µ–º
        top_memory = memory_texts[0][:100] if memory_texts else ""
        self._conn.execute("""
            INSERT INTO cross_attn_log (query, n_memories, gate, top_memory, created_at)
            VALUES (?, ?, ?, ?, ?)
        """, (user_input[:200], len(memory_vectors), info["gate"], top_memory, time.time()))

        if self._total_enrichments % 20 == 0:
            self._save_stats()

        logger.debug(
            f"üîó CrossAttn: gate={info['gate']:.2f}, "
            f"memories={info['n_memories']}, "
            f"top='{top_memory[:40]}...'"
        )

        return {
            "context_vec": enriched_vec,
            "memories": [
                {"text": memory_texts[i][:200] if i < len(memory_texts) else "",
                 "weight": info["weights"][i] if i < len(info["weights"]) else 0.0,
                 "distance": distances[i] if i < len(distances) else 1.0}
                for i in range(len(memory_vectors))
            ],
            "gate": info["gate"],
            "weights": info["weights"],
            "avg_gate": info["avg_gate"],
        }

    def get_stats(self) -> Dict:
        return {
            "total_enrichments": self._total_enrichments,
            "useful_enrichments": self._useful_enrichments,
            "useful_rate": round(
                self._useful_enrichments / max(self._total_enrichments, 1) * 100, 1
            ),
            "avg_gate": round(self.cross_attention._avg_gate, 3),
            "d_model": self.cross_attention.d_model,
            "d_memory": self.cross_attention.d_memory,
            "n_heads": self.cross_attention.n_heads,
        }

    def close(self):
        self._save_stats()
        self._conn.close()
