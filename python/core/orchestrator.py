"""
Orchestrator ‚Äî –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä Multi-Agent —Å–∏—Å—Ç–µ–º—ã

v7.2 –≠–í–û–õ–Æ–¶–ò–Ø –ö –ü–û–ù–ò–ú–ê–ù–ò–Æ:
- IntentRouter (Tier 1+2) –≤–º–µ—Å—Ç–æ LLM –¥–ª—è —Ä–æ—É—Ç–∏–Ω–≥–∞
- ResponseGenerator –≤–º–µ—Å—Ç–æ LLM –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –æ—Ç–≤–µ—Ç–æ–≤
- LearnedPatterns ‚Äî –∫–∞–∂–¥—ã–π LLM-–≤—ã–∑–æ–≤ –æ–±—É—á–∞–µ—Ç –ö—Ä–∏—Å—Ç–∏–Ω—É
- NeuralEngine ‚Äî Word2Vec + N-gram: –ö—Ä–∏—Å—Ç–∏–Ω–∞ —Å—Ç—Ä–æ–∏—Ç –°–í–û–ò –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
- BPE Tokenizer ‚Äî –ø–æ–¥—Å–ª–æ–≤–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (–º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞)
- SentenceEmbeddings ‚Äî –ø–æ–Ω–∏–º–∞–Ω–∏–µ –§–†–ê–ó, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤
- ActiveLearning ‚Äî —É–º–Ω–∞—è –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (–ª—É—á—à–µ —Å–ø—Ä–æ—Å–∏—Ç—å, —á–µ–º –æ—à–∏–±–∏—Ç—å—Å—è)
- KnowledgeDistillation ‚Äî —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ü–†–û–¶–ï–°–°–ê —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM
- LLM = —É—á–∏—Ç–µ–ª—å, –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –Ω–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è
"""

import asyncio
from typing import Dict, List, Any, Optional
from datetime import datetime

from core.agents.director import DirectorAgent
from core.agents.executor import ExecutorAgent
from core.agents.analyst import AnalystAgent
from core.agents.reasoner import ReasonerAgent
from core.vram_manager import VRAMManager
from core.learned_patterns import LearnedPatterns
from core.intent_router import IntentRouter
from core.response_generator import ResponseGenerator
from core.dialogue_engine import DialogueEngine
from core.bpe_tokenizer import BPETokenizer
from core.sentence_embeddings import SentenceEmbeddings
from core.active_learning import ActiveLearning
from core.knowledge_distillation import KnowledgeDistillation
from core.micro_transformer import MicroTransformer
from core.chain_of_thought import ChainOfThought
from core.self_play import SelfPlay
from core.cross_attention import MemoryAugmentedContext
from core.dialogue_memory import DialogueMemory
from core.task_planner import TaskPlanner
from core.conditional_gen import ConditionalGeneration
from core.mixture_of_experts import MixtureOfExperts
from core.code_understanding import CodeUnderstanding
from core.meta_learning import MetaLearner

from utils.logging import get_logger
import config

logger = get_logger("orchestrator")

class Orchestrator:
    """
    –û—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä ‚Äî —É–ø—Ä–∞–≤–ª—è–µ—Ç –≤—Å–µ–π Multi-Agent —Å–∏—Å—Ç–µ–º–æ–π

    v7.2: –ß–µ—Ç—ã—Ä—ë—Ö—É—Ä–æ–≤–Ω–µ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
      Tier 1: LearnedPatterns  ‚Äî –≤—ã—É—á–µ–Ω–Ω—ã–µ —É LLM –ø–∞—Ç—Ç–µ—Ä–Ω—ã (<10–º—Å)
      Tier 2: RuleEngine       ‚Äî regex –ø—Ä–∞–≤–∏–ª–∞ (<5–º—Å)
      Tier 3: KnowledgeDistillation ‚Äî —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (<50–º—Å)
      Tier 4: LLM fallback     ‚Äî director.analyze_request() (~25—Å)

    –ù–æ–≤–æ–µ –≤ v7.2:
      + BPE Tokenizer ‚Äî –ø–æ–¥—Å–ª–æ–≤–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä—É—Å—Å–∫–æ–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏
      + SentenceEmbeddings ‚Äî –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ñ—Ä–∞–∑ —Ü–µ–ª–∏–∫–æ–º (–Ω–µ –ø–æ —Å–ª–æ–≤–∞–º)
      + ActiveLearning ‚Äî –ö—Ä–∏—Å—Ç–∏–Ω–∞ —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç, –∫–æ–≥–¥–∞ –Ω–µ —É–≤–µ—Ä–µ–Ω–∞
      + KnowledgeDistillation ‚Äî –∑–∞–ø–æ–º–∏–Ω–∞–µ—Ç –ö–ê–ö –¥—É–º–∞–µ—Ç LLM, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –ß–¢–û

    –ö–∞–∂–¥—ã–π LLM-–≤—ã–∑–æ–≤ –æ–±—É—á–∞–µ—Ç –í–°–ï –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã.
    """

    def __init__(self, tools: Dict, memory, identity, vector_memory, thread_memory):
        logger.info("üß† –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Multi-Agent —Å–∏—Å—Ç–µ–º—ã...")

        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.tools = tools
        self.memory = memory
        self.identity = identity
        self.vector_memory = vector_memory
        self.thread_memory = thread_memory

        # VRAM Manager
        self.vram_manager = VRAMManager()

        # ‚îÄ‚îÄ v7.0: –°–∞–º–æ–æ–±—É—á–∞—é—â–∏–π—Å—è –º–æ–∑–≥ ‚îÄ‚îÄ
        self.learned_patterns = LearnedPatterns()
        self.intent_router = IntentRouter(
            self.learned_patterns,
            tool_names=list(tools.keys()),
            sentence_embeddings=None,  # –ü–æ–¥–∫–ª—é—á–∏–º –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ SentenceEmbeddings
        )
        self.response_generator = ResponseGenerator(self.learned_patterns)
        self.dialogue_engine = DialogueEngine()

        # ‚îÄ‚îÄ v7.2: –≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã ‚îÄ‚îÄ
        self.bpe_tokenizer = BPETokenizer()
        self.sentence_embeddings = SentenceEmbeddings(
            self.dialogue_engine.neural
        )
        self.active_learning = ActiveLearning(
            neural_engine=self.dialogue_engine.neural,
            sentence_embeddings=self.sentence_embeddings,
        )

        # v7.4: –ü–æ–¥–∫–ª—é—á–∞–µ–º sentence embeddings –∫ IntentRouter –¥–ª—è Tier 2.5
        self.intent_router._sentence_embeddings = self.sentence_embeddings
        self.knowledge_distillation = KnowledgeDistillation(
            sentence_embeddings=self.sentence_embeddings,
        )

        # ‚îÄ‚îÄ v7.2: MicroTransformer (Self-Attention) ‚îÄ‚îÄ
        self.micro_transformer = MicroTransformer(
            vocab_size=max(self.bpe_tokenizer.get_vocab_size(), 8000),
        )

        # ‚îÄ‚îÄ v7.3: Chain-of-Thought (—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–µ–∑ LLM) ‚îÄ‚îÄ
        self.chain_of_thought = ChainOfThought(
            knowledge_distillation=self.knowledge_distillation,
            sentence_embeddings=self.sentence_embeddings,
            tools=tools,
        )

        # ‚îÄ‚îÄ v7.3: Self-Play (—Å–∞–º–æ–æ—Ü–µ–Ω–∫–∞ —á–µ—Ä–µ–∑ LLM) ‚îÄ‚îÄ
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –ø–æ—Å–ª–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∞ (–Ω—É–∂–µ–Ω director –¥–ª—è LLM-–≤—ã–∑–æ–≤–æ–≤)
        self._self_play_pending = True  # –õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è

        # –ê–≥–µ–Ω—Ç—ã
        self.director = DirectorAgent(identity, tool_names=list(tools.keys()))
        self.executor = ExecutorAgent(tools)
        self.analyst = AnalystAgent(tools)
        self.reasoner = ReasonerAgent()

        # Self-Play (–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ—Å–ª–µ director)
        self.self_play = SelfPlay(
            director=self.director,
            learned_patterns=self.learned_patterns,
            neural_engine=self.dialogue_engine.neural if hasattr(self.dialogue_engine, 'neural') else None,
            knowledge_distillation=self.knowledge_distillation,
            chain_of_thought=self.chain_of_thought,
        )

        # ‚îÄ‚îÄ v7.3: Cross-Attention —Å –ø–∞–º—è—Ç—å—é (RAG –≤–Ω—É—Ç—Ä–∏ –º–æ–¥–µ–ª–∏) ‚îÄ‚îÄ
        self.memory_attention = MemoryAugmentedContext(
            vector_memory=vector_memory,
            sentence_embeddings=self.sentence_embeddings,
        )

        # ‚îÄ‚îÄ v7.5: DialogueMemory (–±–µ–∑–ª–∏–º–∏—Ç–Ω–∞—è –ø–∞–º—è—Ç—å –¥–∏–∞–ª–æ–≥–∞) ‚îÄ‚îÄ
        self.dialogue_memory = DialogueMemory(
            sentence_encoder=self.sentence_embeddings.encode,
            llm_summarizer=self._llm_summarize,
        )

        # ‚îÄ‚îÄ v7.3: Task Planner (–¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è –∑–∞–¥–∞—á) ‚îÄ‚îÄ
        self.task_planner = TaskPlanner(
            knowledge_distillation=self.knowledge_distillation,
            sentence_embeddings=self.sentence_embeddings,
        )

        # ‚îÄ‚îÄ v7.3: Conditional Generation (—É—Å–ª–æ–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è) ‚îÄ‚îÄ
        self.conditional_gen = ConditionalGeneration(
            micro_transformer=self.micro_transformer,
            bpe_tokenizer=self.bpe_tokenizer,
        )

        # ‚îÄ‚îÄ v7.3: Mixture of Experts (—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä—Ç—ã) ‚îÄ‚îÄ
        self.moe = MixtureOfExperts()

        # ‚îÄ‚îÄ v7.3: Code Understanding (–ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–¥–∞) ‚îÄ‚îÄ
        self.code_understanding = CodeUnderstanding()

        # ‚îÄ‚îÄ v7.3: Meta-Learning (–æ–±—É—á–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—é) ‚îÄ‚îÄ
        self.meta_learner = MetaLearner()

        self.agents = {
            "director": self.director,
            "executor": self.executor,
            "analyst": self.analyst,
            "reasoner": self.reasoner
        }

        # –û—Ç–º–µ—á–∞–µ–º hot-loaded –∞–≥–µ–Ω—Ç–æ–≤ –∫–∞–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö
        for agent_name in config.config.hot_loaded_agents:
            if agent_name in self.agents:
                self.agents[agent_name].is_loaded = True

        # Consciousness-–º–æ–¥—É–ª–∏ (—É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è –∏–∑ main.py)
        self.vad_emotions = None
        self.self_awareness = None
        self.metacognition = None

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_time": 0.0,
            "avg_time": 0.0,
            # v7.0: —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è
            "tier1_hits": 0,   # –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤—ã—É—á–µ–Ω–Ω—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏
            "tier2_hits": 0,   # –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ø—Ä–∞–≤–∏–ª–∞–º–∏
            "tier3_hits": 0,   # –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ LLM (–∏ –∑–∞–ø–∏—Å–∞–Ω–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è)
        }

        patterns_stats = self.learned_patterns.get_stats()
        dialogue_stats = self.dialogue_engine.get_stats()
        logger.info(f"‚úÖ Multi-Agent —Å–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ (–∞–≥–µ–Ω—Ç–æ–≤: {len(self.agents)})")
        logger.info(
            f"üß† LearnedPatterns: {patterns_stats['routing']} routing, "
            f"{patterns_stats['response']} response, {patterns_stats['slots']} slots"
        )
        logger.info(
            f"üí¨ DialogueEngine: {dialogue_stats['phrases']} —Ñ—Ä–∞–∑ "
            f"({dialogue_stats['phrases_from_llm']} –æ—Ç LLM), "
            f"{dialogue_stats['dialogues']} –¥–∏–∞–ª–æ–≥–æ–≤"
        )
        neural_stats = dialogue_stats.get("neural", {})
        if neural_stats:
            logger.info(
                f"üß† NeuralEngine: {neural_stats.get('vocabulary', 0)} —Å–ª–æ–≤, "
                f"{neural_stats.get('bigrams', 0)} –±–∏–≥—Ä–∞–º–º, "
                f"{neural_stats.get('training_steps', 0)} –æ–±—É—á–µ–Ω–∏–π"
            )
        transformer_stats = self.micro_transformer.get_stats()
        logger.info(
            f"ü§ñ MicroTransformer: {transformer_stats['params']:,} params, "
            f"{transformer_stats['training_steps']} steps"
        )
        cot_stats = self.chain_of_thought.get_stats()
        logger.info(
            f"üß† ChainOfThought: {cot_stats['total_reasonings']} —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, "
            f"{cot_stats['success_rate']}% —É—Å–ø–µ—Ö"
        )
        sp_stats = self.self_play.get_stats()
        logger.info(
            f"üéÆ SelfPlay: {sp_stats['total_evaluations']} –æ—Ü–µ–Ω–æ–∫, "
            f"avg={sp_stats['avg_score']}/10, "
            f"reinforce={sp_stats['reinforce_rate']}%"
        )
        dm_stats = self.dialogue_memory.get_stats()
        logger.info(
            f"üí¨ DialogueMemory: window={config.config.sliding_summary_window}, "
            f"max_summary={config.config.sliding_summary_max_tokens}tok"
        )
        ca_stats = self.memory_attention.get_stats()
        logger.info(
            f"üîó CrossAttention: {ca_stats['total_enrichments']} –æ–±–æ–≥–∞—â–µ–Ω–∏–π, "
            f"gate={ca_stats['avg_gate']}"
        )
        tp_stats = self.task_planner.get_stats()
        logger.info(
            f"üìã TaskPlanner: {tp_stats['total_plans']} –ø–ª–∞–Ω–æ–≤, "
            f"{tp_stats['total_tasks_completed']} –∑–∞–¥–∞—á"
        )
        cg_stats = self.conditional_gen.get_stats()
        logger.info(
            f"üé≠ ConditionalGen: {cg_stats['total_generations']} –≥–µ–Ω–µ—Ä–∞—Ü–∏–π, "
            f"{cg_stats['condition_values']} —É—Å–ª–æ–≤–∏–π"
        )
        moe_stats = self.moe.get_stats()
        logger.info(
            f"üß† MoE: {moe_stats['num_experts']} experts, "
            f"{moe_stats['total_forwards']} forwards, "
            f"balance={moe_stats['balance_loss']:.4f}"
        )
        cu_stats = self.code_understanding.get_stats()
        logger.info(
            f"üíª CodeUnderstanding: {cu_stats['total_analyses']} analyses, "
            f"{cu_stats['indexed_snippets']} indexed"
        )
        ml_stats = self.meta_learner.get_stats()
        improving = sum(1 for c in ml_stats['components'].values() if c['trend'] == 'improving')
        plateau = sum(1 for c in ml_stats['components'].values() if c['trend'] == 'plateau')
        logger.info(
            f"üß¨ MetaLearner: {ml_stats['total_meta_steps']} steps, "
            f"{improving}‚Üë {plateau}‚Üí, "
            f"quality={ml_stats['performance']['avg_quality']:.3f}"
        )
        logger.info(f"üìä VRAM: {self.vram_manager.get_stats()['vram']}")

    async def process(self, user_input: str) -> str:
        """
        –ì–ª–∞–≤–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞.

        v7.0: –¢—Ä—ë—Ö—É—Ä–æ–≤–Ω–µ–≤—ã–π —Ä–æ—É—Ç–∏–Ω–≥ —Å —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ–º.
        –ü–æ—Ä—è–¥–æ–∫: LearnedPatterns ‚Üí Rules ‚Üí LLM (fallback + –æ–±—É—á–µ–Ω–∏–µ)
        """

        start_time = datetime.now()
        self.stats["total_requests"] += 1

        logger.info(f"üéØ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {user_input[:50]}...")

        try:
            # === –®–ê–ì 1: –°–¢–†–û–ò–ú –ö–û–ù–¢–ï–ö–°–¢ ===
            context = await self._build_context(user_input)

            # === v7.3: –û–±–æ–≥–∞—â–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–∞–º—è—Ç—å—é (Cross-Attention) ===
            try:
                enrichment = self.memory_attention.enrich(user_input)
                if enrichment and enrichment["gate"] > 0.3:
                    # –ü–∞–º—è—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞ ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç
                    mem_snippets = [
                        m["text"][:100] for m in enrichment["memories"][:3]
                        if m["weight"] > 0.1
                    ]
                    if mem_snippets:
                        context += "\n[–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –ø–∞–º—è—Ç—å]: " + "; ".join(mem_snippets)
                        logger.debug(
                            f"üîó CrossAttn: gate={enrichment['gate']:.2f}, "
                            f"–¥–æ–±–∞–≤–ª–µ–Ω–æ {len(mem_snippets)} –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π"
                        )
            except Exception as e:
                logger.debug(f"CrossAttention enrichment skipped: {e}")

            # === –®–ê–ì 2: –ß–ï–¢–´–†–Å–•–£–†–û–í–ù–ï–í–´–ô –†–û–£–¢–ò–ù–ì (v7.2) ===
            route = self.intent_router.route(user_input)

            # v7.2: –û—Ü–µ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (ActiveLearning)
            assessment = self.active_learning.assess_confidence(
                user_input, route_result=route,
            )

            if route:
                # ‚îÄ‚îÄ Tier 1 –∏–ª–∏ Tier 2 —Å—Ä–∞–±–æ—Ç–∞–ª: –ë–ï–ó LLM ‚îÄ‚îÄ
                tier = "Tier 1 (learned)" if route["source"] == "learned" else "Tier 2 (rule)"
                logger.info(f"‚ö° {tier}: {route['intent']} ‚Üí {route['agent']}")

                if route["source"] == "learned":
                    self.stats["tier1_hits"] += 1
                else:
                    self.stats["tier2_hits"] += 1

                plan = {
                    "intent": route["intent"],
                    "primary_agent": route["agent"],
                    "supporting_agents": [],
                    "complexity": "simple",
                    "reasoning": f"{tier} routing",
                }

                # v7.2: ActiveLearning –º–æ–∂–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å –ø–æ–≤–µ–¥–µ–Ω–∏–µ
                if assessment["action"] == "clarify":
                    # –ö—Ä–∏—Å—Ç–∏–Ω–∞ –Ω–µ —É–≤–µ—Ä–µ–Ω–∞ ‚Äî —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏–µ
                    logger.info(f"‚ùì ActiveLearning: —É—Ç–æ—á–Ω—è—é (conf={assessment['confidence']:.2f})")
                    final_response = assessment["clarification"]
                elif assessment["action"] == "uncertain":
                    logger.info(f"‚ùì ActiveLearning: –Ω–µ —É–≤–µ—Ä–µ–Ω–∞ (conf={assessment['confidence']:.2f})")
                    final_response = assessment["uncertainty_phrase"]
                else:
                    final_response = await self._process_with_plan(
                        plan, user_input, context, route,
                    )
                    # –î–æ–±–∞–≤–ª—è–µ–º –æ–≥–æ–≤–æ—Ä–∫—É –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                    if assessment["action"] == "hedge":
                        final_response += f"\n\n{assessment['hedge_phrase']}"
            else:
                # ‚îÄ‚îÄ Tier 3: Chain-of-Thought (—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–µ–∑ LLM) ‚îÄ‚îÄ
                cot_result = self.chain_of_thought.reason(
                    user_input, context=context,
                )

                if cot_result and cot_result.overall_confidence >= 0.6:
                    # CoT —Å–ø—Ä–∞–≤–∏–ª—Å—è ‚Äî –æ—Ç–≤–µ—á–∞–µ–º –±–µ–∑ LLM!
                    logger.info(
                        f"üß† Tier 3 (CoT/{cot_result.strategy}): "
                        f"{len(cot_result.steps)} —à–∞–≥–æ–≤, "
                        f"conf={cot_result.overall_confidence:.2f}, "
                        f"{cot_result.reasoning_time_ms:.0f}ms"
                    )
                    self.stats["tier3_hits"] += 1
                    final_response = cot_result.final_answer

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç—å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º
                    plan = {
                        "intent": "cot_reasoning",
                        "primary_agent": "reasoner",
                        "supporting_agents": [],
                        "complexity": "simple",
                        "reasoning": f"Tier 3 (CoT/{cot_result.strategy})",
                    }
                    await self._save_to_memory(user_input, final_response, plan)

                    elapsed = (datetime.now() - start_time).total_seconds()
                    self.stats["successful_requests"] += 1
                    self.stats["total_time"] += elapsed
                    self.stats["avg_time"] = self.stats["total_time"] / self.stats["successful_requests"]
                    logger.info(f"‚úÖ –ó–∞–ø—Ä–æ—Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ {elapsed:.2f}s (CoT, –±–µ–∑ LLM)")
                    return final_response

                # ‚îÄ‚îÄ Tier 4: LLM fallback ‚îÄ‚îÄ
                logger.info("üß† Tier 4 (LLM): –î–∏—Ä–µ–∫—Ç–æ—Ä –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å...")
                self.stats["tier3_hits"] += 1

                plan = await self.director.analyze_request(user_input, context)
                logger.info(f"üìã –ü–ª–∞–Ω: {plan['primary_agent']} + {plan['supporting_agents']}")

                # –û–ë–£–ß–ï–ù–ò–ï: –∑–∞–ø–∏—Å—ã–≤–∞–µ–º LLM-—Ä–µ—à–µ–Ω–∏–µ –≤ LearnedPatterns
                intent = plan.get("intent", "unknown")
                if intent != "unknown" and intent != "error":
                    self.learned_patterns.learn_routing(
                        user_input=user_input,
                        intent=intent,
                        agent=plan["primary_agent"],
                        source="llm",
                    )
                    logger.info(f"üìù Learned: '{user_input[:40]}' ‚Üí {intent}")

                final_response = await self._process_with_plan(
                    plan, user_input, context, route=None,
                )

            # === –°–û–•–†–ê–ù–ï–ù–ò–ï –í –ü–ê–ú–Ø–¢–¨ ===
            await self._save_to_memory(user_input, final_response, plan)

            elapsed = (datetime.now() - start_time).total_seconds()
            self.stats["successful_requests"] += 1
            self.stats["total_time"] += elapsed
            self.stats["avg_time"] = self.stats["total_time"] / self.stats["successful_requests"]

            # MetaCognition: –∑–∞–ø–∏—Å—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
            if hasattr(self, 'metacognition') and self.metacognition:
                _agent_to_strategy = {
                    "director": "direct",
                    "executor": "tool_use",
                    "analyst": "web_search",
                    "reasoner": "delegate",
                }
                strategy = _agent_to_strategy.get(
                    plan.get("primary_agent", "director"), "direct"
                )
                self.metacognition.record_strategy_outcome(strategy, 1.0)
                confidence = self.metacognition.estimate_confidence(topic=user_input[:100])
                self.metacognition.record_outcome(confidence, True, topic=user_input[:100])

            logger.info(f"‚úÖ –ó–∞–ø—Ä–æ—Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ {elapsed:.2f}s")

            return final_response

        except Exception as e:
            self.stats["failed_requests"] += 1
            if hasattr(self, 'metacognition') and self.metacognition:
                self.metacognition.record_outcome(0.5, False, topic=user_input[:100])
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}", exc_info=True)
            return f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}"

    async def _process_with_plan(
        self,
        plan: Dict,
        user_input: str,
        context: str,
        route: Optional[Dict],
    ) -> str:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –ø–æ –≥–æ—Ç–æ–≤–æ–º—É –ø–ª–∞–Ω—É.

        v7.0: –ü—ã—Ç–∞–µ—Ç—Å—è –æ—Ç–≤–µ—Ç–∏—Ç—å –±–µ–∑ LLM (ResponseGenerator).
        –ï—Å–ª–∏ LLM –≤—Å—ë –∂–µ –Ω—É–∂–µ–Ω ‚Äî –û–ë–£–ß–ê–ï–¢ ResponseGenerator.
        """
        primary_agent = plan["primary_agent"]
        intent = plan.get("intent", "unknown")

        # === FAST PATH: director –¥–∏–∞–ª–æ–≥ (simple) ===
        if (primary_agent == "director"
                and plan.get("complexity") == "simple"
                and not plan.get("supporting_agents")):

            # v7.0: –ü—Ä–æ–±—É–µ–º DialogueEngine (–±–µ–∑ LLM)
            mood = self.identity.current_mood
            energy = self.identity.energy_level
            dialogue_response = self.dialogue_engine.generate_response(
                user_input, mood=mood, energy=energy,
            )

            if dialogue_response:
                logger.info("‚ö° DialogueEngine: –æ—Ç–≤–µ—Ç –±–µ–∑ LLM")
                return dialogue_response

            # v7.3: Conditional Generation (—Å —É—á—ë—Ç–æ–º —Å—Ç–∏–ª—è/–Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è)
            if self.micro_transformer._training_steps >= 50:
                try:
                    conditions = self.conditional_gen.detect_conditions(user_input, mood=mood)
                    cond_response = self.conditional_gen.generate(
                        prompt=user_input, conditions=conditions,
                    )
                    if cond_response and len(cond_response) >= 5:
                        logger.info(f"üé≠ ConditionalGen: {conditions} ‚Üí –æ—Ç–≤–µ—Ç –±–µ–∑ LLM")
                        return cond_response
                except Exception as e:
                    logger.debug(f"ConditionalGen failed: {e}")

                # Fallback: raw MicroTransformer (–±–µ–∑ —É—Å–ª–æ–≤–∏–π)
                try:
                    prompt_ids = self.bpe_tokenizer.encode(user_input)
                    if prompt_ids and len(prompt_ids) >= 2:
                        generated_ids = self.micro_transformer.generate(
                            prompt_ids, max_len=40, temperature=0.8,
                            top_k=30, top_p=0.9,
                        )
                        new_ids = generated_ids[len(prompt_ids):]
                        if new_ids:
                            transformer_response = self.bpe_tokenizer.decode(new_ids).strip()
                            if len(transformer_response) >= 5:
                                logger.info("ü§ñ MicroTransformer: –æ—Ç–≤–µ—Ç –±–µ–∑ LLM")
                                return transformer_response
                except Exception as e:
                    logger.debug(f"MicroTransformer generation failed: {e}")

            # Fallback: LLM
            logger.info("üß† Director (LLM): –¥–∏–∞–ª–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç")
            llm_response = await self.director.execute(
                {"type": "general", "input": user_input, "context": context},
            )

            # –û–ë–£–ß–ï–ù–ò–ï: –∑–∞–ø–∏—Å—ã–≤–∞–µ–º LLM-–æ—Ç–≤–µ—Ç –¥–ª—è DialogueEngine
            self.dialogue_engine.learn_from_dialogue(
                user_input=user_input,
                response=llm_response,
                mood=mood,
                source="llm",
            )
            logger.info("üìù DialogueEngine: learned from LLM response")

            return llm_response

        # === EXECUTOR PATH: –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏ ===
        if primary_agent == "executor":
            return await self._executor_path(plan, user_input, context, route)

        # === FULL PATH: —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏ ===
        # v7.3: TaskPlanner –¥–ª—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á
        if plan.get("complexity") == "complex":
            try:
                task_plan = self.task_planner.plan(user_input)
                plan_text = self.task_planner.format_plan(task_plan)
                logger.info(f"üìã TaskPlanner: {task_plan.total_tasks} –ø–æ–¥–∑–∞–¥–∞—á")
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–ª–∞–Ω –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM
                context += f"\n[–ü–ª–∞–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è]:\n{plan_text}"
            except Exception as e:
                logger.debug(f"TaskPlanner skipped: {e}")

        required_agents = [primary_agent] + plan.get("supporting_agents", [])
        await self.vram_manager.ensure_loaded(required_agents)

        results = await self._execute_plan(plan, user_input, context)

        logger.info("üé® –î–∏—Ä–µ–∫—Ç–æ—Ä —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç...")
        final_response = await self.director.synthesize_response(
            user_input, plan, results, context=context
        )

        return final_response

    async def _executor_path(
        self,
        plan: Dict,
        user_input: str,
        context: str,
        route: Optional[Dict],
    ) -> str:
        """
        –ë—ã—Å—Ç—Ä—ã–π –ø—É—Ç—å –¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á.

        v7.0: –ü—ã—Ç–∞–µ—Ç—Å—è –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∏ –æ—Ç–≤–µ—Ç–∏—Ç—å –ë–ï–ó LLM.
        """
        intent = plan.get("intent")

        # –°—Ç—Ä–æ–∏–º –∑–∞–¥–∞—á—É –¥–ª—è executor
        # –ò–∑–≤–ª–µ–∫–∞–µ–º args –∏–∑ route (Tier 1/2) –∏–ª–∏ –ø—É—Å—Ç–æ–π dict –¥–ª—è fallback –Ω–∞ _detect_tool_from_input
        task = {
            "tool": intent,
            "args": route.get("slots", {}) if route else {},
            "user_input": user_input,
        }

        # –í–∞–ª–∏–¥–∞—Ü–∏—è intent
        if intent and intent not in self.tools:
            logger.warning(f"‚ö†Ô∏è –ù–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç '{intent}', fallback –Ω–∞ NLU")
            task["tool"] = None

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç
        try:
            tool_result = await self.executor.execute(task)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ executor: {e}")
            tool_result = f"ERROR: {str(e)}"

        logger.info(f"‚úÖ executor: {tool_result[:100]}")

        # === –û–¢–í–ï–¢ –ë–ï–ó LLM: ResponseGenerator ===
        response = self.response_generator.generate(intent, tool_result)

        if response:
            logger.info("‚ö° –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –±–µ–∑ LLM (ResponseGenerator)")
            # –û–ë–£–ß–ï–ù–ò–ï: –∑–∞–ø–∏—Å—ã–≤–∞–µ–º slot-–ø–∞—Ç—Ç–µ—Ä–Ω—ã –µ—Å–ª–∏ route —Å—Ä–∞–±–æ—Ç–∞–ª
            if route and route.get("slots") and intent:
                self.learned_patterns.learn_slots(
                    intent, user_input, route["slots"]
                )
            return response

        # === FALLBACK: LLM —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç ===
        logger.info("üé® LLM —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç (ResponseGenerator –Ω–µ —Å–ø—Ä–∞–≤–∏–ª—Å—è)...")
        results = {"executor": tool_result}
        final_response = await self.director.synthesize_response(
            user_input, plan, results, context=context
        )

        # –û–ë–£–ß–ï–ù–ò–ï: –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –æ—Ç–≤–µ—Ç LLM –∫–∞–∫ —à–∞–±–ª–æ–Ω
        if intent and not tool_result.startswith("ERROR"):
            self.learned_patterns.learn_response(
                intent=intent,
                tool_result=tool_result,
                final_response=final_response,
            )
            logger.info(f"üìù Learned response: {intent}")

        return final_response

    async def _execute_plan(self, plan: Dict, user_input: str, context: str = "") -> Dict[str, str]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–ª–∞–Ω, –ø–µ—Ä–µ–¥–∞–≤–∞—è –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–∞–º—è—Ç–∏ –∞–≥–µ–Ω—Ç–∞–º."""

        primary_agent = plan["primary_agent"]
        supporting_agents = plan.get("supporting_agents", [])

        results = {}

        # === –û–°–ù–û–í–ù–û–ô –ê–ì–ï–ù–¢ ===
        logger.info(f"‚ö° –û—Å–Ω–æ–≤–Ω–æ–π –∞–≥–µ–Ω—Ç: {primary_agent}")

        primary_task = self._build_task(primary_agent, plan, user_input, context)

        try:
            primary_result = await self.agents[primary_agent].execute(primary_task)
            results[primary_agent] = primary_result
            logger.info(f"‚úÖ {primary_agent}: {primary_result[:100]}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ {primary_agent}: {e}")
            results[primary_agent] = f"ERROR: {str(e)}"

        # === –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –ê–ì–ï–ù–¢–´ (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ) ===
        if supporting_agents:
            logger.info(f"üîÑ –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ: {supporting_agents}")

            tasks = []
            valid_agents = []
            for agent_name in supporting_agents:
                if agent_name in self.agents:
                    task = self._build_task(agent_name, plan, user_input, context)
                    tasks.append(self._execute_agent(agent_name, task))
                    valid_agents.append(agent_name)

            if tasks:
                supporting_results = await asyncio.gather(*tasks, return_exceptions=True)

                for i, agent_name in enumerate(valid_agents):
                    result = supporting_results[i]
                    if isinstance(result, Exception):
                        results[agent_name] = f"ERROR: {str(result)}"
                    else:
                        results[agent_name] = result

        return results

    async def _execute_agent(self, agent_name: str, task: Dict) -> str:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–¥–∞—á—É —á–µ—Ä–µ–∑ –∞–≥–µ–Ω—Ç–∞"""

        agent = self.agents.get(agent_name)

        if not agent:
            return f"ERROR: –ê–≥–µ–Ω—Ç {agent_name} –Ω–µ –Ω–∞–π–¥–µ–Ω"

        try:
            result = await agent.execute(task)
            return result

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è {agent_name}: {e}")
            return f"ERROR: {str(e)}"

    def _build_task(self, agent_name: str, plan: Dict, user_input: str, context: str = "") -> Dict[str, Any]:
        """–°—Ç—Ä–æ–∏—Ç –∑–∞–¥–∞—á—É –¥–ª—è –∞–≥–µ–Ω—Ç–∞, –≤–∫–ª—é—á–∞—è –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–∞–º—è—Ç–∏."""

        # Executor
        if agent_name == "executor":
            intent = plan.get("intent")
            if intent and intent not in self.tools:
                logger.warning(
                    f"‚ö†Ô∏è –î–∏—Ä–µ–∫—Ç–æ—Ä –ø—Ä–µ–¥–ª–æ–∂–∏–ª –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç '{intent}', "
                    f"executor –æ–ø—Ä–µ–¥–µ–ª–∏—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞"
                )
                intent = None
            return {
                "tool": intent,
                "args": [],
                "user_input": user_input,
            }

        # Analyst
        elif agent_name == "analyst":
            task_type = "web_search"
            if "–∞–Ω–∞–ª–∏–∑" in user_input.lower():
                task_type = "data_analysis"

            return {
                "type": task_type,
                "query": user_input,
                "max_results": 3,
            }

        # Reasoner
        elif agent_name == "reasoner":
            task_type = "general"
            if any(w in user_input.lower() for w in ["—Ä–µ—à–∏", "–≤—ã—á–∏—Å–ª–∏", "–ø–æ—Å—á–∏—Ç–∞–π"]):
                task_type = "math"
            elif "–ª–æ–≥–∏–∫–∞" in user_input.lower() or "–¥–æ–∫–∞–∂–∏" in user_input.lower():
                task_type = "logic"

            return {
                "type": task_type,
                "problem": user_input,
            }

        # Director
        elif agent_name == "director":
            return {
                "type": "general",
                "input": user_input,
                "context": context,
            }

        return {
            "type": "general",
            "input": user_input,
            "context": context,
        }

    async def _llm_summarize(self, prompt: str) -> str:
        """–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ LLM (–¥–ª—è DialogueMemory)"""
        try:
            from ollama import AsyncClient
            client = AsyncClient(host=config.config.ollama_hosts.cpu)
            response = await client.generate(
                model=config.config.memory_summarizer_model,
                prompt=prompt,
                options={"temperature": 0.1, "num_predict": 300},
            )
            return response.get("response", "")
        except Exception as e:
            logger.debug(f"LLM summarize failed: {e}")
            return ""

    async def _build_context(self, user_input: str) -> str:
        """
        –°—Ç—Ä–æ–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–∞–º—è—Ç–∏.

        v7.5: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç DialogueMemory –¥–ª—è –±–µ–∑–ª–∏–º–∏—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å–µ—Å—Å–∏–∏.

        –ë—é–¥–∂–µ—Ç ~2000 —Ç–æ–∫–µ–Ω–æ–≤:
          - DialogueMemory (—Ä–µ–∑—é–º–µ + –ø–æ–∏—Å–∫ + recent): ~1800 —Ç–æ–∫–µ–Ω–æ–≤
          - –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å (ChromaDB): ~300 —Ç–æ–∫–µ–Ω–æ–≤
          - Code / MoE: ~200 —Ç–æ–∫–µ–Ω–æ–≤
        """

        # 1. DialogueMemory: —Ä–µ–∑—é–º–µ —Å–µ—Å—Å–∏–∏ + –ø–æ–∏—Å–∫ + –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        dialogue_context = await self.dialogue_memory.build_context(user_input)

        # 2. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å (short-term)
        relevant_memory = self.memory.get_relevant_context(user_input, max_items=3)

        # 3. –í–µ–∫—Ç–æ—Ä–Ω–∞—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å (async)
        vector_context = ""
        try:
            vector_results = await self.vector_memory.search_async(user_input, n_results=3)
            if vector_results:
                vector_parts = []
                for r in vector_results[:3]:
                    date = r['metadata'].get('date', '')
                    text = r['text'][:120]
                    vector_parts.append(f"  [{date}] {text}")
                vector_context = "\n[–î–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø–∞–º—è—Ç—å]:\n" + "\n".join(vector_parts)
        except Exception:
            pass

        # 4. Code Understanding: –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–∏—Å–ª–∞–ª –∫–æ–¥
        code_context = ""
        try:
            import re as _re
            code_match = _re.search(r'```(?:python)?\s*\n(.+?)```', user_input, _re.DOTALL)
            if code_match:
                code_snippet = code_match.group(1)
                analysis = self.code_understanding.analyze_code(code_snippet)
                if analysis and analysis.summary:
                    code_context = f"\n[–ê–Ω–∞–ª–∏–∑ –∫–æ–¥–∞]: {analysis.summary}"
                    if analysis.patterns:
                        warnings = [p.message for p in analysis.patterns[:3]]
                        code_context += "\n  –ó–∞–º–µ—á–∞–Ω–∏—è: " + "; ".join(warnings)
        except Exception:
            pass

        # 5. MoE routing: –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â–µ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–∞
        moe_context = ""
        try:
            input_emb = self.sentence_embeddings.encode(user_input)
            if input_emb:
                from core.mixture_of_experts import D_MODEL as MOE_D
                in_vec = (input_emb[:MOE_D] + [0.0] * MOE_D)[:MOE_D]
                expert_name = self.moe.get_expert_for_text(user_input, in_vec)
                moe_context = f"\n–î–æ–º–∏–Ω–∏—Ä—É—é—â–∏–π —ç–∫—Å–ø–µ—Ä—Ç: {expert_name}"
        except Exception:
            pass

        context = f"""–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{dialogue_context}
{relevant_memory}
{vector_context}
{code_context}
{moe_context}"""

        return context

    async def _save_to_memory(self, user_input: str, response: str, plan: Dict):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∏–∞–ª–æ–≥ –≤ –ø–∞–º—è—Ç—å + –æ–±—É—á–∞–µ—Ç v7.2 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã"""

        try:
            # v7.5: –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ DialogueMemory (–±–µ–∑–ª–∏–º–∏—Ç–Ω–∞—è –ø–∞–º—è—Ç—å —Å–µ—Å—Å–∏–∏)
            self.dialogue_memory.add('user', user_input)
            self.dialogue_memory.add('assistant', response)
            await self.dialogue_memory.maybe_compress()

            self.memory.add_to_working("user", user_input)
            self.memory.add_to_working("assistant", response)

            importance = 2 if plan.get("complexity") == "complex" else 1
            self.memory.add_episode(
                user_input,
                response,
                self.identity.current_mood,
                importance
            )

            await self.vector_memory.add_dialogue_async(
                user_input,
                response,
                importance=importance
            )

            self.thread_memory.update(user_input, response)

            # ‚îÄ‚îÄ v7.2: –û–±—É—á–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ ‚îÄ‚îÄ

            # BPE Tokenizer: —É—á–∏—Ç—Å—è –Ω–∞ –∫–∞–∂–¥–æ–º —Ç–µ–∫—Å—Ç–µ
            self.bpe_tokenizer.train_on_text(user_input)
            self.bpe_tokenizer.train_on_text(response)

            # SentenceEmbeddings: –æ–±–Ω–æ–≤–ª—è–µ—Ç IDF —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.sentence_embeddings.learn_from_text(user_input)
            self.sentence_embeddings.learn_from_text(response)

            # MicroTransformer: –¥–æ–æ–±—É—á–µ–Ω–∏–µ (–º–µ—Ç–∞-—É–ø—Ä–∞–≤–ª—è–µ–º–æ–µ)
            if self.meta_learner.should_train("micro_transformer"):
                try:
                    user_tokens = self.bpe_tokenizer.encode(user_input)
                    resp_tokens = self.bpe_tokenizer.encode(response)
                    if len(user_tokens) >= 3 and len(resp_tokens) >= 3:
                        combined = user_tokens + [4] + resp_tokens + [3]
                        loss = self.micro_transformer.train_step(combined)
                        if isinstance(loss, (int, float)):
                            self.meta_learner.report_loss("micro_transformer", loss)
                except Exception as e:
                    logger.debug(f"MicroTransformer training error: {e}")

            # ConditionalGen: –æ–±—É—á–∞–µ–º —Å —É—Å–ª–æ–≤–∏—è–º–∏ (–º–µ—Ç–∞-—É–ø—Ä–∞–≤–ª—è–µ–º–æ–µ)
            if self.meta_learner.should_train("conditional_gen"):
                try:
                    conditions = self.conditional_gen.detect_conditions(user_input)
                    self.conditional_gen.train(response, conditions)
                except Exception as e:
                    logger.debug(f"ConditionalGen training error: {e}")

            # MoE: –æ–±—É—á–∞–µ–º —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (–º–µ—Ç–∞-—É–ø—Ä–∞–≤–ª—è–µ–º–æ–µ)
            if self.meta_learner.should_train("moe"):
                try:
                    input_emb = self.sentence_embeddings.encode(user_input)
                    resp_emb = self.sentence_embeddings.encode(response[:200])
                    if input_emb and resp_emb:
                        from core.mixture_of_experts import D_MODEL as MOE_D
                        in_vec = (input_emb[:MOE_D] + [0.0] * MOE_D)[:MOE_D]
                        tgt_vec = (resp_emb[:MOE_D] + [0.0] * MOE_D)[:MOE_D]
                        loss = self.moe.train_step(in_vec, tgt_vec)
                        self.meta_learner.report_loss("moe", loss)
                except Exception as e:
                    logger.debug(f"MoE training error: {e}")

            # v7.4: –û–±—É—á–∞–µ–º EmbeddingClassifier (Tier 2.5) –Ω–∞ –∫–∞–∂–¥–æ–º —Ä–æ—É—Ç–∏–Ω–≥–µ
            intent = plan.get("intent", "unknown")
            primary_agent = plan.get("primary_agent", "director")
            if intent != "unknown" and intent != "error":
                self.intent_router.learn_from_route(user_input, intent, primary_agent)

            # KnowledgeDistillation: –¥–∏—Å—Ç–∏–ª–ª–∏—Ä—É–µ—Ç LLM-–æ—Ç–≤–µ—Ç—ã
            intent = plan.get("intent", "unknown")
            reasoning = plan.get("reasoning", "")
            is_llm_response = reasoning.startswith("Tier 3") or \
                              reasoning.startswith("Tier 4") or \
                              "LLM" in reasoning
            if is_llm_response and intent != "unknown":
                self.knowledge_distillation.distill(
                    user_input=user_input,
                    llm_response=response,
                    intent=intent,
                    result_success=True,
                )

            # Self-Play: –±–∞—Ç—á–µ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –æ—Ç–≤–µ—Ç–æ–≤ Tier 1-3 (–±–µ–∑ LLM)
            is_own_response = reasoning.startswith("Tier 1") or \
                              reasoning.startswith("Tier 2") or \
                              reasoning.startswith("Tier 3 (CoT")
            if is_own_response:
                tier = "tier1" if "Tier 1" in reasoning else \
                       "tier2" if "Tier 2" in reasoning else "tier3"
                self.self_play.add_to_batch(user_input, response, source_tier=tier)

                # –ï—Å–ª–∏ –±—É—Ñ–µ—Ä –∑–∞–ø–æ–ª–Ω–∏–ª—Å—è ‚Äî –∑–∞–ø—É—Å–∫–∞–µ–º –±–∞—Ç—á–µ–≤—É—é –æ—Ü–µ–Ω–∫—É
                if self.self_play.batch_ready:
                    try:
                        await self.self_play.evaluate_batch()
                    except Exception as sp_err:
                        logger.debug(f"SelfPlay batch eval deferred: {sp_err}")

            # Meta-Learning: —Å–æ–æ–±—â–∞–µ–º –æ –∫–∞—á–µ—Å—Ç–≤–µ –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º
            try:
                tier = "tier1" if "Tier 1" in reasoning else \
                       "tier2" if "Tier 2" in reasoning else \
                       "tier3" if "Tier 3" in reasoning else "tier4"
                # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞ + –Ω–∞–ª–∏—á–∏–µ —Å–º—ã—Å–ª–∞
                quality = min(1.0, len(response) / 200) * 0.5 + 0.5
                components = ["micro_transformer", "moe", "conditional_gen"]
                if is_llm_response:
                    components.append("knowledge_distillation")
                self.meta_learner.report_response(quality, tier, components)
                self.meta_learner.optimize_step()
            except Exception as e:
                logger.debug(f"MetaLearner step error: {e}")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ø–∞–º—è—Ç—å: {e}")

    async def close(self):
        """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç –≤—Å–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç ResourceWarning)"""
        logger.info("üîå –ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –∞–≥–µ–Ω—Ç–æ–≤...")
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤
        for name, agent in self.agents.items():
            try:
                await agent.close()
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è {name}: {e}")

        # –ó–∞–∫—Ä—ã–≤–∞–µ–º vector_memory async client
        if hasattr(self.vector_memory, 'close'):
            try:
                await self.vector_memory.close()
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è vector_memory: {e}")

        # –ó–∞–∫—Ä—ã–≤–∞–µ–º SQLite-–∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        for component_name in ('micro_transformer', 'self_play', 'knowledge_distillation'):
            component = getattr(self, component_name, None)
            if component and hasattr(component, 'close'):
                try:
                    component.close()
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è {component_name}: {e}")

        logger.info("‚úÖ –í—Å–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –∑–∞–∫—Ä—ã—Ç—ã")

    def get_stats(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞"""

        agent_stats = {
            name: agent.get_stats()
            for name, agent in self.agents.items()
        }

        total_routed = (
            self.stats["tier1_hits"]
            + self.stats["tier2_hits"]
            + self.stats["tier3_hits"]
        )
        llm_free_pct = 0.0
        if total_routed > 0:
            llm_free_pct = (
                (self.stats["tier1_hits"] + self.stats["tier2_hits"])
                / total_routed * 100
            )

        dialogue_stats = self.dialogue_engine.get_stats()

        return {
            "orchestrator": self.stats,
            "agents": agent_stats,
            "vram": self.vram_manager.get_stats(),
            "learning": {
                "patterns": self.learned_patterns.get_stats(),
                "dialogue": dialogue_stats,
                "neural": dialogue_stats.get("neural", {}),
                "llm_free_percent": round(llm_free_pct, 1),
                "tier1_hits": self.stats["tier1_hits"],
                "tier2_hits": self.stats["tier2_hits"],
                "tier3_hits": self.stats["tier3_hits"],
            },
            # v7.2: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
            "evolution": {
                "bpe_tokenizer": self.bpe_tokenizer.get_stats(),
                "sentence_embeddings": self.sentence_embeddings.get_stats(),
                "active_learning": self.active_learning.get_stats(),
                "knowledge_distillation": self.knowledge_distillation.get_stats(),
                "micro_transformer": self.micro_transformer.get_stats(),
                "chain_of_thought": self.chain_of_thought.get_stats(),
                "self_play": self.self_play.get_stats(),
                "cross_attention": self.memory_attention.get_stats(),
                "dialogue_memory": self.dialogue_memory.get_stats(),
                "task_planner": self.task_planner.get_stats(),
                "conditional_gen": self.conditional_gen.get_stats(),
                "mixture_of_experts": self.moe.get_stats(),
                "code_understanding": self.code_understanding.get_stats(),
                "meta_learner": self.meta_learner.get_stats(),
            },
        }
