"""
–ö—Ä–∏—Å—Ç–∏–Ω–∞ 7.2 ‚Äî SentenceEmbeddings (–æ—Ç —Å–ª–æ–≤ –∫ —Ñ—Ä–∞–∑–∞–º)

–ó–ê–ß–ï–ú:
  Word2Vec –ø–æ–Ω–∏–º–∞–µ—Ç —Å–ª–æ–≤–∞ –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏:
    "–Ω–µ" = [0.1, -0.3, ...]
    "—Ä–∞–±–æ—Ç–∞–µ—Ç" = [0.4, 0.2, ...]

  –ù–æ "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç" != "—Ä–∞–±–æ—Ç–∞–µ—Ç" + "–Ω–µ"!
  –ù—É–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å –§–†–ê–ó–´ —Ü–µ–ª–∏–∫–æ–º.

–ê–†–•–ò–¢–ï–ö–¢–£–†–ê (3 —É—Ä–æ–≤–Ω—è, –æ—Ç –ø—Ä–æ—Å—Ç–æ–≥–æ –∫ —Å–ª–æ–∂–Ω–æ–º—É):

  Level 1: Weighted Average
    sentence_vec = Œ£(word_vec * idf_weight) / N
    –ü—Ä–æ—Å—Ç–æ, –Ω–æ —Ç–µ—Ä—è–µ—Ç –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤.

  Level 2: Positional Encoding
    sentence_vec = Œ£(word_vec + pos_encoding(i)) / N
    –£—á–∏—Ç—ã–≤–∞–µ—Ç –ü–û–ó–ò–¶–ò–Æ —Å–ª–æ–≤–∞ –≤–æ —Ñ—Ä–∞–∑–µ (–∫–∞–∫ –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö).

  Level 3: Learned Attention Pooling
    attention_weights = softmax(W @ word_vecs)
    sentence_vec = Œ£(attention_weight_i * word_vec_i)
    –£—á–∏—Ç—Å—è –∫–∞–∫–∏–µ —Å–ª–æ–≤–∞ –í–ê–ñ–ù–ï–ï –≤ –∫–∞–∂–¥–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.

–û–ë–£–ß–ï–ù–ò–ï:
  Level 1-2: –ù–µ —Ç—Ä–µ–±—É—é—Ç –æ–±—É—á–µ–Ω–∏—è (–∏—Å–ø–æ–ª—å–∑—É—é—Ç –≥–æ—Ç–æ–≤—ã–µ Word2Vec).
  Level 3: –û–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –ø–∞—Ä–∞—Ö (–≤–æ–ø—Ä–æ—Å, –æ—Ç–≤–µ—Ç) ‚Äî –ø–æ—Ö–æ–∂–∏–µ –ø–∞—Ä—ã
           –¥–æ–ª–∂–Ω—ã –¥–∞–≤–∞—Ç—å –ø–æ—Ö–æ–∂–∏–µ sentence vectors.

–ò–ù–¢–ï–ì–†–ê–¶–ò–Ø:
  - NeuralEngine.understand_sentence() ‚Üí —Ç–µ–ø–µ—Ä—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç sentence vector
  - VectorStore ‚Üí –º–æ–∂–Ω–æ –∏—Å–∫–∞—Ç—å –ø–æ sentence embeddings –≤–º–µ—Å—Ç–æ bge-m3
  - IntentRouter ‚Üí —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
"""

import math
import json
import sqlite3
import time
import random
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from collections import Counter

from utils.logging import get_logger
import config

logger = get_logger("sentence_embeddings")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#               –ú–ê–¢–ï–ú–ê–¢–ò–ö–ê
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


def _cosine_similarity(v1: List[float], v2: List[float]) -> float:
    """–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–≤—É—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤"""
    if len(v1) != len(v2) or not v1:
        return 0.0
    dot = sum(a * b for a, b in zip(v1, v2))
    norm1 = math.sqrt(sum(a * a for a in v1))
    norm2 = math.sqrt(sum(b * b for b in v2))
    if norm1 < 1e-10 or norm2 < 1e-10:
        return 0.0
    return dot / (norm1 * norm2)


def _vec_add(v1: List[float], v2: List[float]) -> List[float]:
    return [a + b for a, b in zip(v1, v2)]


def _vec_scale(v: List[float], s: float) -> List[float]:
    return [a * s for a in v]


def _vec_normalize(v: List[float]) -> List[float]:
    norm = math.sqrt(sum(a * a for a in v))
    if norm < 1e-10:
        return v
    return [a / norm for a in v]


def _softmax(values: List[float]) -> List[float]:
    """–°—Ç–∞–±–∏–ª—å–Ω–∞—è softmax"""
    if not values:
        return []
    max_val = max(values)
    exps = [math.exp(v - max_val) for v in values]
    total = sum(exps)
    if total < 1e-10:
        return [1.0 / len(values)] * len(values)
    return [e / total for e in exps]


class SentenceEmbeddings:
    """
    –ü–æ–Ω–∏–º–∞–Ω–∏–µ —Ñ—Ä–∞–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ Word2Vec —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.

    –¢—Ä–∏ —É—Ä–æ–≤–Ω—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏:
    1. Weighted Average (IDF-–≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ)
    2. Positional Encoding (—Å —É—á—ë—Ç–æ–º –ø–æ–∑–∏—Ü–∏–∏)
    3. Attention Pooling (–æ–±—É—á–∞–µ–º–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ)

    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
        se = SentenceEmbeddings(neural_engine)
        vec = se.encode("–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?")          # Level 1
        vec = se.encode("–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?", level=2)  # Level 2
        vec = se.encode("–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?", level=3)  # Level 3

        sim = se.similarity("–ü—Ä–∏–≤–µ—Ç!", "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π!")   # 0.87
    """

    def __init__(self, neural_engine, db_path: Path = None):
        """
        Args:
            neural_engine: NeuralEngine instance (–¥–ª—è Word2Vec —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤)
        """
        self._engine = neural_engine
        self._db_path = db_path or (config.config.data_dir / "sentence_embeddings.db")
        self._db_path.parent.mkdir(parents=True, exist_ok=True)

        self._conn = sqlite3.connect(str(self._db_path))
        self._conn.row_factory = sqlite3.Row
        self._conn.execute("PRAGMA journal_mode=WAL")

        self._create_tables()

        # IDF –∫–µ—à (Inverse Document Frequency)
        self._idf_cache: Dict[str, float] = {}
        self._doc_count = 0

        # Attention weights (Level 3) ‚Äî –æ–±—É—á–∞–µ–º—ã–µ
        self._embedding_dim = 128  # –°–æ–≤–ø–∞–¥–∞–µ—Ç —Å NeuralEngine EMBEDDING_DIM (v7.3: 64‚Üí128)
        self._attention_w: Optional[List[float]] = None  # –í–µ–∫—Ç–æ—Ä –≤–Ω–∏–º–∞–Ω–∏—è

        self._load_state()

        logger.info(
            f"üìê SentenceEmbeddings: dim={self._embedding_dim}, "
            f"idf_words={len(self._idf_cache)}, "
            f"attention={'trained' if self._attention_w else 'untrained'}"
        )

    def _create_tables(self):
        cur = self._conn.cursor()

        # IDF —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (–≤ —Å–∫–æ–ª—å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —Å–ª–æ–≤–æ)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS word_doc_freq (
                word TEXT PRIMARY KEY,
                doc_count INTEGER DEFAULT 1,
                updated_at REAL NOT NULL
            )
        """)

        # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        cur.execute("""
            CREATE TABLE IF NOT EXISTS meta (
                key TEXT PRIMARY KEY,
                value TEXT NOT NULL
            )
        """)

        # Attention weights (Level 3)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS attention_weights (
                id INTEGER PRIMARY KEY CHECK (id = 1),
                weights TEXT NOT NULL,
                training_steps INTEGER DEFAULT 0,
                updated_at REAL NOT NULL
            )
        """)

        # –ö–µ—à sentence embeddings (–¥–ª—è —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Ñ—Ä–∞–∑)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS embedding_cache (
                text_hash TEXT PRIMARY KEY,
                text TEXT NOT NULL,
                embedding TEXT NOT NULL,
                level INTEGER NOT NULL,
                created_at REAL NOT NULL
            )
        """)

        self._conn.commit()

    def _load_state(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç IDF –∏ attention weights"""
        # IDF
        rows = self._conn.execute(
            "SELECT word, doc_count FROM word_doc_freq"
        ).fetchall()
        for row in rows:
            self._idf_cache[row["word"]] = row["doc_count"]

        # Doc count
        meta = self._conn.execute(
            "SELECT value FROM meta WHERE key = 'doc_count'"
        ).fetchone()
        self._doc_count = int(meta["value"]) if meta else 0

        # Attention weights
        att = self._conn.execute(
            "SELECT weights FROM attention_weights WHERE id = 1"
        ).fetchone()
        if att:
            try:
                self._attention_w = json.loads(att["weights"])
            except (json.JSONDecodeError, TypeError):
                self._attention_w = None

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    #               –ö–û–î–ò–†–û–í–ê–ù–ò–ï –§–†–ê–ó
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def encode(
        self,
        text: str,
        level: int = 2,
    ) -> Optional[List[float]]:
        """
        –ö–æ–¥–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ –≤–µ–∫—Ç–æ—Ä —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏.

        Args:
            text: —Ç–µ–∫—Å—Ç –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
            level: —É—Ä–æ–≤–µ–Ω—å –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ (1=avg, 2=positional, 3=attention)

        Returns:
            –í–µ–∫—Ç–æ—Ä —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ EMBEDDING_DIM –∏–ª–∏ None
        """
        tokens = self._engine.tokenize(text)
        words = [t for t in tokens if t.isalpha() or (len(t) > 1 and t.isalnum())]

        if not words:
            return None

        # –ü–æ–ª—É—á–∞–µ–º Word2Vec —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        word_vecs = []
        valid_words = []
        for word in words:
            emb = self._engine._embeddings_cache.get(word.lower())
            if emb:
                word_vecs.append(emb)
                valid_words.append(word.lower())

        if not word_vecs:
            return None

        if level == 1:
            return self._encode_weighted_avg(valid_words, word_vecs)
        elif level == 2:
            return self._encode_positional(valid_words, word_vecs)
        elif level == 3:
            return self._encode_attention(valid_words, word_vecs)
        else:
            return self._encode_weighted_avg(valid_words, word_vecs)

    def _encode_weighted_avg(
        self,
        words: List[str],
        word_vecs: List[List[float]],
    ) -> List[float]:
        """
        Level 1: IDF-–≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ.

        –†–µ–¥–∫–∏–µ —Å–ª–æ–≤–∞ –≤–∞–∂–Ω–µ–µ (IDF = log(N / doc_freq)).
        "–°–æ–∑–¥–∞–π —Ñ–∞–π–ª Python" ‚Üí "Python" –≤–µ—Å–∏—Ç –±–æ–ª—å—à–µ —á–µ–º "—Ñ–∞–π–ª".
        """
        dim = len(word_vecs[0])
        result = [0.0] * dim

        total_weight = 0.0
        for word, vec in zip(words, word_vecs):
            weight = self._get_idf(word)
            result = _vec_add(result, _vec_scale(vec, weight))
            total_weight += weight

        if total_weight > 0:
            result = _vec_scale(result, 1.0 / total_weight)

        return _vec_normalize(result)

    def _encode_positional(
        self,
        words: List[str],
        word_vecs: List[List[float]],
    ) -> List[float]:
        """
        Level 2: –° –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º (–∫–∞–∫ –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö).

        –ü–æ–∑–∏—Ü–∏—è —Å–ª–æ–≤–∞ –≤–ª–∏—è–µ—Ç –Ω–∞ –≤–µ–∫—Ç–æ—Ä:
        "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç" ‚â† "—Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–µ" (—Ö–æ—Ç—è —Å–ª–æ–≤–∞ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ)
        """
        dim = len(word_vecs[0])
        n = len(word_vecs)
        result = [0.0] * dim

        total_weight = 0.0
        for i, (word, vec) in enumerate(zip(words, word_vecs)):
            # IDF weight
            idf_weight = self._get_idf(word)

            # Positional encoding (—Å–∏–Ω—É—Å–æ–∏–¥–∞–ª—å–Ω–æ–µ, –∫–∞–∫ –≤ Vaswani et al.)
            pos_enc = self._positional_encoding(i, dim)

            # word_vec + pos_encoding
            enriched = _vec_add(vec, _vec_scale(pos_enc, 0.1))  # –ù–µ–±–æ–ª—å—à–æ–π –≤–µ—Å –ø–æ–∑–∏—Ü–∏–∏

            result = _vec_add(result, _vec_scale(enriched, idf_weight))
            total_weight += idf_weight

        if total_weight > 0:
            result = _vec_scale(result, 1.0 / total_weight)

        return _vec_normalize(result)

    def _encode_attention(
        self,
        words: List[str],
        word_vecs: List[List[float]],
    ) -> List[float]:
        """
        Level 3: –û–±—É—á–∞–µ–º—ã–π attention pooling.

        attention_score(word) = W ¬∑ word_vec
        weights = softmax(attention_scores)
        sentence_vec = Œ£(weight_i * word_vec_i)

        W –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –ø–∞—Ä–∞—Ö (–≤–æ–ø—Ä–æ—Å, –æ—Ç–≤–µ—Ç).
        """
        dim = len(word_vecs[0])

        # –ï—Å–ª–∏ attention weights –µ—â—ë –Ω–µ –æ–±—É—á–µ–Ω—ã ‚Äî fallback –Ω–∞ Level 2
        if self._attention_w is None or len(self._attention_w) != dim:
            return self._encode_positional(words, word_vecs)

        # –í—ã—á–∏—Å–ª—è–µ–º attention scores
        scores = []
        for vec in word_vecs:
            score = sum(a * b for a, b in zip(self._attention_w, vec))
            scores.append(score)

        # Softmax
        weights = _softmax(scores)

        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞
        result = [0.0] * dim
        for weight, vec in zip(weights, word_vecs):
            result = _vec_add(result, _vec_scale(vec, weight))

        return _vec_normalize(result)

    def _positional_encoding(self, pos: int, dim: int) -> List[float]:
        """
        –°–∏–Ω—É—Å–æ–∏–¥–∞–ª—å–Ω–æ–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ (Vaswani et al., 2017).

        PE(pos, 2i)   = sin(pos / 10000^(2i/dim))
        PE(pos, 2i+1) = cos(pos / 10000^(2i/dim))
        """
        pe = [0.0] * dim
        for i in range(0, dim, 2):
            div = math.pow(10000.0, (2 * i) / dim)
            pe[i] = math.sin(pos / div)
            if i + 1 < dim:
                pe[i + 1] = math.cos(pos / div)
        return pe

    def _get_idf(self, word: str) -> float:
        """IDF weight: log(total_docs / word_doc_count + 1)"""
        doc_freq = self._idf_cache.get(word, 0)
        if doc_freq == 0 or self._doc_count == 0:
            return 1.0
        return math.log(self._doc_count / (doc_freq + 1)) + 1.0

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    #               –°–†–ê–í–ù–ï–ù–ò–ï –§–†–ê–ó
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def similarity(self, text1: str, text2: str, level: int = 2) -> float:
        """
        –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–≤—É—Ö —Ñ—Ä–∞–∑.

        similarity("–ü—Ä–∏–≤–µ—Ç!", "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π!") ‚Üí 0.87
        similarity("–°–æ–∑–¥–∞–π —Ñ–∞–π–ª", "–£–¥–∞–ª–∏ —Ñ–∞–π–ª") ‚Üí 0.45
        """
        vec1 = self.encode(text1, level=level)
        vec2 = self.encode(text2, level=level)
        if vec1 is None or vec2 is None:
            return 0.0
        return _cosine_similarity(vec1, vec2)

    def find_most_similar(
        self,
        query: str,
        candidates: List[str],
        level: int = 2,
        top_n: int = 5,
    ) -> List[Tuple[str, float]]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–µ —Ñ—Ä–∞–∑—ã –∏–∑ —Å–ø–∏—Å–∫–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤.

        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ IntentRouter –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤.
        """
        query_vec = self.encode(query, level=level)
        if query_vec is None:
            return []

        results = []
        for candidate in candidates:
            cand_vec = self.encode(candidate, level=level)
            if cand_vec is not None:
                sim = _cosine_similarity(query_vec, cand_vec)
                results.append((candidate, sim))

        results.sort(key=lambda x: x[1], reverse=True)
        return results[:top_n]

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    #               –û–ë–£–ß–ï–ù–ò–ï
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def learn_from_text(self, text: str):
        """
        –û–±–Ω–æ–≤–ª—è–µ—Ç IDF —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –Ω–∞ –Ω–æ–≤–æ–º —Ç–µ–∫—Å—Ç–µ.
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–∏–∞–ª–æ–≥–∞.
        """
        tokens = self._engine.tokenize(text)
        words = set(t.lower() for t in tokens if t.isalpha() and len(t) > 2)

        if not words:
            return

        now = time.time()
        self._doc_count += 1

        for word in words:
            self._idf_cache[word] = self._idf_cache.get(word, 0) + 1
            self._conn.execute("""
                INSERT INTO word_doc_freq (word, doc_count, updated_at)
                VALUES (?, 1, ?)
                ON CONFLICT(word)
                DO UPDATE SET doc_count = doc_count + 1, updated_at = ?
            """, (word, now, now))

        self._conn.execute("""
            INSERT INTO meta (key, value) VALUES ('doc_count', ?)
            ON CONFLICT(key) DO UPDATE SET value = ?
        """, (str(self._doc_count), str(self._doc_count)))

        self._conn.commit()

    def train_attention(
        self,
        positive_pairs: List[Tuple[str, str]],
        negative_pairs: List[Tuple[str, str]] = None,
        learning_rate: float = 0.01,
        epochs: int = 10,
    ):
        """
        –û–±—É—á–∞–µ—Ç attention weights (Level 3) –Ω–∞ –ø–∞—Ä–∞—Ö —Ñ—Ä–∞–∑.

        positive_pairs: –ø–æ—Ö–æ–∂–∏–µ —Ñ—Ä–∞–∑—ã (–≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç, —Å–∏–Ω–æ–Ω–∏–º—ã)
        negative_pairs: –Ω–µ–ø–æ—Ö–æ–∂–∏–µ —Ñ—Ä–∞–∑—ã (—Å–ª—É—á–∞–π–Ω—ã–µ –ø–∞—Ä—ã)

        –¶–µ–ª—å: attention weights –¥–æ–ª–∂–Ω—ã –¥–∞–≤–∞—Ç—å –≤—ã—Å–æ–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        –¥–ª—è positive_pairs –∏ –Ω–∏–∑–∫–æ–µ –¥–ª—è negative_pairs.
        """
        dim = self._embedding_dim

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º weights –µ—Å–ª–∏ –Ω–µ—Ç
        if self._attention_w is None or len(self._attention_w) != dim:
            self._attention_w = [(random.random() - 0.5) * 0.1 for _ in range(dim)]

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º negative pairs –µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω—ã
        if negative_pairs is None and len(positive_pairs) >= 2:
            negative_pairs = []
            texts = [t for pair in positive_pairs for t in pair]
            for _ in range(len(positive_pairs)):
                t1 = random.choice(texts)
                t2 = random.choice(texts)
                if t1 != t2:
                    negative_pairs.append((t1, t2))

        for epoch in range(epochs):
            total_loss = 0.0

            # Positive: —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ
            for text1, text2 in positive_pairs:
                vec1 = self._encode_attention_with_grad(text1)
                vec2 = self._encode_attention_with_grad(text2)
                if vec1 is None or vec2 is None:
                    continue

                sim = _cosine_similarity(vec1["vector"], vec2["vector"])
                loss = max(0, 1.0 - sim)  # Hinge loss: —Ö–æ—Ç–∏–º sim ‚Üí 1.0
                total_loss += loss

                if loss > 0:
                    self._update_attention_weights(
                        vec1, vec2, learning_rate, positive=True
                    )

            # Negative: —É–º–µ–Ω—å—à–∞–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ
            if negative_pairs:
                for text1, text2 in negative_pairs:
                    vec1 = self._encode_attention_with_grad(text1)
                    vec2 = self._encode_attention_with_grad(text2)
                    if vec1 is None or vec2 is None:
                        continue

                    sim = _cosine_similarity(vec1["vector"], vec2["vector"])
                    margin = 0.5
                    loss = max(0, sim - margin)  # –•–æ—Ç–∏–º sim < margin
                    total_loss += loss

                    if loss > 0:
                        self._update_attention_weights(
                            vec1, vec2, learning_rate, positive=False
                        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º weights
        self._save_attention_weights()
        logger.debug(f"üìê Attention trained: {epochs} epochs, final loss={total_loss:.4f}")

    def _encode_attention_with_grad(self, text: str) -> Optional[Dict]:
        """–ö–æ–¥–∏—Ä—É–µ—Ç —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞"""
        tokens = self._engine.tokenize(text)
        words = [t.lower() for t in tokens if t.isalpha() and len(t) > 1]

        word_vecs = []
        valid_words = []
        for word in words:
            emb = self._engine._embeddings_cache.get(word)
            if emb:
                word_vecs.append(emb)
                valid_words.append(word)

        if not word_vecs:
            return None

        dim = len(word_vecs[0])
        if len(self._attention_w) != dim:
            return None

        scores = [sum(a * b for a, b in zip(self._attention_w, vec)) for vec in word_vecs]
        weights = _softmax(scores)

        result = [0.0] * dim
        for w, vec in zip(weights, word_vecs):
            result = _vec_add(result, _vec_scale(vec, w))

        return {
            "vector": _vec_normalize(result),
            "word_vecs": word_vecs,
            "weights": weights,
            "scores": scores,
        }

    def _update_attention_weights(
        self,
        data1: Dict,
        data2: Dict,
        lr: float,
        positive: bool,
    ):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç attention weights –Ω–∞ –æ–¥–Ω–æ–π –ø–∞—Ä–µ"""
        dim = len(self._attention_w)
        direction = 1.0 if positive else -1.0

        # –ü—Ä–æ—Å—Ç–æ–π gradient: –¥–≤–∏–≥–∞–µ–º W —á—Ç–æ–±—ã —É–≤–µ–ª–∏—á–∏—Ç—å/—É–º–µ–Ω—å—à–∏—Ç—å dot product
        # –º–µ–∂–¥—É encodings –¥–≤—É—Ö —Ç–µ–∫—Å—Ç–æ–≤
        for k in range(dim):
            grad = 0.0
            for vec, weight in zip(data1["word_vecs"], data1["weights"]):
                grad += vec[k] * data2["vector"][k]
            for vec, weight in zip(data2["word_vecs"], data2["weights"]):
                grad += vec[k] * data1["vector"][k]

            self._attention_w[k] += direction * lr * grad

    def _save_attention_weights(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç attention weights –≤ SQLite"""
        now = time.time()
        self._conn.execute("""
            INSERT INTO attention_weights (id, weights, training_steps, updated_at)
            VALUES (1, ?, 1, ?)
            ON CONFLICT(id)
            DO UPDATE SET weights = ?, training_steps = training_steps + 1, updated_at = ?
        """, (json.dumps(self._attention_w), now, json.dumps(self._attention_w), now))
        self._conn.commit()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    #               –£–¢–ò–õ–ò–¢–´
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def get_stats(self) -> Dict:
        return {
            "embedding_dim": self._embedding_dim,
            "idf_words": len(self._idf_cache),
            "doc_count": self._doc_count,
            "attention_trained": self._attention_w is not None,
        }

    def close(self):
        self._conn.commit()
        self._conn.close()
