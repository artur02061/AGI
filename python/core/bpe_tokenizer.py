"""
–ö—Ä–∏—Å—Ç–∏–Ω–∞ 7.2 ‚Äî BPE Tokenizer (Byte-Pair Encoding)

–ó–ê–ß–ï–ú:
  –û–±—ã—á–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø–æ —Å–ª–æ–≤–∞–º –ù–ï —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞:
  - "–ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å" = –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ —Å–ª–æ–≤–æ (OOV)
  - "–Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å" = –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ —Å–ª–æ–≤–æ (OOV)

  BPE —Ä–∞–∑–±–∏–≤–∞–µ—Ç –Ω–∞ –ø–æ–¥—Å–ª–æ–≤–∞:
  - "–ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å" ‚Üí ["–ø–µ—Ä–µ", "–∑–∞", "–ø—É—Å—Ç", "–∏—Ç—å"]
  - "–Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å" ‚Üí ["–Ω–µ", "–≤–æ–∑–º–æ–∂–Ω", "–æ—Å—Ç—å"]

  –≠—Ç–æ –¥–∞—ë—Ç:
  1. –ù–µ—Ç OOV ‚Äî –õ–Æ–ë–û–ï —Å–ª–æ–≤–æ —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —á–∞—Å—Ç–∏
  2. –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è ‚Äî –ö—Ä–∏—Å—Ç–∏–Ω–∞ –ø–æ–Ω–∏–º–∞–µ—Ç –ø—Ä–∏—Å—Ç–∞–≤–∫–∏, —Å—É—Ñ—Ñ–∏–∫—Å—ã, –∫–æ—Ä–Ω–∏
  3. –ö–æ–º–ø–∞–∫—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å ‚Äî 8000-16000 –ø–æ–¥—Å–ª–æ–≤ –≤–º–µ—Å—Ç–æ 100K+ —Å–ª–æ–≤
  4. –§—É–Ω–¥–∞–º–µ–Ω—Ç –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ ‚Äî BPE —Ç–æ–∫–µ–Ω—ã = –≤—Ö–æ–¥ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞

–ê–õ–ì–û–†–ò–¢–ú:
  1. –ù–∞—á–∏–Ω–∞–µ–º —Å —Å–∏–º–≤–æ–ª–æ–≤ (–∫–∞–∂–¥—ã–π —Å–∏–º–≤–æ–ª = —Ç–æ–∫–µ–Ω)
  2. –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—É –ü–ê–†–´ —Å–æ—Å–µ–¥–Ω–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤
  3. –°–∞–º—É—é —á–∞—Å—Ç—É—é –ø–∞—Ä—É –°–õ–ò–í–ê–ï–ú –≤ –æ–¥–∏–Ω —Ç–æ–∫–µ–Ω
  4. –ü–æ–≤—Ç–æ—Ä—è–µ–º –¥–æ –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Å–ª–æ–≤–∞—Ä—è

–û–ë–£–ß–ï–ù–ò–ï:
  –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ ‚Äî –º–æ–∂–Ω–æ –¥–æ–æ–±—É—á–∞—Ç—å –Ω–∞ –Ω–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö
  –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Å—Ç–∞—Ä—ã—Ö merge rules.

–•–†–ê–ù–ï–ù–ò–ï:
  SQLite ‚Äî merge rules + vocabulary (–ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ)
"""

import sqlite3
import json
import re
import time
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Set
from collections import Counter, defaultdict

from utils.logging import get_logger
import config

logger = get_logger("bpe_tokenizer")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#               –ö–û–ù–°–¢–ê–ù–¢–´
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

DEFAULT_VOCAB_SIZE = 8000        # –¶–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è
MIN_PAIR_FREQ = 2               # –ú–∏–Ω. —á–∞—Å—Ç–æ—Ç–∞ –ø–∞—Ä—ã –¥–ª—è —Å–ª–∏—è–Ω–∏—è
SPECIAL_TOKENS = {
    "<PAD>": 0,
    "<UNK>": 1,
    "<S>": 2,     # –ù–∞—á–∞–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    "</S>": 3,    # –ö–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    "<SEP>": 4,   # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å (–≤–æ–ø—Ä–æ—Å/–æ—Ç–≤–µ—Ç)
    "<MASK>": 5,  # –î–ª—è masked language modeling
}

# –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ —á–∞—Å—Ç—ã–µ –ø–æ–¥—Å–ª–æ–≤–∞ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ (—É—Å–∫–æ—Ä—è—é—Ç –Ω–∞—á–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ)
RUSSIAN_SEED_MERGES = [
    # –ü—Ä–∏—Å—Ç–∞–≤–∫–∏
    ("–ø", "–æ"), ("–ø", "—Ä–µ"), ("–ø—Ä–µ", "–¥"), ("–Ω", "–µ"), ("–≤", "—ã"),
    ("–ø", "–µ—Ä"), ("–ø–µ—Ä", "–µ"), ("–Ω", "–∞"), ("–∑", "–∞"), ("–æ", "—Ç"),
    ("–ø", "—Ä–∏"), ("–≤", "–æ"), ("—Ä", "–∞"), ("—Ä–∞", "–∑"),
    # –°—É—Ñ—Ñ–∏–∫—Å—ã
    ("–Ω", "–æ"), ("—Ç", "—å"), ("—Å", "—Ç"), ("—Å—Ç", "—å"),
    ("–µ", "–Ω"), ("–µ–Ω", "–∏"), ("–µ–Ω–∏", "–µ"),
    ("–æ", "—Å"), ("–æ—Å", "—Ç"), ("–æ—Å—Ç", "—å"),
    # –ö–æ—Ä–Ω–∏
    ("–º", "–æ"), ("–º–æ", "–≥"), ("–º–æ–≥", "—É"),
    ("–¥", "–µ"), ("–¥–µ", "–ª"), ("–¥–µ–ª", "–∞"),
    ("—Ä", "–∞"), ("—Ä–∞", "–±"), ("—Ä–∞–±", "–æ"), ("—Ä–∞–±–æ", "—Ç"),
    ("–ø", "–æ"), ("–ø–æ", "–º"), ("–ø–æ–º", "–æ"),
]


class BPETokenizer:
    """
    Byte-Pair Encoding —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –ö—Ä–∏—Å—Ç–∏–Ω—ã.

    –£—á–∏—Ç—Å—è –Ω–∞ –¥–∏–∞–ª–æ–≥–∞—Ö, —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –ø–æ–¥—Å–ª–æ–≤–∞.
    –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ ‚Äî —Ä–∞—Å—Ç—ë—Ç —Å –∫–∞–∂–¥—ã–º –Ω–æ–≤—ã–º —Ç–µ–∫—Å—Ç–æ–º.

    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
        tokenizer = BPETokenizer()
        tokenizer.train_on_text("–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?")  # –æ–±—É—á–µ–Ω–∏–µ
        tokens = tokenizer.encode("–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å")     # [23, 45, 67]
        text = tokenizer.decode([23, 45, 67])           # "–Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å"
    """

    def __init__(self, db_path: Path = None, vocab_size: int = DEFAULT_VOCAB_SIZE):
        self._db_path = db_path or (config.config.data_dir / "bpe_tokenizer.db")
        self._db_path.parent.mkdir(parents=True, exist_ok=True)
        self._target_vocab_size = vocab_size

        self._conn = sqlite3.connect(str(self._db_path))
        self._conn.row_factory = sqlite3.Row
        self._conn.execute("PRAGMA journal_mode=WAL")
        self._conn.execute("PRAGMA synchronous=NORMAL")

        self._create_tables()

        # In-memory —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        self._merges: List[Tuple[str, str]] = []     # –£–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã–µ merge rules
        self._vocab: Dict[str, int] = {}              # token ‚Üí id
        self._id_to_token: Dict[int, str] = {}        # id ‚Üí token
        self._pair_freqs: Counter = Counter()          # –ß–∞—Å—Ç–æ—Ç—ã –ø–∞—Ä (–¥–ª—è –∏–Ω–∫—Ä–µ–º. –æ–±—É—á–µ–Ω–∏—è)
        self._word_freqs: Counter = Counter()          # –ß–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤ (–¥–ª—è –æ–±—É—á–µ–Ω–∏—è)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        self._load_state()

        # –ï—Å–ª–∏ —Å–ª–æ–≤–∞—Ä—å –ø—É—Å—Ç–æ–π ‚Äî –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º
        if not self._vocab:
            self._init_base_vocab()

        stats = self.get_stats()
        logger.info(
            f"üìù BPE Tokenizer: {stats['vocab_size']} —Ç–æ–∫–µ–Ω–æ–≤, "
            f"{stats['merge_rules']} merge rules, "
            f"{stats['texts_trained']} —Ç–µ–∫—Å—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ"
        )

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    #               –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _create_tables(self):
        cur = self._conn.cursor()

        # Merge rules (–ø–æ—Ä—è–¥–æ–∫ –≤–∞–∂–µ–Ω!)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS merge_rules (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                token_a TEXT NOT NULL,
                token_b TEXT NOT NULL,
                merged TEXT NOT NULL,
                frequency INTEGER DEFAULT 0,
                created_at REAL NOT NULL,
                UNIQUE(token_a, token_b)
            )
        """)

        # Vocabulary: token ‚Üí id
        cur.execute("""
            CREATE TABLE IF NOT EXISTS vocabulary (
                token TEXT PRIMARY KEY,
                token_id INTEGER NOT NULL UNIQUE,
                frequency INTEGER DEFAULT 0,
                is_special INTEGER DEFAULT 0,
                created_at REAL NOT NULL
            )
        """)

        # Word frequencies (–¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS word_frequencies (
                word TEXT PRIMARY KEY,
                frequency INTEGER DEFAULT 1,
                updated_at REAL NOT NULL
            )
        """)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
        cur.execute("""
            CREATE TABLE IF NOT EXISTS training_stats (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp REAL NOT NULL,
                texts_count INTEGER DEFAULT 0,
                words_count INTEGER DEFAULT 0,
                merges_added INTEGER DEFAULT 0
            )
        """)

        cur.execute("CREATE INDEX IF NOT EXISTS idx_vocab_id ON vocabulary(token_id)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_merge_order ON merge_rules(id)")

        self._conn.commit()

    def _init_base_vocab(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å —Å–∏–º–≤–æ–ª–æ–≤ + —Å–ø–µ—Ü—Ç–æ–∫–µ–Ω–æ–≤"""
        now = time.time()

        # 1. –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        for token, token_id in SPECIAL_TOKENS.items():
            self._vocab[token] = token_id
            self._id_to_token[token_id] = token
            self._conn.execute("""
                INSERT OR IGNORE INTO vocabulary (token, token_id, is_special, created_at)
                VALUES (?, ?, 1, ?)
            """, (token, token_id, now))

        # 2. –ë–∞–∑–æ–≤—ã–µ —Å–∏–º–≤–æ–ª—ã (—Ä—É—Å—Å–∫–∏–π + –ª–∞—Ç–∏–Ω–∏—Ü–∞ + —Ü–∏—Ñ—Ä—ã + –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è)
        next_id = len(SPECIAL_TOKENS)
        base_chars = (
            "–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è"
            "abcdefghijklmnopqrstuvwxyz"
            "0123456789"
            " .!?,;:-‚Äî()\"'/"
        )
        for char in base_chars:
            if char not in self._vocab:
                self._vocab[char] = next_id
                self._id_to_token[next_id] = char
                self._conn.execute("""
                    INSERT OR IGNORE INTO vocabulary (token, token_id, created_at)
                    VALUES (?, ?, ?)
                """, (char, next_id, now))
                next_id += 1

        self._conn.commit()
        logger.info(f"üìù BPE: initialized base vocabulary with {len(self._vocab)} tokens")

    def _load_state(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç merge rules –∏ vocabulary –∏–∑ SQLite"""
        # Vocabulary
        rows = self._conn.execute(
            "SELECT token, token_id, frequency FROM vocabulary ORDER BY token_id"
        ).fetchall()
        for row in rows:
            self._vocab[row["token"]] = row["token_id"]
            self._id_to_token[row["token_id"]] = row["token"]

        # Merge rules (–ø–æ—Ä—è–¥–æ–∫ –∫—Ä–∏—Ç–∏—á–µ–Ω!)
        rows = self._conn.execute(
            "SELECT token_a, token_b FROM merge_rules ORDER BY id"
        ).fetchall()
        self._merges = [(row["token_a"], row["token_b"]) for row in rows]

        # Word frequencies
        rows = self._conn.execute(
            "SELECT word, frequency FROM word_frequencies"
        ).fetchall()
        self._word_freqs = Counter({row["word"]: row["frequency"] for row in rows})

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    #               –û–ë–£–ß–ï–ù–ò–ï (–ò–ù–ö–†–ï–ú–ï–ù–¢–ê–õ–¨–ù–û–ï)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def train_on_text(self, text: str, num_merges: int = 50):
        """
        –û–±—É—á–∞–µ—Ç BPE –Ω–∞ –Ω–æ–≤–æ–º —Ç–µ–∫—Å—Ç–µ (–∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ).

        1. –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–ª–æ–≤–∞
        2. –û–±–Ω–æ–≤–ª—è–µ—Ç —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤
        3. –í—ã–ø–æ–ª–Ω—è–µ—Ç num_merges –Ω–æ–≤—ã—Ö —Å–ª–∏—è–Ω–∏–π (–µ—Å–ª–∏ –µ—Å—Ç—å —á–∞—Å—Ç—ã–µ –ø–∞—Ä—ã)

        Args:
            text: —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            num_merges: –º–∞–∫—Å. –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤—ã—Ö merge rules –∑–∞ –æ–¥–∏–Ω –≤—ã–∑–æ–≤
        """
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        words = self._preprocess_text(text)
        if not words:
            return

        # –û–±–Ω–æ–≤–ª—è–µ–º —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤
        now = time.time()
        word_counter = Counter(words)
        self._word_freqs.update(word_counter)

        for word, freq in word_counter.items():
            self._conn.execute("""
                INSERT INTO word_frequencies (word, frequency, updated_at)
                VALUES (?, ?, ?)
                ON CONFLICT(word)
                DO UPDATE SET frequency = frequency + ?, updated_at = ?
            """, (word, freq, now, freq, now))

        # –ï—Å–ª–∏ —Å–ª–æ–≤–∞—Ä—å –µ—â—ë –Ω–µ –¥–æ—Å—Ç–∏–≥ —Ü–µ–ª–µ–≤–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ ‚Äî —É—á–∏–º –Ω–æ–≤—ã–µ merge rules
        merges_added = 0
        if len(self._vocab) < self._target_vocab_size:
            merges_added = self._learn_merges(num_merges)

        # –õ–æ–≥–∏—Ä—É–µ–º
        self._conn.execute("""
            INSERT INTO training_stats (timestamp, texts_count, words_count, merges_added)
            VALUES (?, 1, ?, ?)
        """, (now, len(words), merges_added))
        self._conn.commit()

        logger.debug(
            f"üìù BPE trained: {len(words)} words, "
            f"{merges_added} new merges, "
            f"vocab={len(self._vocab)}"
        )

    def train_on_corpus(self, texts: List[str], num_merges: int = 500):
        """
        –û–±—É—á–∞–µ—Ç BPE –Ω–∞ –∫–æ—Ä–ø—É—Å–µ —Ç–µ–∫—Å—Ç–æ–≤ (–ø–∞–∫–µ—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ).
        –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ, —á–µ–º train_on_text –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –æ—Ç–¥–µ–ª—å–Ω–æ.
        """
        all_words = []
        for text in texts:
            all_words.extend(self._preprocess_text(text))

        if not all_words:
            return

        now = time.time()
        word_counter = Counter(all_words)
        self._word_freqs.update(word_counter)

        for word, freq in word_counter.items():
            self._conn.execute("""
                INSERT INTO word_frequencies (word, frequency, updated_at)
                VALUES (?, ?, ?)
                ON CONFLICT(word)
                DO UPDATE SET frequency = frequency + ?, updated_at = ?
            """, (word, freq, now, freq, now))

        merges_added = self._learn_merges(num_merges)

        self._conn.execute("""
            INSERT INTO training_stats (timestamp, texts_count, words_count, merges_added)
            VALUES (?, ?, ?, ?)
        """, (now, len(texts), len(all_words), merges_added))
        self._conn.commit()

        logger.info(
            f"üìù BPE corpus training: {len(texts)} texts, "
            f"{len(all_words)} words, {merges_added} merges, "
            f"vocab={len(self._vocab)}"
        )

    def _preprocess_text(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–ª–æ–≤–∞ (–¥–ª—è BPE –æ–±—É—á–µ–Ω–∏—è)"""
        text = text.lower().strip()
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞ (—Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã –∏ —Ü–∏—Ñ—Ä—ã)
        words = re.findall(r'[–∞-—è—ëa-z0-9]+', text)
        return [w for w in words if len(w) >= 2]

    def _learn_merges(self, max_merges: int) -> int:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º BPE: –Ω–∞—Ö–æ–¥–∏—Ç –∏ —Å–ª–∏–≤–∞–µ—Ç —á–∞—Å—Ç—ã–µ –ø–∞—Ä—ã.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤—ã—Ö merge rules.
        """
        # –°—Ç—Ä–æ–∏–º —Ç–µ–∫—É—â–µ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤ —á–µ—Ä–µ–∑ —Å–∏–º–≤–æ–ª—ã + —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ merges
        word_splits = {}
        for word, freq in self._word_freqs.items():
            if freq < MIN_PAIR_FREQ:
                continue
            split = self._split_word(word)
            if len(split) >= 2:
                word_splits[word] = (split, freq)

        merges_added = 0

        for _ in range(max_merges):
            if len(self._vocab) >= self._target_vocab_size:
                break

            # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã –ø–∞—Ä
            pair_freqs = Counter()
            for word, (split, freq) in word_splits.items():
                for i in range(len(split) - 1):
                    pair = (split[i], split[i + 1])
                    pair_freqs[pair] += freq

            if not pair_freqs:
                break

            # –ù–∞—Ö–æ–¥–∏–º —Å–∞–º—É—é —á–∞—Å—Ç—É—é –ø–∞—Ä—É
            best_pair = pair_freqs.most_common(1)[0]
            pair, freq = best_pair

            if freq < MIN_PAIR_FREQ:
                break

            token_a, token_b = pair
            new_token = token_a + token_b

            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º merge rule
            self._merges.append(pair)
            now = time.time()

            try:
                self._conn.execute("""
                    INSERT INTO merge_rules (token_a, token_b, merged, frequency, created_at)
                    VALUES (?, ?, ?, ?, ?)
                """, (token_a, token_b, new_token, freq, now))
            except sqlite3.IntegrityError:
                # –ü–∞—Ä–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                continue

            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω –≤ —Å–ª–æ–≤–∞—Ä—å
            if new_token not in self._vocab:
                new_id = max(self._id_to_token.keys()) + 1 if self._id_to_token else 0
                self._vocab[new_token] = new_id
                self._id_to_token[new_id] = new_token
                self._conn.execute("""
                    INSERT OR IGNORE INTO vocabulary (token, token_id, frequency, created_at)
                    VALUES (?, ?, ?, ?)
                """, (new_token, new_id, freq, now))

            # –û–±–Ω–æ–≤–ª—è–µ–º splits –≤—Å–µ—Ö —Å–ª–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö —ç—Ç—É –ø–∞—Ä—É
            for word in list(word_splits.keys()):
                split, wfreq = word_splits[word]
                new_split = self._merge_pair(split, token_a, token_b)
                word_splits[word] = (new_split, wfreq)

            merges_added += 1

        self._conn.commit()
        return merges_added

    def _split_word(self, word: str) -> List[str]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç —Å–ª–æ–≤–æ –Ω–∞ —Ç–æ–∫–µ–Ω—ã —Å —É—á—ë—Ç–æ–º —Ç–µ–∫—É—â–∏—Ö merge rules.
        –ù–∞—á–∏–Ω–∞–µ–º —Å —Å–∏–º–≤–æ–ª–æ–≤, –∑–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω—è–µ–º merges –ø–æ –ø–æ—Ä—è–¥–∫—É.
        """
        # –ù–∞—á–∏–Ω–∞–µ–º —Å –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        tokens = list(word)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤—Å–µ merge rules –ø–æ –ø–æ—Ä—è–¥–∫—É
        for merge_a, merge_b in self._merges:
            tokens = self._merge_pair(tokens, merge_a, merge_b)
            if len(tokens) == 1:
                break

        return tokens

    @staticmethod
    def _merge_pair(tokens: List[str], a: str, b: str) -> List[str]:
        """–°–ª–∏–≤–∞–µ—Ç –≤—Å–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è –ø–∞—Ä—ã (a, b) –≤ —Ç–æ–∫–µ–Ω–∞—Ö"""
        if len(tokens) < 2:
            return tokens

        result = []
        i = 0
        while i < len(tokens):
            if i < len(tokens) - 1 and tokens[i] == a and tokens[i + 1] == b:
                result.append(a + b)
                i += 2
            else:
                result.append(tokens[i])
                i += 1
        return result

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    #               –ö–û–î–ò–†–û–í–ê–ù–ò–ï / –î–ï–ö–û–î–ò–†–û–í–ê–ù–ò–ï
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def encode(self, text: str) -> List[int]:
        """
        –ö–æ–¥–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å token IDs.

        "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä" ‚Üí [234, 56, 78, 11, 345, 67]
        """
        text = text.lower().strip()
        if not text:
            return []

        token_ids = []

        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞ –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
        parts = re.findall(r'[–∞-—è—ëa-z0-9]+|[.!?,;:\-‚Äî()\s]', text)

        for part in parts:
            if not part.strip() and part == " ":
                # –ü—Ä–æ–±–µ–ª –∫–∞–∫ —Ç–æ–∫–µ–Ω
                if " " in self._vocab:
                    token_ids.append(self._vocab[" "])
                continue

            if len(part) == 1 and part in self._vocab:
                token_ids.append(self._vocab[part])
                continue

            # –†–∞–∑–±–∏–≤–∞–µ–º —Å–ª–æ–≤–æ –Ω–∞ BPE-—Ç–æ–∫–µ–Ω—ã
            subtokens = self._split_word(part)
            for st in subtokens:
                if st in self._vocab:
                    token_ids.append(self._vocab[st])
                else:
                    # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø–æ–¥—Ç–æ–∫–µ–Ω ‚Äî —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–∏–º–≤–æ–ª—ã
                    for char in st:
                        if char in self._vocab:
                            token_ids.append(self._vocab[char])
                        else:
                            token_ids.append(SPECIAL_TOKENS["<UNK>"])

        return token_ids

    def encode_with_tokens(self, text: str) -> List[Tuple[str, int]]:
        """
        –ö–æ–¥–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç, –≤–æ–∑–≤—Ä–∞—â–∞—è –ø–∞—Ä—ã (token_text, token_id).
        –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.

        "–ü—Ä–∏–≤–µ—Ç" ‚Üí [("–ø—Ä–∏", 234), ("–≤–µ—Ç", 56)]
        """
        text = text.lower().strip()
        if not text:
            return []

        result = []
        parts = re.findall(r'[–∞-—è—ëa-z0-9]+|[.!?,;:\-‚Äî()\s]', text)

        for part in parts:
            if not part.strip() and part == " ":
                if " " in self._vocab:
                    result.append((" ", self._vocab[" "]))
                continue

            if len(part) == 1 and part in self._vocab:
                result.append((part, self._vocab[part]))
                continue

            subtokens = self._split_word(part)
            for st in subtokens:
                if st in self._vocab:
                    result.append((st, self._vocab[st]))
                else:
                    for char in st:
                        tid = self._vocab.get(char, SPECIAL_TOKENS["<UNK>"])
                        result.append((char, tid))

        return result

    def decode(self, token_ids: List[int]) -> str:
        """
        –î–µ–∫–æ–¥–∏—Ä—É–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å token IDs –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç.

        [234, 56, 78] ‚Üí "–ø—Ä–∏–≤–µ—Ç"
        """
        tokens = []
        for tid in token_ids:
            token = self._id_to_token.get(tid, "")
            if token and token not in SPECIAL_TOKENS:
                tokens.append(token)
        return "".join(tokens)

    def tokenize(self, text: str) -> List[str]:
        """
        –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ (–±–µ–∑ ID).
        –°–æ–≤–º–µ—Å—Ç–∏–º—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —Å NeuralEngine.tokenize().

        "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä" ‚Üí ["–ø—Ä–∏", "–≤–µ—Ç", " ", "–º–∏—Ä"]
        """
        text = text.lower().strip()
        if not text:
            return []

        result = []
        parts = re.findall(r'[–∞-—è—ëa-z0-9]+|[.!?,;:\-‚Äî()\s]', text)

        for part in parts:
            if not part.strip() and part == " ":
                result.append(" ")
                continue

            if len(part) == 1:
                result.append(part)
                continue

            subtokens = self._split_word(part)
            result.extend(subtokens)

        return result

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    #               –£–¢–ò–õ–ò–¢–´
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def get_vocab_size(self) -> int:
        return len(self._vocab)

    def get_token_id(self, token: str) -> Optional[int]:
        return self._vocab.get(token)

    def get_token_by_id(self, token_id: int) -> Optional[str]:
        return self._id_to_token.get(token_id)

    def get_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞"""
        texts_trained = self._conn.execute(
            "SELECT COALESCE(SUM(texts_count), 0) as c FROM training_stats"
        ).fetchone()["c"]

        return {
            "vocab_size": len(self._vocab),
            "merge_rules": len(self._merges),
            "unique_words": len(self._word_freqs),
            "texts_trained": texts_trained,
            "target_vocab_size": self._target_vocab_size,
            "coverage_pct": round(
                len(self._vocab) / self._target_vocab_size * 100, 1
            ),
        }

    def analyze_tokenization(self, text: str) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ ‚Äî –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        - tokens: —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤
        - token_ids: —Å–ø–∏—Å–æ–∫ ID
        - compression_ratio: —Å–∂–∞—Ç–∏–µ (—Å–∏–º–≤–æ–ª—ã / —Ç–æ–∫–µ–Ω—ã)
        - unknown_count: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ <UNK> —Ç–æ–∫–µ–Ω–æ–≤
        """
        pairs = self.encode_with_tokens(text)
        tokens = [t for t, _ in pairs]
        ids = [i for _, i in pairs]
        unknown = sum(1 for i in ids if i == SPECIAL_TOKENS["<UNK>"])

        return {
            "text": text,
            "tokens": tokens,
            "token_ids": ids,
            "num_tokens": len(tokens),
            "num_chars": len(text),
            "compression_ratio": round(len(text) / max(len(tokens), 1), 2),
            "unknown_count": unknown,
            "unknown_pct": round(unknown / max(len(tokens), 1) * 100, 1),
        }

    def close(self):
        self._conn.commit()
        self._conn.close()
