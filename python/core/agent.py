"""
ÐšÑ€Ð¸ÑÑ‚Ð¸Ð½Ð° 6.0 â€” ÐÐ²Ñ‚Ð¾Ð½Ð¾Ð¼Ð½Ñ‹Ð¹ Ð°Ð³ÐµÐ½Ñ‚ Ñ Ð½Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ð¼ tool calling

Ð˜Ð—ÐœÐ•ÐÐ•ÐÐ˜Ð¯ v6.0:
- âœ… ÐÐ°Ñ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ollama tool calling (tools= parameter)
- âœ… Ð£Ð±Ñ€Ð°Ð½Ñ‹ ACTION: / FINAL_ANSWER: / THOUGHT: Ð¼Ð°Ñ€ÐºÐµÑ€Ñ‹
- âœ… Ð£Ð±Ñ€Ð°Ð½ regex-Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³ tool_name("args")
- âœ… Ð£Ð±Ñ€Ð°Ð½Ð° Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚ÑŒ Ð¾Ñ‚ utils/parsers.py
- âœ… ÐœÐ¾Ð´ÐµÐ»ÑŒ ÑÐ°Ð¼Ð° Ñ€ÐµÑˆÐ°ÐµÑ‚ ÐºÐ¾Ð³Ð´Ð° Ð²Ñ‹Ð·Ñ‹Ð²Ð°Ñ‚ÑŒ tool, ÐºÐ¾Ð³Ð´Ð° Ð¾Ñ‚Ð²ÐµÑ‡Ð°Ñ‚ÑŒ
- âœ… Structured output â€” Ollama Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ JSON tool_calls Ð½Ð°Ð¿Ñ€ÑÐ¼ÑƒÑŽ
"""

import asyncio
import hashlib
import re
from typing import Dict, List, Optional, Any
from datetime import datetime
from ollama import AsyncClient

from utils.logging import get_logger
import config

logger = get_logger("agent")


class AgentCore:
    """
    ÐÐ²Ñ‚Ð¾Ð½Ð¾Ð¼Ð½Ñ‹Ð¹ Ð°Ð³ÐµÐ½Ñ‚ Ñ Ð½Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ð¼ tool calling.

    Ð’Ð¼ÐµÑÑ‚Ð¾ ReAct-Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° Ñ‚ÐµÐºÑÑ‚Ð° (ACTION: / FINAL_ANSWER:)
    Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ollama tools= API â€” Ð¼Ð¾Ð´ÐµÐ»ÑŒ ÑÐ°Ð¼Ð° Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚
    ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ tool_calls Ð¸Ð»Ð¸ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚.
    """

    def __init__(self, tools: Dict, memory, identity, vector_memory, thread_memory):
        self.client = AsyncClient()
        self.tools = tools  # {name: callable}
        self.memory = memory
        self.identity = identity
        self.vector_memory = vector_memory
        self.thread_memory = thread_memory
        # v6.0: Ð¡Ñ‚Ñ€Ð¾Ð¸Ð¼ Ollama tool schemas Ð¸Ð· BaseTool.schema
        self._ollama_tools = self._build_ollama_tools()

        # Consciousness-Ð¼Ð¾Ð´ÑƒÐ»Ð¸ (Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÑŽÑ‚ÑÑ Ñ‡ÐµÑ€ÐµÐ· set_consciousness_modules)
        self.vad_emotions = None
        self.self_awareness = None
        self.metacognition = None

        # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°
        self.stats = {
            "total_queries": 0,
            "total_tool_calls": 0,
            "total_errors": 0,
            "cache_hits": 0,
        }

        # ÐšÑÑˆ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð² (asyncio-safe Ñ‡ÐµÑ€ÐµÐ· Lock)
        self.response_cache = {} if config.RESPONSE_CACHE_ENABLED else None
        self._cache_lock = asyncio.Lock()

        logger.info(
            f"ÐÐ³ÐµÐ½Ñ‚ v6.0 (native tool calling): "
            f"{len(self.tools)} Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð², "
            f"{len(self._ollama_tools)} Ð·Ð°Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾"
        )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #                    Ð“Ð›ÐÐ’ÐÐ«Ð™ Ð¦Ð˜ÐšÐ›
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    async def process(self, user_input: str) -> str:
        """ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ"""

        self.stats["total_queries"] += 1
        self._request_tool_calls = 0
        self._request_errors = 0
        logger.info(f"Ð—Ð°Ð¿Ñ€Ð¾Ñ: {user_input[:50]}...")

        self._current_query = user_input

        # ÐšÑÑˆ
        if self.response_cache is not None:
            cached = await self._check_cache(user_input)
            if cached:
                self.stats["cache_hits"] += 1
                return cached

        # Thread
        self._update_thread_context(user_input)

        # === ÐÐÐ¢Ð˜Ð’ÐÐ«Ð™ TOOL CALLING LOOP ===
        final_response = await self._tool_calling_loop(user_input)

        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ
        if self.response_cache is not None:
            await self._save_to_cache(user_input, final_response)

        if self.thread_memory.current_thread:
            self.thread_memory.add_to_thread(user_input, final_response)

        self._save_to_vector_memory(user_input, final_response)

        return final_response

    async def _tool_calling_loop(self, user_input: str) -> str:
        """
        v6.0: ÐÐ°Ñ‚Ð¸Ð²Ð½Ñ‹Ð¹ tool calling loop.

        Ð’Ð¼ÐµÑÑ‚Ð¾ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° ACTION:/FINAL_ANSWER: Ð¸Ð· Ñ‚ÐµÐºÑÑ‚Ð°:
        1. ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ + tools= Ð² Ollama
        2. Ð•ÑÐ»Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð²ÐµÑ€Ð½ÑƒÐ»Ð° tool_calls â†’ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÐ¼, Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾
        3. Ð•ÑÐ»Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð²ÐµÑ€Ð½ÑƒÐ»Ð° Ñ‚ÐµÐºÑÑ‚ â†’ ÑÑ‚Ð¾ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚
        4. ÐŸÐ¾Ð²Ñ‚Ð¾Ñ€ÑÐµÐ¼ Ð´Ð¾ max_iterations
        """

        system_prompt = self._build_system_prompt()

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_input},
        ]

        error_count = 0
        tool_call_count = 0

        for iteration in range(config.AGENT_MAX_ITERATIONS):
            logger.debug(f"Ð˜Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ñ {iteration + 1}/{config.AGENT_MAX_ITERATIONS}")

            try:
                response = await self.client.chat(
                    model=config.MODEL,
                    messages=messages,
                    tools=self._ollama_tools if self._ollama_tools else None,
                    options={
                        "temperature": config.TEMPERATURE,
                        "num_ctx": config.CONTEXT_WINDOW,
                        "num_predict": config.MAX_RESPONSE_TOKENS,
                    },
                )
            except Exception as e:
                logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸: {e}")
                return f"ÐŸÑ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ°: {e}"

            message = response["message"]

            # â”€â”€ ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð²Ñ‹Ð·Ð²Ð°Ð»Ð° Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹? â”€â”€
            tool_calls = message.get("tool_calls")

            if tool_calls:
                # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð² Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ
                messages.append(message)

                # Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ ÐºÐ°Ð¶Ð´Ñ‹Ð¹ tool call
                for tool_call in tool_calls:
                    func_name = tool_call["function"]["name"]
                    func_args = tool_call["function"].get("arguments", {})

                    logger.info(f"ðŸ”§ Tool call: {func_name}({func_args})")

                    result = await self._execute_tool(func_name, func_args)

                    if config.LOG_TOOL_CALLS:
                        logger.info(f"ðŸ“Š Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚: {result[:100]}...")

                    # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ ÐºÐ°Ðº tool response
                    messages.append({
                        "role": "tool",
                        "content": str(result),
                    })

                    tool_call_count += 1
                    self.stats["total_tool_calls"] += 1
                    self._request_tool_calls += 1

                    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¾ÑˆÐ¸Ð±Ð¾Ðº
                    if "ERROR" in str(result):
                        error_count += 1
                        self.stats["total_errors"] += 1
                        self._request_errors += 1

                        if error_count >= config.AGENT_MAX_ERRORS:
                            logger.warning(f"âš ï¸ Ð›Ð¸Ð¼Ð¸Ñ‚ Ð¾ÑˆÐ¸Ð±Ð¾Ðº ({config.AGENT_MAX_ERRORS})")
                            return (
                                "Ð˜Ð·Ð²Ð¸Ð½Ð¸, Ð²Ð¾Ð·Ð½Ð¸ÐºÐ»Ð¸ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ñ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸ÐµÐ¼. "
                                "ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹ Ð¿ÐµÑ€ÐµÑ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð·Ð°Ð¿Ñ€Ð¾Ñ."
                            )

                # ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð°ÐµÐ¼ Ñ†Ð¸ÐºÐ» â€” Ð¼Ð¾Ð´ÐµÐ»ÑŒ ÑƒÐ²Ð¸Ð´Ð¸Ñ‚ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¸ Ñ€ÐµÑˆÐ¸Ñ‚ Ñ‡Ñ‚Ð¾ Ð´Ð°Ð»ÑŒÑˆÐµ
                continue

            else:
                # â”€â”€ ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð²ÐµÑ€Ð½ÑƒÐ»Ð° Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ â†’ ÑÑ‚Ð¾ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ â”€â”€
                final_text = message.get("content", "").strip()

                if final_text:
                    logger.info(
                        f"âœ… ÐžÑ‚Ð²ÐµÑ‚ Ð·Ð° {iteration + 1} Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¹, "
                        f"{tool_call_count} tool calls"
                    )
                    return final_text
                else:
                    # ÐŸÑƒÑÑ‚Ð¾Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ â€” Ð¿Ð¾Ð´Ñ‚Ð°Ð»ÐºÐ¸Ð²Ð°ÐµÐ¼
                    messages.append(message)
                    messages.append({
                        "role": "user",
                        "content": "ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, Ð´Ð°Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ.",
                    })
                    continue

        # Ð›Ð¸Ð¼Ð¸Ñ‚ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¹
        logger.warning("âš ï¸ Ð”Ð¾ÑÑ‚Ð¸Ð³Ð½ÑƒÑ‚ Ð»Ð¸Ð¼Ð¸Ñ‚ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¹")
        return "Ð¥Ð¼, Ñ Ð½ÐµÐ¼Ð½Ð¾Ð³Ð¾ Ð·Ð°Ð¿ÑƒÑ‚Ð°Ð»Ð°ÑÑŒ. ÐœÐ¾Ð¶ÐµÑˆÑŒ Ð¿ÐµÑ€ÐµÑ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾Ñ‰Ðµ?"

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #                  Ð’Ð«ÐŸÐžÐ›ÐÐ•ÐÐ˜Ð• Ð˜ÐÐ¡Ð¢Ð Ð£ÐœÐ•ÐÐ¢ÐžÐ’
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    async def _execute_tool(self, name: str, args: Dict[str, Any]) -> str:
        """
        v6.0: Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ tool call Ð¿Ð¾ Ð¸Ð¼ÐµÐ½Ð¸ Ð¸ kwargs.
        Ð‘Ð¾Ð»ÑŒÑˆÐµ Ð½Ðµ Ð½ÑƒÐ¶ÐµÐ½ parse_tool_call / shlex / regex!
        """

        if name not in self.tools:
            available = ", ".join(self.tools.keys())
            return f"ERROR: Ð˜Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚ '{name}' Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½. Ð”Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹: {available}"

        tool_func = self.tools[name]

        try:
            # Ollama native tool calling Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‘Ñ‚ args ÐºÐ°Ðº dict â†’ kwargs
            if isinstance(args, dict):
                result = await tool_func(**args)
            else:
                result = await tool_func(*args)
            return str(result)
        except Exception as e:
            logger.error(f"Tool error {name}(args={args}): {e}", exc_info=True)
            return f"ERROR: {e}"

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #                  OLLAMA TOOL SCHEMAS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def _build_ollama_tools(self) -> List[Dict]:
        """
        v6.0: ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐµÑ‚ Ð²ÑÐµ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ð² Ollama tool format.

        Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚:
        [
            {"type": "function", "function": {"name": "web_search", ...}},
            ...
        ]
        """
        ollama_tools = []

        for name, tool_func in self.tools.items():
            # ÐŸÑ€Ð¾Ð±ÑƒÐµÐ¼ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ schema Ð¸Ð· BaseTool
            tool_obj = None

            # tool_func Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ bound method (tool.execute)
            if hasattr(tool_func, "__self__"):
                tool_obj = tool_func.__self__

            if tool_obj and hasattr(tool_obj, "schema"):
                schema = tool_obj.schema
                ollama_tool = schema.to_ollama_tool()
                ollama_tools.append(ollama_tool)
            else:
                # Fallback: Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½ÑƒÑŽ schema
                logger.warning(f"âš ï¸ {name}: Ð½ÐµÑ‚ schema, Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸")
                ollama_tools.append({
                    "type": "function",
                    "function": {
                        "name": name,
                        "description": f"Ð˜Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚ {name}",
                        "parameters": {
                            "type": "object",
                            "properties": {},
                            "required": [],
                        },
                    },
                })

        logger.info(f"ðŸ”§ Ð—Ð°Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ {len(ollama_tools)} Ollama tools")
        return ollama_tools

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #                  Ð¡Ð˜Ð¡Ð¢Ð•ÐœÐÐ«Ð™ ÐŸÐ ÐžÐœÐŸÐ¢
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def _build_system_prompt(self) -> str:
        """
        v6.0: Ð£Ð¿Ñ€Ð¾Ñ‰Ñ‘Ð½Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚.
        ÐÐµ Ð½ÑƒÐ¶Ð½Ð¾ Ð¾Ð±ÑŠÑÑÐ½ÑÑ‚ÑŒ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ ACTION:/FINAL_ANSWER: â€”
        Ð¼Ð¾Ð´ÐµÐ»ÑŒ ÑÐ°Ð¼Ð° Ð²Ñ‹Ð·Ñ‹Ð²Ð°ÐµÑ‚ tools Ñ‡ÐµÑ€ÐµÐ· native API.
        """

        # ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð¿Ð°Ð¼ÑÑ‚Ð¸
        memory_context = self._build_memory_context()

        # Thread ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚
        thread_str = ""
        if self.thread_memory.current_thread:
            thread = self.thread_memory.current_thread
            thread_str = f"ðŸ§µ Ð¢ÐµÐºÑƒÑ‰Ð°Ñ Ñ‚ÐµÐ¼Ð°: {thread['topic']}"
            if thread.get('entities'):
                thread_str += f"\n   Ð£Ð¿Ð¾Ð¼Ð¸Ð½Ð°ÐµÑ‚ÑÑ: {', '.join(thread['entities'][:3])}"

        # ÐŸÑ€Ð¸Ð½ÑƒÐ´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ñ€ÑƒÑÑÐºÐ¸Ð¹
        russian_instruction = ""
        if config.FORCE_RUSSIAN_ONLY:
            russian_instruction = (
                "âš ï¸ ÐšÐ Ð˜Ð¢Ð˜Ð§Ð•Ð¡ÐšÐ˜ Ð’ÐÐ–ÐÐž: ÐžÐ¢Ð’Ð•Ð§ÐÐ™ Ð¢ÐžÐ›Ð¬ÐšÐž ÐÐ Ð Ð£Ð¡Ð¡ÐšÐžÐœ Ð¯Ð—Ð«ÐšÐ•!\n"
            )

        # Self-awareness narrative (2.5)
        awareness_str = ""
        if hasattr(self, 'self_awareness') and self.self_awareness:
            narrative = self.self_awareness.get_narrative_summary()
            if narrative:
                awareness_str = f"\n{narrative}"

        # VAD emotional style
        vad_str = ""
        if hasattr(self, 'vad_emotions') and self.vad_emotions:
            style = self.vad_emotions.get_response_style()
            vad_str = f"\nÐ¢Ð¾Ð½: {style.get('tone', 'neutral')} | Ð­Ð¼Ð¾Ñ†Ð¸Ñ: {style.get('emotional_label', 'Ð½ÐµÐ¹Ñ‚Ñ€Ð°Ð»ÑŒÐ½Ð¾')}"

        prompt = f"""{russian_instruction}Ð¢Ñ‹ â€” ÐšÑ€Ð¸ÑÑ‚Ð¸Ð½Ð°, AI-Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚.

{thread_str}

{memory_context}
{awareness_str}

ÐŸÐ ÐÐ’Ð˜Ð›Ð:
1. ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ð¢ÐžÐ›Ð¬ÐšÐž Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼
2. ÐÐ• Ð¿Ñ€Ð¸Ð´ÑƒÐ¼Ñ‹Ð²Ð°Ð¹ Ð´Ð°Ð½Ð½Ñ‹Ðµ â€” Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ°
3. web_search Ð´Ð°Ñ‘Ñ‚ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ URLs â€” Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ web_fetch Ð´Ð»Ñ Ñ‡Ñ‚ÐµÐ½Ð¸Ñ
4. Ð”Ð»Ñ Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ ÐŸÐžÐ›ÐÐ«Ð• Ð¿ÑƒÑ‚Ð¸
5. ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ ÐºÑ€Ð°Ñ‚ÐºÐ¾ Ð¸ Ð¿Ð¾ Ð´ÐµÐ»Ñƒ (3-5 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹)

ÐÐ°ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ: {self.identity.current_mood} | Ð­Ð½ÐµÑ€Ð³Ð¸Ñ: {self.identity.energy_level}%{vad_str}
"""

        return prompt

    def _build_memory_context(self) -> str:
        """ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð¸Ð· Ð¿Ð°Ð¼ÑÑ‚Ð¸"""

        lines = []

        # Ð Ð°Ð±Ð¾Ñ‡Ð°Ñ Ð¿Ð°Ð¼ÑÑ‚ÑŒ
        if self.memory.working:
            recent = []
            for msg in self.memory.working[-5:]:
                content = msg['content'][:80]
                recent.append(f"{msg['role']}: {content}")

            if recent:
                lines.append("â±ï¸ Ð¢Ð•ÐšÐ£Ð©ÐÐ¯ Ð¡Ð•Ð¡Ð¡Ð˜Ð¯:")
                lines.extend(recent)

        # Ð’ÐµÐºÑ‚Ð¾Ñ€Ð½Ð°Ñ Ð¿Ð°Ð¼ÑÑ‚ÑŒ
        if hasattr(self, '_current_query'):
            results = self.vector_memory.search(
                self._current_query,
                n_results=2,
                filter_metadata={"type": "dialogue"},
            )

            old_memories = []
            for r in results:
                ts = r['metadata'].get('timestamp', '')
                if ts:
                    try:
                        age = (datetime.now() - datetime.fromisoformat(ts)).total_seconds() / 60
                    except (ValueError, TypeError):
                        age = config.VECTOR_MIN_AGE_MINUTES + 1  # ÑÑ‡Ð¸Ñ‚Ð°ÐµÐ¼ ÑÑ‚Ð°Ñ€Ñ‹Ð¼
                    if age > config.VECTOR_MIN_AGE_MINUTES:
                        date = r['metadata'].get('date', '')
                        text = r['text'][:60]
                        old_memories.append(f"[{date}] {text}")

            if old_memories:
                lines.append("\nðŸ“š Ð˜Ð· Ð´Ð¾Ð»Ð³Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ Ð¿Ð°Ð¼ÑÑ‚Ð¸:")
                lines.extend(old_memories)

        return "\n".join(lines) if lines else ""

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #                  THREAD / CACHE / STATS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def _update_thread_context(self, user_input: str):
        if not self.thread_memory.is_related_to_thread(user_input):
            topic = self._detect_topic(user_input)
            entities = self._extract_entities(user_input)
            if topic:
                self.thread_memory.start_thread(topic, entities)

    def _detect_topic(self, text: str) -> Optional[str]:
        text_lower = text.lower()
        starters = {
            "Ð´Ð°Ð²Ð°Ð¹ Ð¿Ð¾Ð³Ð¾Ð²Ð¾Ñ€Ð¸Ð¼ Ð¾": "Ð¾Ð±ÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ðµ",
            "Ñ€Ð°ÑÑÐºÐ°Ð¶Ð¸ Ð¾": "Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ",
            "Ð¿Ð¾Ð¼Ð¾Ð³Ð¸ Ñ": "Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒ",
            "Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÑŽ Ð½Ð°Ð´": "Ð¿Ñ€Ð¾ÐµÐºÑ‚",
            "Ð½Ð°Ð¹Ð´Ð¸": "Ð¿Ð¾Ð¸ÑÐº",
        }
        for starter, topic_type in starters.items():
            if starter in text_lower:
                after = text_lower.split(starter)[1].split('.')[0].split('?')[0].strip()
                return f"{topic_type}: {after[:50]}"
        return text[:50] if len(text.split()) > 3 else None

    def _extract_entities(self, text: str) -> List[str]:
        return list(set(re.findall(r'\b[A-ZÐ-Ð¯][a-zÐ°-Ñ]+\b', text)[:3]))

    def _make_cache_key(self, query: str) -> str:
        """ÐšÑÑˆ-ÐºÐ»ÑŽÑ‡ Ð±ÐµÐ· ÐºÐ¾Ð»Ð»Ð¸Ð·Ð¸Ð¹ (hashlib Ð²Ð¼ÐµÑÑ‚Ð¾ truncation)"""
        normalized = query.lower().strip()
        return hashlib.md5(normalized.encode()).hexdigest()

    async def _check_cache(self, query: str) -> Optional[str]:
        async with self._cache_lock:
            cache_key = self._make_cache_key(query)
            if cache_key in self.response_cache:
                cached = self.response_cache[cache_key]
                age = (datetime.now() - cached['timestamp']).total_seconds()
                if age < config.AGENT_CACHE_TTL:
                    return cached['response']
                del self.response_cache[cache_key]
            return None

    async def _save_to_cache(self, query: str, response: str):
        async with self._cache_lock:
            cache_key = self._make_cache_key(query)
            self.response_cache[cache_key] = {
                'response': response,
                'timestamp': datetime.now(),
            }
            if len(self.response_cache) > 100:
                sorted_items = sorted(
                    self.response_cache.items(),
                    key=lambda x: x[1]['timestamp'],
                )
                for key, _ in sorted_items[:20]:
                    del self.response_cache[key]

    def _save_to_vector_memory(self, user_input: str, response: str):
        importance = min(3, 1 + self._request_tool_calls)
        metadata = {
            'tool_calls': self._request_tool_calls,
            'had_errors': self._request_errors > 0,
        }
        if self.thread_memory.current_thread:
            metadata['thread'] = self.thread_memory.current_thread['topic']

        self.vector_memory.add_dialogue(
            user_input=user_input,
            assistant_response=response,
            importance=importance,
            metadata=metadata,
        )

    def get_stats(self) -> Dict:
        return self.stats.copy()

    def reset_stats(self):
        self.stats = {
            "total_queries": 0,
            "total_tool_calls": 0,
            "total_errors": 0,
            "cache_hits": 0,
        }
