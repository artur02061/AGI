"""
–ö—Ä–∏—Å—Ç–∏–Ω–∞ 7.5 ‚Äî DialogueMemory (–ë–µ–∑–ª–∏–º–∏—Ç–Ω–∞—è –ø–∞–º—è—Ç—å –¥–∏–∞–ª–æ–≥–∞)

–ü–†–û–ë–õ–ï–ú–ê:
  –†–∞–Ω—å—à–µ –ö—Ä–∏—Å—Ç–∏–Ω–∞ –ø–æ–º–Ω–∏–ª–∞ —Ç–æ–ª—å–∫–æ 3 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.
  –ß–µ—Ä–µ–∑ 10 –º–∏–Ω—É—Ç ‚Äî –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π —Å–±—Ä–æ—Å —Ç–µ–º—ã.
  –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≥–æ–≤–æ—Ä–∏—Ç "–∫–∞–∫ —è –≥–æ–≤–æ—Ä–∏–ª –≤ –Ω–∞—á–∞–ª–µ" ‚Äî –∞ –ö—Ä–∏—Å—Ç–∏–Ω–∞ –Ω–µ –ø–æ–º–Ω–∏—Ç.

–†–ï–®–ï–ù–ò–ï ‚Äî 3 –º–µ—Ö–∞–Ω–∏–∑–º–∞:

  1. SlidingSummary (–°–∫–æ–ª—å–∑—è—â–µ–µ —Ä–µ–∑—é–º–µ)
     –ü–æ—Å–ª–µ –∫–∞–∂–¥—ã—Ö N —Å–æ–æ–±—â–µ–Ω–∏–π —Å—Ç–∞—Ä–∞—è —á–∞—Å—Ç—å —Å–∂–∏–º–∞–µ—Ç—Å—è –≤ —Ä–µ–∑—é–º–µ.
     –†–µ–∑—é–º–µ —Ä–∞—Å—Ç—ë—Ç –õ–û–ì–ê–†–ò–§–ú–ò–ß–ï–°–ö–ò ‚Äî 10 –∏ 1000 —Å–æ–æ–±—â–µ–Ω–∏–π –¥–∞—é—Ç
     –ø—Ä–∏–º–µ—Ä–Ω–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä (~500 —Ç–æ–∫–µ–Ω–æ–≤).

  2. SessionIndex (–ò–Ω–¥–µ–∫—Å —Å–µ—Å—Å–∏–∏)
     –ö–∞–∂–¥–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Ö—Ä–∞–Ω–∏—Ç—Å—è —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–º –≤ RAM.
     –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –õ–Æ–ë–û–ú–£ –º–æ–º–µ–Ω—Ç—É —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ –∑–∞ O(N).
     –ü—Ä–∏ 1000 —Å–æ–æ–±—â–µ–Ω–∏–π √ó 128 dim = ~500 KB ‚Äî –Ω–∏—á—Ç–æ–∂–Ω–æ.

  3. KeyFacts (–ö–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã)
     –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º—ë–Ω, —á–∏—Å–µ–ª, —Ä–µ—à–µ–Ω–∏–π, —Ç–µ–º.
     –§–∞–∫—Ç—ã –Ω–µ —Å–∂–∏–º–∞—é—Ç—Å—è –∏ –∂–∏–≤—É—Ç –≤—Å—é —Å–µ—Å—Å–∏—é.

–†–ï–ó–£–õ–¨–¢–ê–¢:
  - –ö—Ä–∏—Å—Ç–∏–Ω–∞ –ø–æ–º–Ω–∏—Ç –í–°–Å –∏–∑ —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏
  - –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM: ~1800 —Ç–æ–∫–µ–Ω–æ–≤ (–±—ã–ª–æ ~200-300)
  - –ü–∞–º—è—Ç—å –±–µ–∑–ª–∏–º–∏—Ç–Ω–∞ (—Ä–µ–∑—é–º–µ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ)
"""

import math
import re
import time
import threading
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field

from utils.logging import get_logger
import config

logger = get_logger("dialogue_memory")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#               –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def _cosine_similarity(v1: List[float], v2: List[float]) -> float:
    """–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–≤—É—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤"""
    if not v1 or not v2 or len(v1) != len(v2):
        return 0.0
    dot = sum(a * b for a, b in zip(v1, v2))
    norm1 = math.sqrt(sum(a * a for a in v1))
    norm2 = math.sqrt(sum(b * b for b in v2))
    if norm1 < 1e-10 or norm2 < 1e-10:
        return 0.0
    return dot / (norm1 * norm2)


def _estimate_tokens(text: str) -> int:
    """–û—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ (heuristic)"""
    if not text:
        return 0
    ascii_chars = sum(1 for c in text if ord(c) < 128)
    cyrillic_chars = len(text) - ascii_chars
    return ascii_chars // 4 + cyrillic_chars // 2


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#               –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ö–õ–Æ–ß–ï–í–´–• –§–ê–ö–¢–û–í
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ñ–∞–∫—Ç–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞
_FACT_PATTERNS = [
    # –ò–º–µ–Ω–∞ (—Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã –ø–æ—Å–ª–µ "–º–µ–Ω—è –∑–æ–≤—É—Ç", "—è ‚Äî" –∏ —Ç.–¥.)
    (r'(?:–º–µ–Ω—è –∑–æ–≤—É—Ç|—è\s+‚Äî|—è\s*-)\s+([–ê-–Ø–ÅA-Z][–∞-—è—ëa-z]+)', 'name'),
    # –ß–∏—Å–ª–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
    (r'(\d+)\s*(?:–ª–µ—Ç|–≥–æ–¥–∞|–≥–æ–¥)', 'age'),
    (r'(\d+)\s*(?:—Ä—É–±–ª–µ–π|—Ä—É–±|‚ÇΩ|\$|–¥–æ–ª–ª–∞—Ä–æ–≤|–µ–≤—Ä–æ|‚Ç¨)', 'money'),
    # –†–µ—à–µ–Ω–∏—è / –≤—ã–≤–æ–¥—ã
    (r'(?:—Ä–µ—à–∏–ª–∏?|–¥–æ–≥–æ–≤–æ—Ä–∏–ª–∏—Å—å|–∏—Ç–æ–≥–æ|–≤—ã–≤–æ–¥)[:\s]+(.{10,80})', 'decision'),
    # –ì–æ—Ä–æ–¥–∞ / —Å—Ç—Ä–∞–Ω—ã (–ø–æ—Å–ª–µ "–∂–∏–≤—É –≤", "–∏–∑")
    (r'(?:–∂–∏–≤—É –≤|–∏–∑|–≤ –≥–æ—Ä–æ–¥–µ)\s+([–ê-–Ø–ÅA-Z][–∞-—è—ëa-z]+)', 'location'),
    # –ü—Ä–æ—Ñ–µ—Å—Å–∏—è
    (r'(?:—Ä–∞–±–æ—Ç–∞—é|—è\s+(?:–ø–æ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏|–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç|–¥–∏–∑–∞–π–Ω–µ—Ä|–∏–Ω–∂–µ–Ω–µ—Ä|—É—á–∏—Ç–µ–ª—å|–≤—Ä–∞—á|—Å—Ç—É–¥–µ–Ω—Ç))\s*([^\.,]{3,40})', 'profession'),
]

_ANAPHORA_PATTERNS = [
    "–∫–∞–∫ —è –≥–æ–≤–æ—Ä–∏–ª", "–∫–∞–∫ –º—ã –æ–±—Å—É–∂–¥–∞–ª–∏", "–ø–æ–º–Ω–∏—à—å",
    "–≤ –Ω–∞—á–∞–ª–µ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞", "—Ä–∞–Ω—å—à–µ —è", "—Ä–∞–Ω–µ–µ",
    "–≤–µ—Ä–Ω—ë–º—Å—è –∫", "–Ω–∞—Å—á—ë—Ç —Ç–æ–≥–æ", "–ø–æ –ø–æ–≤–æ–¥—É",
    "–æ–± —ç—Ç–æ–º –∂–µ", "–ø—Ä–æ–¥–æ–ª–∂–∏–º", "–∫–∞–∫ —è —É–∂–µ",
    "–º—ã —É–∂–µ", "—è —É–∂–µ –≥–æ–≤–æ—Ä–∏–ª", "—Ç—ã —É–∂–µ",
]


def _extract_facts(text: str) -> List[Dict]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    facts = []
    for pattern, fact_type in _FACT_PATTERNS:
        for match in re.finditer(pattern, text, re.IGNORECASE):
            facts.append({
                'type': fact_type,
                'value': match.group(1).strip() if match.lastindex else match.group(0).strip(),
                'source': text[:60],
            })
    return facts


def _has_anaphora(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∞–Ω–∞—Ñ–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å—Å—ã–ª–æ–∫ (–æ—Ç—Å—ã–ª–∫–∏ –∫ –ø—Ä–æ—à–ª–æ–º—É)"""
    text_lower = text.lower()
    return any(p in text_lower for p in _ANAPHORA_PATTERNS)


def _extractive_summarize(messages: List[Dict], max_chars: int = 600) -> str:
    """
    Extractive summarization –ë–ï–ó LLM.

    –ë–µ—Ä—ë–º —Å–∞–º—ã–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è:
    - –° –∏–º–µ–Ω–∞–º–∏, —á–∏—Å–ª–∞–º–∏, —Ä–µ—à–µ–Ω–∏—è–º–∏
    - –° –≤–æ–ø—Ä–æ—Å–∞–º–∏ (?)
    - –ü–µ—Ä–≤–æ–µ –∏ –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å–µ—Å—Å–∏–∏
    """
    if not messages:
        return ""

    scored_sentences = []

    for msg in messages:
        text = msg.get('user', '') or ''
        if not text:
            continue

        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
        score = 0.0

        # –ß–∏—Å–ª–∞ ‚Äî –≤–∞–∂–Ω–æ
        if re.search(r'\d+', text):
            score += 2.0

        # –ò–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ (—Å–ª–æ–≤–∞ —Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã)
        caps = re.findall(r'[–ê-–Ø–ÅA-Z][–∞-—è—ëa-z]{2,}', text)
        score += len(caps) * 1.5

        # –í–æ–ø—Ä–æ—Å—ã ‚Äî –≤–∞–∂–Ω–æ
        if '?' in text:
            score += 1.5

        # –†–µ—à–µ–Ω–∏—è / –≤—ã–≤–æ–¥—ã
        if any(w in text.lower() for w in ['—Ä–µ—à–∏–ª–∏', '–∏—Ç–æ–≥–æ', '–≤—ã–≤–æ–¥', '–¥–æ–≥–æ–≤–æ—Ä–∏–ª–∏—Å—å', '–Ω—É–∂–Ω–æ', '–ø–ª–∞–Ω']):
            score += 3.0

        # –î–ª–∏–Ω–∞ (—Å—Ä–µ–¥–Ω—è—è –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–µ–µ)
        if 20 < len(text) < 200:
            score += 1.0

        scored_sentences.append((score, text[:150], msg.get('role', 'user')))

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å–∫–æ—Ä—É, –±–µ—Ä—ë–º –ª—É—á—à–∏–µ
    scored_sentences.sort(key=lambda x: x[0], reverse=True)

    parts = []
    total_chars = 0
    for score, text, role in scored_sentences:
        if total_chars + len(text) > max_chars:
            break
        prefix = "–ü" if role == "user" else "–ö"
        parts.append(f"{prefix}: {text}")
        total_chars += len(text) + 3

    if not parts:
        # Fallback: –±–µ—Ä—ë–º –ø–µ—Ä–≤–æ–µ –∏ –ø–æ—Å–ª–µ–¥–Ω–µ–µ
        if messages:
            first = messages[0].get('user', '')[:100]
            if first:
                parts.append(f"–ù–∞—á–∞–ª–∏ —Å: {first}")
            if len(messages) > 1:
                last = messages[-1].get('user', '')[:100]
                if last:
                    parts.append(f"–ü–æ—Å–ª–µ–¥–Ω–µ–µ: {last}")

    return "; ".join(parts)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#               SESSION MESSAGE
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@dataclass
class SessionMessage:
    """–û–¥–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ —Å–µ—Å—Å–∏–∏"""
    role: str              # 'user' –∏–ª–∏ 'assistant'
    text: str              # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è
    embedding: Optional[List[float]]  # –í–µ–∫—Ç–æ—Ä (128-dim)
    timestamp: datetime
    index: int             # –ü–æ—Ä—è–¥–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä –≤ —Å–µ—Å—Å–∏–∏
    facts: List[Dict] = field(default_factory=list)  # –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#               SLIDING SUMMARY
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class SlidingSummary:
    """
    –°–∫–æ–ª—å–∑—è—â–µ–µ —Ä–µ–∑—é–º–µ –¥–∏–∞–ª–æ–≥–∞.

    –ö–∞–∂–¥—ã–µ `window_size` —Å–æ–æ–±—â–µ–Ω–∏–π —Å—Ç–∞—Ä–∞—è —á–∞—Å—Ç—å —Å–∂–∏–º–∞–µ—Ç—Å—è –≤ —Ä–µ–∑—é–º–µ.
    –†–µ–∑—é–º–µ —Ä–∞—Å—Ç—ë—Ç –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏:
      - –ü–µ—Ä–≤–æ–µ —Å–∂–∞—Ç–∏–µ: 6 —Å–æ–æ–±—â–µ–Ω–∏–π ‚Üí 150 —Å–ª–æ–≤
      - –í—Ç–æ—Ä–æ–µ: —Ä–µ–∑—é–º–µ + 6 —Å–æ–æ–±—â–µ–Ω–∏–π ‚Üí 200 —Å–ª–æ–≤
      - –¢—Ä–µ—Ç—å–µ: —Ä–µ–∑—é–º–µ + 6 —Å–æ–æ–±—â–µ–Ω–∏–π ‚Üí 230 —Å–ª–æ–≤ (–Ω–µ 300!)
    """

    def __init__(
        self,
        window_size: int = 6,
        max_summary_tokens: int = 500,
        llm_summarizer=None,
    ):
        self.window_size = window_size
        self.max_summary_tokens = max_summary_tokens
        self._llm_summarizer = llm_summarizer  # async callable –∏–ª–∏ None

        self.summary_text: str = ""
        self.summary_tokens: int = 0
        self.topic_history: List[str] = []
        self.compression_count: int = 0
        self.total_messages_compressed: int = 0

    def needs_compression(self, n_recent_messages: int) -> bool:
        """–ü–æ—Ä–∞ –ª–∏ —Å–∂–∏–º–∞—Ç—å?"""
        return n_recent_messages >= self.window_size

    async def compress(self, messages: List[Dict]) -> str:
        """
        –°–∂–∏–º–∞–µ—Ç messages –≤ —Ä–µ–∑—é–º–µ.

        Args:
            messages: [{'role': 'user'/'assistant', 'text': '...'}]

        Returns:
            –û–±–Ω–æ–≤–ª—ë–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ
        """
        if not messages:
            return self.summary_text

        self.compression_count += 1
        self.total_messages_compressed += len(messages)

        # –ü—Ä–æ–±—É–µ–º LLM-—Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é
        if self._llm_summarizer:
            try:
                new_summary = await self._llm_compress(messages)
                if new_summary and len(new_summary) > 20:
                    self.summary_text = new_summary
                    self.summary_tokens = _estimate_tokens(new_summary)
                    logger.info(
                        f"üìù SlidingSummary: LLM compress #{self.compression_count} "
                        f"({len(messages)} msgs ‚Üí {self.summary_tokens} tokens)"
                    )
                    return self.summary_text
            except Exception as e:
                logger.debug(f"LLM summarization failed, using extractive: {e}")

        # Fallback: extractive summarization
        new_summary = self._extractive_compress(messages)
        self.summary_text = new_summary
        self.summary_tokens = _estimate_tokens(new_summary)

        logger.info(
            f"üìù SlidingSummary: extractive compress #{self.compression_count} "
            f"({len(messages)} msgs ‚Üí {self.summary_tokens} tokens)"
        )
        return self.summary_text

    async def _llm_compress(self, messages: List[Dict]) -> str:
        """–°–∂–∏–º–∞–µ—Ç —á–µ—Ä–µ–∑ LLM (gemma3:4b)"""
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è LLM
        dialogue_parts = []
        for msg in messages:
            role = "–ü" if msg['role'] == 'user' else "–ö"
            dialogue_parts.append(f"{role}: {msg['text'][:200]}")
        dialogue_text = "\n".join(dialogue_parts)

        prev_context = ""
        if self.summary_text:
            prev_context = f"\n–ü–†–ï–î–´–î–£–©–ï–ï –†–ï–ó–Æ–ú–ï:\n{self.summary_text}\n"

        prompt = f"""–°–æ–∂–º–∏ –¥–∏–∞–ª–æ–≥ –≤ –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ (–º–∞–∫—Å 4-5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π).
{prev_context}
–ü–†–ê–í–ò–õ–ê:
- –°–æ—Ö—Ä–∞–Ω–∏ –í–°–ï —Ñ–∞–∫—Ç—ã: –∏–º–µ–Ω–∞, —á–∏—Å–ª–∞, —Ä–µ—à–µ–Ω–∏—è, –ø—Ä–æ—Å—å–±—ã
- –û–±—ä–µ–¥–∏–Ω–∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–µ —Ä–µ–∑—é–º–µ —Å –Ω–æ–≤—ã–º–∏ —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
- –ü–∏—à–∏ –∫—Ä–∞—Ç–∫–æ, –ø–æ –¥–µ–ª—É
- –ù–ï –¥–æ–±–∞–≤–ª—è–π –Ω–∏—á–µ–≥–æ –æ—Ç —Å–µ–±—è

–ù–û–í–´–ï –°–û–û–ë–©–ï–ù–ò–Ø:
{dialogue_text}

–†–ï–ó–Æ–ú–ï:"""

        return await self._llm_summarizer(prompt)

    def _extractive_compress(self, messages: List[Dict]) -> str:
        """Extractive compression –±–µ–∑ LLM"""
        msg_dicts = [
            {'user': m['text'], 'role': m['role']}
            for m in messages
        ]
        new_part = _extractive_summarize(msg_dicts, max_chars=400)

        if self.summary_text:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç–∞—Ä–æ–µ —Ä–µ–∑—é–º–µ + –Ω–æ–≤–æ–µ
            # –ù–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â–∏–π —Ä–∞–∑–º–µ—Ä
            max_old = max(200, self.max_summary_tokens * 2 - len(new_part))
            old_trimmed = self.summary_text[:max_old]
            return f"{old_trimmed} | {new_part}"
        else:
            return new_part

    def get_summary(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–µ–µ —Ä–µ–∑—é–º–µ"""
        return self.summary_text


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#               SESSION INDEX (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class SessionIndex:
    """
    In-memory –∏–Ω–¥–µ–∫—Å –≤—Å–µ—Ö —Å–æ–æ–±—â–µ–Ω–∏–π —Å–µ—Å—Å–∏–∏ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏.

    –ü–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞–π—Ç–∏ –ª—é–±–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ —Å–µ–º–∞–Ω—Ç–∏–∫–µ:
    "–Ω–∞–π–¥–∏ –≥–¥–µ –º—ã –≥–æ–≤–æ—Ä–∏–ª–∏ –æ Python" ‚Üí –Ω–∞—Ö–æ–¥–∏—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –ø—Ä–æ Python

    –ü–∞–º—è—Ç—å: 1000 —Å–æ–æ–±—â–µ–Ω–∏–π √ó 128 dim √ó 4 bytes = ~500 KB
    –°–∫–æ—Ä–æ—Å—Ç—å: cosine similarity –¥–ª—è 1000 –≤–µ–∫—Ç–æ—Ä–æ–≤ < 1ms
    """

    def __init__(self, sentence_encoder=None):
        self._messages: List[SessionMessage] = []
        self._encoder = sentence_encoder  # SentenceEmbeddings.encode()
        self._msg_counter = 0

    def add(self, role: str, text: str) -> SessionMessage:
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –∏–Ω–¥–µ–∫—Å"""
        embedding = None
        if self._encoder:
            try:
                embedding = self._encoder(text)
            except Exception as e:
                logger.debug(f"Embedding encode failed: {e}")

        facts = _extract_facts(text)

        msg = SessionMessage(
            role=role,
            text=text,
            embedding=embedding,
            timestamp=datetime.now(),
            index=self._msg_counter,
            facts=facts,
        )

        self._messages.append(msg)
        self._msg_counter += 1
        return msg

    def search(self, query: str, top_k: int = 3, min_score: float = 0.3) -> List[Tuple[SessionMessage, float]]:
        """
        –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è–º —Å–µ—Å—Å–∏–∏.

        Returns:
            [(SessionMessage, score), ...] –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —Å–∫–æ—Ä—É
        """
        if not self._messages or not self._encoder:
            return []

        try:
            query_embedding = self._encoder(query)
        except Exception as e:
            logger.debug(f"Query embedding failed: {e}")
            return []

        if not query_embedding:
            return []

        scored = []
        for msg in self._messages:
            if msg.embedding is None:
                continue

            score = _cosine_similarity(query_embedding, msg.embedding)

            # –ë–æ–Ω—É—Å –∑–∞ —Å–≤–µ–∂–µ—Å—Ç—å (–Ω–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è —á—É—Ç—å –≤–∞–∂–Ω–µ–µ)
            recency_bonus = 0.05 * (msg.index / max(self._msg_counter, 1))
            score += recency_bonus

            if score >= min_score:
                scored.append((msg, score))

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å–∫–æ—Ä—É (—É–±—ã–≤–∞–Ω–∏–µ)
        scored.sort(key=lambda x: x[1], reverse=True)
        return scored[:top_k]

    def search_by_facts(self, fact_type: str = None) -> List[SessionMessage]:
        """–ü–æ–∏—Å–∫ –ø–æ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–º —Ñ–∞–∫—Ç–∞–º"""
        results = []
        for msg in self._messages:
            for fact in msg.facts:
                if fact_type is None or fact['type'] == fact_type:
                    results.append(msg)
                    break
        return results

    def get_all(self) -> List[SessionMessage]:
        return list(self._messages)

    def get_recent(self, n: int) -> List[SessionMessage]:
        return self._messages[-n:] if self._messages else []

    @property
    def size(self) -> int:
        return len(self._messages)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#               DIALOGUE MEMORY (–æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π —Ñ–∞—Å–∞–¥)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class DialogueMemory:
    """
    –ë–µ–∑–ª–∏–º–∏—Ç–Ω–∞—è –ø–∞–º—è—Ç—å –¥–∏–∞–ª–æ–≥–∞.

    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç:
    - SlidingSummary: —Å–∫–æ–ª—å–∑—è—â–µ–µ —Ä–µ–∑—é–º–µ –≤—Å–µ–π —Å–µ—Å—Å–∏–∏
    - SessionIndex: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –ª—é–±–æ–º—É —Å–æ–æ–±—â–µ–Ω–∏—é
    - KeyFacts: –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã (–∏–º–µ–Ω–∞, —á–∏—Å–ª–∞, —Ä–µ—à–µ–Ω–∏—è)

    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
        dm = DialogueMemory(sentence_encoder=embeddings.encode)
        dm.add('user', '–ü—Ä–∏–≤–µ—Ç, –º–µ–Ω—è –∑–æ–≤—É—Ç –ê—Ä—Ç—É—Ä')
        dm.add('assistant', '–ü—Ä–∏–≤–µ—Ç, –ê—Ä—Ç—É—Ä!')
        ...
        context = await dm.build_context('–Ω–∞–ø–æ–º–Ω–∏ –∫–∞–∫ –º–µ–Ω—è –∑–æ–≤—É—Ç')
    """

    def __init__(
        self,
        sentence_encoder=None,
        llm_summarizer=None,
        window_size: int = None,
        max_summary_tokens: int = None,
        session_search_top_k: int = None,
        session_search_threshold: float = None,
    ):
        cfg = config.config

        _window = window_size or getattr(cfg, 'sliding_summary_window', 6)
        _max_tokens = max_summary_tokens or getattr(cfg, 'sliding_summary_max_tokens', 500)
        _top_k = session_search_top_k or getattr(cfg, 'session_search_top_k', 3)
        _threshold = session_search_threshold or getattr(cfg, 'session_search_threshold', 0.3)

        self._summary = SlidingSummary(
            window_size=_window,
            max_summary_tokens=_max_tokens,
            llm_summarizer=llm_summarizer,
        )

        self._index = SessionIndex(sentence_encoder=sentence_encoder)
        self._search_top_k = _top_k
        self._search_threshold = _threshold

        # –ë—É—Ñ–µ—Ä —Å–≤–µ–∂–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π (–ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç, –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç)
        self._recent_buffer: List[Dict] = []
        self._recent_max = _window  # –°—Ç–æ–ª—å–∫–æ –∂–µ, —Å–∫–æ–ª—å–∫–æ –æ–∫–Ω–æ —Å–∂–∞—Ç–∏—è

        # –í—Å–µ —Ñ–∞–∫—Ç—ã —Å–µ—Å—Å–∏–∏
        self._session_facts: List[Dict] = []

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self._total_messages = 0
        self._total_compressions = 0
        self._total_searches = 0

        self._lock = threading.Lock()

        logger.info(
            f"üß† DialogueMemory: window={_window}, "
            f"max_summary={_max_tokens}tok, "
            f"search_top_k={_top_k}"
        )

    def add(self, role: str, text: str):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –ø–∞–º—è—Ç—å –¥–∏–∞–ª–æ–≥–∞.

        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –∏–∑ orchestrator._save_to_memory() –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è.
        """
        with self._lock:
            self._total_messages += 1

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏–Ω–¥–µ–∫—Å (—Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–º)
            msg = self._index.add(role, text)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–∫—Ç—ã
            if msg.facts:
                self._session_facts.extend(msg.facts)

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±—É—Ñ–µ—Ä —Å–≤–µ–∂–∏—Ö
            self._recent_buffer.append({
                'role': role,
                'text': text,
                'timestamp': datetime.now(),
            })

    async def maybe_compress(self):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ —Å–∂–∞—Ç–∏–µ, –∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –µ–≥–æ.
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ add() –∏–∑ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞.
        """
        with self._lock:
            needs = self._summary.needs_compression(len(self._recent_buffer))

        if not needs:
            return

        with self._lock:
            # –ó–∞–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ä—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ –±—É—Ñ–µ—Ä–∞, –æ—Å—Ç–∞–≤–ª—è—è –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2
            to_compress = self._recent_buffer[:-2] if len(self._recent_buffer) > 2 else []
            if to_compress:
                self._recent_buffer = self._recent_buffer[-2:]
                self._total_compressions += 1

        if to_compress:
            await self._summary.compress(to_compress)

    def search_session(self, query: str) -> List[Tuple[SessionMessage, float]]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è–º —Å–µ—Å—Å–∏–∏"""
        self._total_searches += 1
        return self._index.search(
            query,
            top_k=self._search_top_k,
            min_score=self._search_threshold,
        )

    def get_recent_messages(self, n: int = 6) -> List[Dict]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Å–æ–æ–±—â–µ–Ω–∏–π (–ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç)"""
        with self._lock:
            return list(self._recent_buffer[-n:])

    def get_summary(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–µ–µ —Å–∫–æ–ª—å–∑—è—â–µ–µ —Ä–µ–∑—é–º–µ"""
        return self._summary.get_summary()

    def get_session_facts(self) -> List[Dict]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã —Å–µ—Å—Å–∏–∏"""
        return list(self._session_facts)

    def has_anaphora(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø—Ä–æ—à–ª–æ–µ –≤ —Ç–µ–∫—Å—Ç–µ"""
        return _has_anaphora(text)

    async def build_context(self, user_input: str, max_tokens: int = 1800) -> str:
        """
        –°—Ç—Ä–æ–∏—Ç –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–∞–º—è—Ç–∏.

        –ë—é–¥–∂–µ—Ç ~1800 —Ç–æ–∫–µ–Ω–æ–≤:
          - –°–∫–æ–ª—å–∑—è—â–µ–µ —Ä–µ–∑—é–º–µ: ~500 —Ç–æ–∫–µ–Ω–æ–≤
          - –§–∞–∫—Ç—ã —Å–µ—Å—Å–∏–∏: ~100 —Ç–æ–∫–µ–Ω–æ–≤
          - –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è (–ø–æ–∏—Å–∫): ~300 —Ç–æ–∫–µ–Ω–æ–≤
          - –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è (–ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç): ~600 —Ç–æ–∫–µ–Ω–æ–≤
          - –ó–∞–ø–∞—Å: ~300 —Ç–æ–∫–µ–Ω–æ–≤

        Returns:
            –°—Ç—Ä–æ–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è LLM
        """
        parts = []
        used_tokens = 0

        # 1. –°–∫–æ–ª—å–∑—è—â–µ–µ —Ä–µ–∑—é–º–µ —Å–µ—Å—Å–∏–∏
        summary = self.get_summary()
        if summary:
            summary_tokens = _estimate_tokens(summary)
            budget = min(summary_tokens, 500)
            trimmed = summary[:budget * 3]  # ~3 chars per token
            parts.append(f"[–ö–æ–Ω—Ç–µ–∫—Å—Ç —Å–µ—Å—Å–∏–∏ ({self._total_messages} —Å–æ–æ–±—â–µ–Ω–∏–π)]: {trimmed}")
            used_tokens += _estimate_tokens(trimmed)

        # 2. –ö–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã
        facts = self.get_session_facts()
        if facts:
            unique_facts = {}
            for f in facts:
                key = f"{f['type']}:{f['value']}"
                unique_facts[key] = f
            fact_strs = [f"{f['type']}: {f['value']}" for f in unique_facts.values()]
            facts_text = "; ".join(fact_strs[:10])  # –ú–∞–∫—Å–∏–º—É–º 10 —Ñ–∞–∫—Ç–æ–≤
            if _estimate_tokens(facts_text) < 150:
                parts.append(f"[–§–∞–∫—Ç—ã]: {facts_text}")
                used_tokens += _estimate_tokens(facts_text)

        # 3. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ —Å–µ—Å—Å–∏–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø—Ä–æ—à–ª–æ–µ)
        if self._index.size > 3:
            # –í—Å–µ–≥–¥–∞ –∏—â–µ–º –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–Ω–µ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –∞–Ω–∞—Ñ–æ—Ä–µ)
            search_results = self.search_session(user_input)
            if search_results:
                search_parts = []
                for msg, score in search_results:
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–≤—Å–µ–º –Ω–µ–¥–∞–≤–Ω–∏–µ (–æ–Ω–∏ –∏ —Ç–∞–∫ –≤ recent)
                    if msg.index >= self._total_messages - 4:
                        continue
                    role = "–ü" if msg.role == "user" else "–ö"
                    snippet = msg.text[:150]
                    search_parts.append(f"  [{msg.index}] {role}: {snippet}")

                if search_parts:
                    relevance_text = "\n".join(search_parts)
                    if used_tokens + _estimate_tokens(relevance_text) < max_tokens - 600:
                        parts.append(f"[–ò–∑ —Ä–∞–Ω–µ–µ –≤ —Ä–∞–∑–≥–æ–≤–æ—Ä–µ]:\n{relevance_text}")
                        used_tokens += _estimate_tokens(relevance_text)

        # 4. –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è (–ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç ‚Äî —Å–∞–º—ã–π –≤–∞–∂–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç)
        recent = self.get_recent_messages(n=6)
        if recent:
            recent_parts = []
            for msg in recent:
                role = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å" if msg['role'] == 'user' else "–ö—Ä–∏—Å—Ç–∏–Ω–∞"
                text = msg['text'][:250]
                recent_parts.append(f"{role}: {text}")
            recent_text = "\n".join(recent_parts)
            parts.append(recent_text)

        return "\n\n".join(parts) if parts else ""

    def get_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–æ–¥—É–ª—è"""
        return {
            'total_messages': self._total_messages,
            'index_size': self._index.size,
            'summary_tokens': self._summary.summary_tokens,
            'compressions': self._total_compressions,
            'searches': self._total_searches,
            'facts_count': len(self._session_facts),
            'recent_buffer': len(self._recent_buffer),
        }
