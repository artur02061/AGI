"""
–ö—Ä–∏—Å—Ç–∏–Ω–∞ 7.4 ‚Äî MicroTransformer (–ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è)

–≠–¢–û –ö–í–ê–ù–¢–û–í–´–ô –°–ö–ê–ß–û–ö.

–ó–ê–ß–ï–ú:
  N-gram –≤–∏–¥–∏—Ç 2-3 —Å–ª–æ–≤–∞. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –≤–∏–¥–∏—Ç –í–°–ï —Å–ª–æ–≤–∞ —Å—Ä–∞–∑—É
  –∏ –ø–æ–Ω–∏–º–∞–µ—Ç –°–í–Ø–ó–ò –º–µ–∂–¥—É –Ω–∏–º–∏.

  "–ë–∞–Ω–∫ —Å—Ç–æ–∏—Ç –Ω–∞ –±–µ—Ä–µ–≥—É —Ä–µ–∫–∏" ‚Üí –±–∞–Ω–∫ = –∑–¥–∞–Ω–∏–µ (attention –Ω–∞ "–±–µ—Ä–µ–≥—É", "—Ä–µ–∫–∏")
  "–ë–∞–Ω–∫ –≤—ã–¥–∞–ª –∫—Ä–µ–¥–∏—Ç"         ‚Üí –±–∞–Ω–∫ = —Ñ–∏–Ω–∞–Ω—Å—ã (attention –Ω–∞ "–∫—Ä–µ–¥–∏—Ç")

–ê–†–•–ò–¢–ï–ö–¢–£–†–ê (Decoder-only, LLaMA-—Å—Ç–∏–ª—å):
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ Input: BPE token IDs               ‚îÇ
  ‚îÇ [23, 45, 67, 89, ...]              ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚Üì
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ Token Embedding + Positional (RoPE) ‚îÇ
  ‚îÇ [0.12, -0.34, 0.56, ...]  per token‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚Üì
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  √óN layers
  ‚îÇ RMSNorm                             ‚îÇ
  ‚îÇ         ‚Üì                           ‚îÇ
  ‚îÇ Multi-Head Self-Attention           ‚îÇ
  ‚îÇ   Q = X @ Wq, K = X @ Wk, V = X @ Wv‚îÇ
  ‚îÇ   Attn = softmax(Q @ K.T / ‚àöd) @ V ‚îÇ
  ‚îÇ         ‚Üì                           ‚îÇ
  ‚îÇ Residual + RMSNorm                  ‚îÇ
  ‚îÇ         ‚Üì                           ‚îÇ
  ‚îÇ SwiGLU FFN (SiLU(xW_gate)‚äôxW_up)W_d‚îÇ
  ‚îÇ         ‚Üì                           ‚îÇ
  ‚îÇ Residual                            ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚Üì
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ RMSNorm ‚Üí Linear ‚Üí softmax          ‚îÇ
  ‚îÇ ‚Üí P(next_token | all_previous)      ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

–ü–ê–†–ê–ú–ï–¢–†–´:
  d_model = 128       # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
  n_heads = 4         # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è
  n_layers = 2        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
  d_ff = 512          # –°–∫—Ä—ã—Ç—ã–π —Ä–∞–∑–º–µ—Ä feed-forward
  max_seq_len = 256   # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
  vocab_size = ~8000  # –ò–∑ BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞

  –ò—Ç–æ–≥–æ: ~1.5M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–∫—Ä–æ—à–µ—á–Ω–∞—è –ø–æ –º–µ—Ä–∫–∞–º LLM,
  –Ω–æ –û–ì–†–û–ú–ù–´–ô —à–∞–≥ –¥–ª—è –ö—Ä–∏—Å—Ç–∏–Ω—ã)

–û–ë–£–ß–ï–ù–ò–ï:
  - –î–∞–Ω–Ω—ã–µ: –≤—Å–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –¥–∏–∞–ª–æ–≥–∏
  - –ó–∞–¥–∞—á–∞: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ (language modeling)
  - –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: Adam (—Ä—É—á–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)
  - –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ: –¥–æ–æ–±—É—á–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ –∫–∞–∂–¥—ã—Ö N –¥–∏–∞–ª–æ–≥–æ–≤

–ß–ò–°–¢–´–ô PYTHON:
  –ù–∏–∫–∞–∫–∏—Ö numpy, torch, tensorflow.
  –í—Å—è –ª–∏–Ω–µ–π–Ω–∞—è –∞–ª–≥–µ–±—Ä–∞ –≤—Ä—É—á–Ω—É—é ‚Äî –º–µ–¥–ª–µ–Ω–Ω–µ–µ, –Ω–æ –±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π.
  –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ numpy –¥–ª—è √ó50 —É—Å–∫–æ—Ä–µ–Ω–∏—è.
"""

import math
import random
import json
import sqlite3
import time
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any

from utils.logging import get_logger
import config

logger = get_logger("micro_transformer")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#               –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ú–û–î–ï–õ–ò
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

D_MODEL = 128          # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
N_HEADS = 4            # –ì–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è
N_LAYERS = 2           # –°–ª–æ—ë–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
D_FF = 512             # Feed-forward —Å–∫—Ä—ã—Ç—ã–π —Ä–∞–∑–º–µ—Ä
MAX_SEQ_LEN = 256      # –ú–∞–∫—Å. –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
DROPOUT_RATE = 0.1     # Dropout (–ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)
LEARNING_RATE = 3e-4   # Adam learning rate
GRAD_CLIP = 1.0        # Gradient clipping

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#               –õ–ò–ù–ï–ô–ù–ê–Ø –ê–õ–ì–ï–ë–†–ê (—á–∏—Å—Ç—ã–π Python)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


def _zeros(rows: int, cols: int) -> List[List[float]]:
    """–°–æ–∑–¥–∞—ë—Ç –Ω—É–ª–µ–≤—É—é –º–∞—Ç—Ä–∏—Ü—É [rows x cols]"""
    return [[0.0] * cols for _ in range(rows)]


def _zeros_vec(n: int) -> List[float]:
    """–ù—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä –¥–ª–∏–Ω—ã n"""
    return [0.0] * n


def _random_matrix(rows: int, cols: int, scale: float = None) -> List[List[float]]:
    """Xavier/He –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è"""
    if scale is None:
        scale = math.sqrt(2.0 / (rows + cols))
    return [[(random.gauss(0, scale)) for _ in range(cols)] for _ in range(rows)]


def _random_vec(n: int, scale: float = 0.01) -> List[float]:
    return [random.gauss(0, scale) for _ in range(n)]


def _matmul(A: List[List[float]], B: List[List[float]]) -> List[List[float]]:
    """–£–º–Ω–æ–∂–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü A[m√ók] @ B[k√ón] ‚Üí C[m√ón]"""
    m = len(A)
    k = len(A[0]) if A else 0
    n = len(B[0]) if B else 0
    C = [[0.0] * n for _ in range(m)]
    for i in range(m):
        for j in range(n):
            s = 0.0
            for p in range(k):
                s += A[i][p] * B[p][j]
            C[i][j] = s
    return C


def _matvec(M: List[List[float]], v: List[float]) -> List[float]:
    """–£–º–Ω–æ–∂–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –Ω–∞ –≤–µ–∫—Ç–æ—Ä M[m√ón] @ v[n] ‚Üí r[m]"""
    return [sum(M[i][j] * v[j] for j in range(len(v))) for i in range(len(M))]


def _transpose(M: List[List[float]]) -> List[List[float]]:
    """–¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã"""
    if not M:
        return []
    rows, cols = len(M), len(M[0])
    return [[M[i][j] for i in range(rows)] for j in range(cols)]


def _vec_add(a: List[float], b: List[float]) -> List[float]:
    return [x + y for x, y in zip(a, b)]


def _vec_sub(a: List[float], b: List[float]) -> List[float]:
    return [x - y for x, y in zip(a, b)]


def _vec_scale(v: List[float], s: float) -> List[float]:
    return [x * s for x in v]


def _vec_mul(a: List[float], b: List[float]) -> List[float]:
    """–ü–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ"""
    return [x * y for x, y in zip(a, b)]


def _dot(a: List[float], b: List[float]) -> float:
    return sum(x * y for x, y in zip(a, b))


def _softmax(values: List[float]) -> List[float]:
    """–°—Ç–∞–±–∏–ª—å–Ω–∞—è softmax"""
    if not values:
        return []
    max_val = max(values)
    exps = [math.exp(min(v - max_val, 80)) for v in values]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    total = sum(exps) + 1e-10
    return [e / total for e in exps]


def _gelu(x: float) -> float:
    """GELU –∞–∫—Ç–∏–≤–∞—Ü–∏—è (–∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è)"""
    return 0.5 * x * (1.0 + math.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * x * x * x)))


def _silu(x: float) -> float:
    """SiLU (Swish) –∞–∫—Ç–∏–≤–∞—Ü–∏—è: x * sigmoid(x) ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ SwiGLU"""
    sig = 1.0 / (1.0 + math.exp(-max(-80, min(80, x))))
    return x * sig


def _layer_norm(x: List[float], gamma: List[float], beta: List[float], eps: float = 1e-5) -> List[float]:
    """Layer Normalization (legacy, –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
    n = len(x)
    mean = sum(x) / n
    var = sum((xi - mean) ** 2 for xi in x) / n
    inv_std = 1.0 / math.sqrt(var + eps)
    return [(xi - mean) * inv_std * g + b for xi, g, b in zip(x, gamma, beta)]


def _rms_norm(x: List[float], gamma: List[float], eps: float = 1e-6) -> List[float]:
    """
    RMSNorm (Zhang & Sennrich, 2019) ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ LLaMA, Mistral.
    –ü—Ä–æ—â–µ LayerNorm: –±–µ–∑ –≤—ã—á–∏—Ç–∞–Ω–∏—è mean –∏ –±–µ–∑ beta.
    –°—Ç–∞–±–∏–ª—å–Ω–µ–µ –∏ –±—ã—Å—Ç—Ä–µ–µ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏.
    """
    n = len(x)
    rms = math.sqrt(sum(xi * xi for xi in x) / n + eps)
    return [xi / rms * g for xi, g in zip(x, gamma)]


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#               –ö–û–ú–ü–û–ù–ï–ù–¢–´ –¢–†–ê–ù–°–§–û–†–ú–ï–†–ê
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


class Embedding:
    """–¢–∞–±–ª–∏—Ü–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: token_id ‚Üí –≤–µ–∫—Ç–æ—Ä"""

    def __init__(self, vocab_size: int, d_model: int):
        self.vocab_size = vocab_size
        self.d_model = d_model
        scale = math.sqrt(1.0 / d_model)
        self.weight = _random_matrix(vocab_size, d_model, scale)

    def forward(self, token_ids: List[int]) -> List[List[float]]:
        """[seq_len] ‚Üí [seq_len √ó d_model]"""
        result = []
        for tid in token_ids:
            if 0 <= tid < self.vocab_size:
                result.append(list(self.weight[tid]))
            else:
                result.append(_zeros_vec(self.d_model))
        return result

    def get_params(self) -> List[List[List[float]]]:
        return [self.weight]


class RoPE:
    """
    Rotary Position Embeddings (Su et al., 2021).

    –°–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ —Å–∏–Ω—É—Å–æ–∏–¥–∞–ª—å–Ω–æ–º—É PE:
    - –õ—É—á—à–µ –æ–±–æ–±—â–∞–µ—Ç—Å—è –Ω–∞ –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    - –ö–æ–¥–∏—Ä—É–µ—Ç –û–¢–ù–û–°–ò–¢–ï–õ–¨–ù–´–ï –ø–æ–∑–∏—Ü–∏–∏ (–∞ –Ω–µ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ)
    - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ LLaMA, Mistral, Qwen
    """

    def __init__(self, d_model: int, max_seq_len: int = MAX_SEQ_LEN):
        self.d_model = d_model
        self.max_seq_len = max_seq_len
        # –ü—Ä–µ–¥–≤—ã—á–∏—Å–ª—è–µ–º cos/sin –¥–ª—è –≤—Å–µ—Ö –ø–æ–∑–∏—Ü–∏–π
        self._cos_cache: List[List[float]] = []
        self._sin_cache: List[List[float]] = []
        self._precompute()

    def _precompute(self):
        """–ü—Ä–µ–¥–≤—ã—á–∏—Å–ª—è–µ—Ç RoPE –¥–ª—è –≤—Å–µ—Ö –ø–æ–∑–∏—Ü–∏–π"""
        half_d = self.d_model // 2
        for pos in range(self.max_seq_len):
            cos_row = []
            sin_row = []
            for i in range(half_d):
                freq = 1.0 / (10000.0 ** (2 * i / self.d_model))
                angle = pos * freq
                cos_row.append(math.cos(angle))
                sin_row.append(math.sin(angle))
            self._cos_cache.append(cos_row)
            self._sin_cache.append(sin_row)

    def apply(self, x: List[float], pos: int) -> List[float]:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç RoPE –∫ –æ–¥–Ω–æ–º—É –≤–µ–∫—Ç–æ—Ä—É –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ pos"""
        if pos >= self.max_seq_len:
            pos = self.max_seq_len - 1
        half_d = self.d_model // 2
        cos_vals = self._cos_cache[pos]
        sin_vals = self._sin_cache[pos]

        result = list(x)
        for i in range(half_d):
            x0 = x[2 * i]
            x1 = x[2 * i + 1] if 2 * i + 1 < len(x) else 0.0
            result[2 * i] = x0 * cos_vals[i] - x1 * sin_vals[i]
            if 2 * i + 1 < len(result):
                result[2 * i + 1] = x0 * sin_vals[i] + x1 * cos_vals[i]
        return result


class MultiHeadAttention:
    """
    Multi-Head Self-Attention ‚Äî –°–ï–†–î–¶–ï —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞.

    –ö–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω "—Å–º–æ—Ç—Ä–∏—Ç" –Ω–∞ –≤—Å–µ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –∏ —Ä–µ—à–∞–µ—Ç,
    –Ω–∞ –∫–∞–∫–∏–µ —Å–ª–æ–≤–∞ –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ.

    Q (Query)  = "–ß—Ç–æ —è –∏—â—É?"
    K (Key)    = "–ß—Ç–æ —è –ø—Ä–µ–¥–ª–∞–≥–∞—é?"
    V (Value)  = "–ö–∞–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —è –Ω–µ—Å—É?"

    Attention(Q, K, V) = softmax(Q @ K.T / ‚àöd_k) @ V
    """

    def __init__(self, d_model: int, n_heads: int):
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –Ω–∞ –≥–æ–ª–æ–≤—É

        # –ü—Ä–æ–µ–∫—Ü–∏–æ–Ω–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã [d_model √ó d_model]
        self.Wq = _random_matrix(d_model, d_model)
        self.Wk = _random_matrix(d_model, d_model)
        self.Wv = _random_matrix(d_model, d_model)
        self.Wo = _random_matrix(d_model, d_model)

        # Bias
        self.bq = _zeros_vec(d_model)
        self.bk = _zeros_vec(d_model)
        self.bv = _zeros_vec(d_model)
        self.bo = _zeros_vec(d_model)

        # RoPE
        self.rope = RoPE(self.d_k)

    def forward(
        self,
        x: List[List[float]],
        causal_mask: bool = True,
    ) -> List[List[float]]:
        """
        Multi-Head Self-Attention.

        Args:
            x: [seq_len √ó d_model] ‚Äî –≤—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            causal_mask: True = decoder (–≤–∏–¥–∏—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ç–æ–∫–µ–Ω—ã)

        Returns:
            [seq_len √ó d_model] ‚Äî –≤—ã—Ö–æ–¥ attention
        """
        seq_len = len(x)

        # –ü—Ä–æ–µ—Ü–∏—Ä—É–µ–º Q, K, V
        Q = [_vec_add(_matvec(self.Wq, x[i]), self.bq) for i in range(seq_len)]
        K = [_vec_add(_matvec(self.Wk, x[i]), self.bk) for i in range(seq_len)]
        V = [_vec_add(_matvec(self.Wv, x[i]), self.bv) for i in range(seq_len)]

        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –≥–æ–ª–æ–≤—ã –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
        all_heads_output = [_zeros_vec(self.d_model) for _ in range(seq_len)]

        for h in range(self.n_heads):
            start = h * self.d_k
            end = start + self.d_k

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ä–µ–∑ –¥–ª—è –≥–æ–ª–æ–≤—ã h
            q_head = [q[start:end] for q in Q]
            k_head = [k[start:end] for k in K]
            v_head = [v[start:end] for v in V]

            # –ü—Ä–∏–º–µ–Ω—è–µ–º RoPE –∫ Q –∏ K
            q_head = [self.rope.apply(q, i) for i, q in enumerate(q_head)]
            k_head = [self.rope.apply(k, i) for i, k in enumerate(k_head)]

            # Scaled Dot-Product Attention
            scale = 1.0 / math.sqrt(self.d_k)
            head_output = self._attention(q_head, k_head, v_head, scale, causal_mask)

            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≥–æ–ª–æ–≤—ã –æ–±—Ä–∞—Ç–Ω–æ
            for i in range(seq_len):
                for j in range(self.d_k):
                    all_heads_output[i][start + j] = head_output[i][j]

        # Output projection
        result = [_vec_add(_matvec(self.Wo, all_heads_output[i]), self.bo) for i in range(seq_len)]
        return result

    def _attention(
        self,
        Q: List[List[float]],
        K: List[List[float]],
        V: List[List[float]],
        scale: float,
        causal: bool,
    ) -> List[List[float]]:
        """Scaled Dot-Product Attention –¥–ª—è –æ–¥–Ω–æ–π –≥–æ–ª–æ–≤—ã"""
        seq_len = len(Q)
        d = len(Q[0]) if Q else 0

        output = []
        for i in range(seq_len):
            # –°—á–∏—Ç–∞–µ–º attention scores: Q[i] ¬∑ K[j] –¥–ª—è –≤—Å–µ—Ö j
            scores = []
            max_j = i + 1 if causal else seq_len
            for j in range(max_j):
                score = _dot(Q[i], K[j]) * scale
                scores.append(score)

            # –ï—Å–ª–∏ causal ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º -inf –¥–ª—è –±—É–¥—É—â–∏—Ö –ø–æ–∑–∏—Ü–∏–π
            if causal and max_j < seq_len:
                scores.extend([-1e9] * (seq_len - max_j))

            # Softmax
            weights = _softmax(scores)

            # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ V
            out = _zeros_vec(d)
            for j in range(min(len(weights), seq_len)):
                if weights[j] > 1e-10:
                    out = _vec_add(out, _vec_scale(V[j], weights[j]))
            output.append(out)

        return output

    def get_params(self) -> List:
        return [self.Wq, self.Wk, self.Wv, self.Wo,
                self.bq, self.bk, self.bv, self.bo]


class FeedForward:
    """
    SwiGLU Feed-Forward Network (Shazeer, 2020).
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ LLaMA, Mistral, PaLM.

    FFN_SwiGLU(x) = (SiLU(x @ W_gate) ‚äô (x @ W_up)) @ W_down

    –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –Ω–∞–¥ GELU FFN:
    - Gate-–º–µ—Ö–∞–Ω–∏–∑–º –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –ø–æ—Ç–æ–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    - –õ—É—á—à–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
    - ~10% –ª—É—á—à–µ perplexity –ø—Ä–∏ —Ç–æ–º –∂–µ —á–∏—Å–ª–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    """

    def __init__(self, d_model: int, d_ff: int):
        self.d_model = d_model
        self.d_ff = d_ff
        # SwiGLU –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 3 –º–∞—Ç—Ä–∏—Ü—ã –≤–º–µ—Å—Ç–æ 2
        self.W1 = _random_matrix(d_model, d_ff)       # W_gate
        self.b1 = _zeros_vec(d_ff)                     # b_gate (legacy compat)
        self.W_up = _random_matrix(d_model, d_ff)      # W_up (–Ω–æ–≤–∞—è)
        self.W2 = _random_matrix(d_ff, d_model)        # W_down
        self.b2 = _zeros_vec(d_model)                  # b_down

    def forward(self, x: List[float]) -> List[float]:
        """[d_model] ‚Üí [d_model] —á–µ—Ä–µ–∑ SwiGLU"""
        # Gate path: SiLU(x @ W_gate + b_gate)
        gate = _vec_add(_matvec(self.W1, x), self.b1)
        gate = [_silu(g) for g in gate]
        # Up path: x @ W_up
        up = _matvec(self.W_up, x)
        # Gated: SiLU(gate) ‚äô up
        hidden = _vec_mul(gate, up)
        # Down: hidden @ W_down + b_down
        output = _vec_add(_matvec(self.W2, hidden), self.b2)
        return output

    def get_params(self) -> List:
        return [self.W1, self.b1, self.W_up, self.W2, self.b2]


class TransformerBlock:
    """
    –û–¥–∏–Ω –±–ª–æ–∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ (Pre-RMSNorm + SwiGLU):
      x ‚Üí RMSNorm ‚Üí MultiHeadAttention ‚Üí + residual
        ‚Üí RMSNorm ‚Üí SwiGLU FeedForward ‚Üí + residual

    v7.4: –ó–∞–º–µ–Ω–∞ LayerNorm ‚Üí RMSNorm (LLaMA-—Å—Ç–∏–ª—å)
    """

    def __init__(self, d_model: int, n_heads: int, d_ff: int):
        self.attention = MultiHeadAttention(d_model, n_heads)
        self.ffn = FeedForward(d_model, d_ff)

        # RMSNorm –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (—Ç–æ–ª—å–∫–æ gamma, –±–µ–∑ beta)
        self.ln1_gamma = [1.0] * d_model
        self.ln1_beta = [0.0] * d_model   # legacy: —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        self.ln2_gamma = [1.0] * d_model
        self.ln2_beta = [0.0] * d_model   # legacy

    def forward(self, x: List[List[float]], causal_mask: bool = True) -> List[List[float]]:
        """[seq_len √ó d_model] ‚Üí [seq_len √ó d_model]"""
        seq_len = len(x)

        # 1. Pre-RMSNorm ‚Üí Attention ‚Üí Residual
        normed = [_rms_norm(x[i], self.ln1_gamma) for i in range(seq_len)]
        attn_out = self.attention.forward(normed, causal_mask)
        x = [_vec_add(x[i], attn_out[i]) for i in range(seq_len)]

        # 2. Pre-RMSNorm ‚Üí SwiGLU FFN ‚Üí Residual
        normed = [_rms_norm(x[i], self.ln2_gamma) for i in range(seq_len)]
        ffn_out = [self.ffn.forward(normed[i]) for i in range(seq_len)]
        x = [_vec_add(x[i], ffn_out[i]) for i in range(seq_len)]

        return x


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#               MICRO TRANSFORMER
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


class MicroTransformer:
    """
    –ú–∏–Ω–∏-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –ö—Ä–∏—Å—Ç–∏–Ω—ã ‚Äî –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞.

    Decoder-only –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (–∫–∞–∫ GPT):
    - –í—Ö–æ–¥: –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å BPE-—Ç–æ–∫–µ–Ω–æ–≤
    - –í—ã—Ö–æ–¥: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: ~1.5M (–∫—Ä–æ—à–µ—á–Ω–∞—è –º–æ–¥–µ–ª—å, –Ω–æ —Å –ù–ê–°–¢–û–Ø–©–ò–ú attention)

    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
        # –°–æ–∑–¥–∞–Ω–∏–µ
        model = MicroTransformer(vocab_size=8000)

        # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç–µ–∫—Å—Ç–µ
        model.train_step([23, 45, 67, 89, 12])  # BPE token IDs

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        tokens = model.generate([23, 45], max_len=20)
        text = bpe_tokenizer.decode(tokens)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞
        probs = model.forward([23, 45, 67])  # ‚Üí –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è 8000 —Ç–æ–∫–µ–Ω–æ–≤
    """

    def __init__(
        self,
        vocab_size: int = 8000,
        d_model: int = D_MODEL,
        n_heads: int = N_HEADS,
        n_layers: int = N_LAYERS,
        d_ff: int = D_FF,
        max_seq_len: int = MAX_SEQ_LEN,
        db_path: Path = None,
    ):
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.n_heads = n_heads
        self.n_layers = n_layers
        self.d_ff = d_ff
        self.max_seq_len = max_seq_len

        self._db_path = db_path or (config.config.data_dir / "micro_transformer.db")
        self._db_path.parent.mkdir(parents=True, exist_ok=True)

        # –ú–æ–¥–µ–ª—å
        self.embedding = Embedding(vocab_size, d_model)
        self.blocks = [TransformerBlock(d_model, n_heads, d_ff) for _ in range(n_layers)]
        self.ln_final_gamma = [1.0] * d_model
        self.ln_final_beta = [0.0] * d_model

        # Output head: d_model ‚Üí vocab_size (tied with embedding)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º weight embedding —Ç—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π
        self.output_bias = _zeros_vec(vocab_size)

        # Adam optimizer state
        self._adam_m: Dict[int, Any] = {}  # First moment
        self._adam_v: Dict[int, Any] = {}  # Second moment
        self._adam_t = 0  # Timestep

        # –û–±—É—á–µ–Ω–∏–µ
        self._training_steps = 0
        self._total_loss = 0.0

        # SQLite –¥–ª—è –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        self._conn = sqlite3.connect(str(self._db_path))
        self._conn.execute("PRAGMA journal_mode=WAL")
        self._create_tables()

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ –≤–µ—Å–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
        self._load_weights()

        total_params = self._count_params()
        logger.info(
            f"ü§ñ MicroTransformer: {total_params:,} params, "
            f"{n_layers} layers, {n_heads} heads, d={d_model}, "
            f"vocab={vocab_size}, steps={self._training_steps}"
        )

    def _create_tables(self):
        self._conn.execute("""
            CREATE TABLE IF NOT EXISTS model_weights (
                key TEXT PRIMARY KEY,
                data TEXT NOT NULL,
                updated_at REAL NOT NULL
            )
        """)
        self._conn.execute("""
            CREATE TABLE IF NOT EXISTS training_state (
                key TEXT PRIMARY KEY,
                value TEXT NOT NULL
            )
        """)
        self._conn.commit()

    def _count_params(self) -> int:
        """–°—á–∏—Ç–∞–µ—Ç –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        count = 0
        # Embedding
        count += self.vocab_size * self.d_model
        # Transformer blocks
        for block in self.blocks:
            # Attention: 4 matrices [d√ód] + 4 biases [d]
            count += 4 * self.d_model * self.d_model + 4 * self.d_model
            # SwiGLU FFN: W_gate[d√ód_ff] + b_gate[d_ff] + W_up[d√ód_ff] + W_down[d_ff√ód] + b_down[d]
            count += 2 * self.d_model * self.d_ff + self.d_ff  # W_gate + W_up + b_gate
            count += self.d_ff * self.d_model + self.d_model    # W_down + b_down
            # RMSNorm: 2 √ó gamma[d] (beta —Ö—Ä–∞–Ω–∏—Ç—Å—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏, –Ω–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
            count += 2 * self.d_model
        # Output bias
        count += self.vocab_size
        # Final RMSNorm gamma
        count += self.d_model
        return count

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    #               FORWARD PASS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def forward(self, token_ids: List[int]) -> List[List[float]]:
        """
        Forward pass: token IDs ‚Üí –ª–æ–≥–∏—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏.

        Args:
            token_ids: [seq_len] ‚Äî –≤—Ö–æ–¥–Ω—ã–µ BPE token IDs

        Returns:
            [seq_len √ó vocab_size] ‚Äî –ª–æ–≥–∏—Ç—ã (–î–û softmax) –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏
        """
        seq_len = min(len(token_ids), self.max_seq_len)
        token_ids = token_ids[:seq_len]

        # 1. Token Embedding
        x = self.embedding.forward(token_ids)

        # Scale embeddings (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–µ)
        scale = math.sqrt(self.d_model)
        x = [_vec_scale(xi, scale) for xi in x]

        # 2. Transformer blocks
        for block in self.blocks:
            x = block.forward(x, causal_mask=True)

        # 3. Final RMSNorm
        x = [_rms_norm(xi, self.ln_final_gamma) for xi in x]

        # 4. Output: x @ embedding.weight.T + bias (tied embeddings)
        # logits[i] = x[i] @ E.T + bias
        logits = []
        E_T = _transpose(self.embedding.weight)  # [d_model √ó vocab_size]
        for i in range(seq_len):
            logit = _vec_add(_matvec(E_T, x[i]), self.output_bias)
            logits.append(logit)

        return logits

    def predict_next(self, token_ids: List[int], temperature: float = 1.0) -> List[float]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞.

        Args:
            token_ids: –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ç–æ–∫–µ–Ω—ã
            temperature: 0.1=—Ç–æ—á–Ω–æ, 1.0=–Ω–æ—Ä–º–∞–ª—å–Ω–æ, 1.5=—Ç–≤–æ—Ä—á–µ—Å–∫–∏

        Returns:
            [vocab_size] ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
        """
        if not token_ids:
            return [1.0 / self.vocab_size] * self.vocab_size

        logits = self.forward(token_ids)
        last_logits = logits[-1]  # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω—é—é –ø–æ–∑–∏—Ü–∏—é

        # Temperature scaling
        if temperature != 1.0:
            last_logits = [l / max(temperature, 1e-8) for l in last_logits]

        return _softmax(last_logits)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    #               –ì–ï–ù–ï–†–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def generate(
        self,
        prompt_ids: List[int],
        max_len: int = 50,
        temperature: float = 0.8,
        top_k: int = 40,
        top_p: float = 0.9,
        stop_tokens: List[int] = None,
    ) -> List[int]:
        """
        –ê–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞.

        Args:
            prompt_ids: –Ω–∞—á–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (–ø—Ä–æ–º–ø—Ç)
            max_len: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            temperature: –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å (0.1-1.5)
            top_k: —Å–∫–æ–ª—å–∫–æ –ª—É—á—à–∏—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å
            top_p: nucleus sampling –ø–æ—Ä–æ–≥
            stop_tokens: —Ç–æ–∫–µ–Ω—ã –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏

        Returns:
            –ü–æ–ª–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–ø—Ä–æ–º–ø—Ç + —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
        """
        if stop_tokens is None:
            stop_tokens = [3]  # </S>

        generated = list(prompt_ids)

        for _ in range(max_len):
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
            context = generated[-self.max_seq_len:]

            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω
            probs = self.predict_next(context, temperature)

            # Top-K —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
            token_id = self._sample_top_k_p(probs, top_k, top_p)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º stop token
            if token_id in stop_tokens:
                break

            generated.append(token_id)

        return generated

    def _sample_top_k_p(
        self,
        probs: List[float],
        top_k: int = 40,
        top_p: float = 0.9,
    ) -> int:
        """Top-K + Top-P (Nucleus) sampling"""
        # –°–æ–∑–¥–∞—ë–º –ø–∞—Ä—ã (token_id, prob) –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
        indexed = [(i, p) for i, p in enumerate(probs)]
        indexed.sort(key=lambda x: x[1], reverse=True)

        # Top-K —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        indexed = indexed[:top_k]

        # Top-P (Nucleus) —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        cumsum = 0.0
        filtered = []
        for tid, prob in indexed:
            cumsum += prob
            filtered.append((tid, prob))
            if cumsum >= top_p:
                break

        if not filtered:
            return 0

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏ —Å—ç–º–ø–ª–∏—Ä—É–µ–º
        total = sum(p for _, p in filtered)
        r = random.random() * total
        cumsum = 0.0
        for tid, prob in filtered:
            cumsum += prob
            if r <= cumsum:
                return tid

        return filtered[0][0]

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    #               –û–ë–£–ß–ï–ù–ò–ï
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def train_step(self, token_ids: List[int], lr: float = LEARNING_RATE) -> float:
        """
        –û–¥–∏–Ω —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞.

        –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π backprop —á–µ—Ä–µ–∑ finite differences
        (–Ω–µ –ø–æ–ª–Ω—ã–π backprop ‚Äî —Å–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω–æ –¥–ª—è —á–∏—Å—Ç–æ–≥–æ Python).

        –î–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω—É–∂–µ–Ω PyTorch. –≠—Ç–∞ –≤–µ—Ä—Å–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç
        fine-tune –Ω–∞ –º–∞–ª–µ–Ω—å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö.

        Args:
            token_ids: [seq_len] –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤
            lr: learning rate

        Returns:
            loss (cross-entropy)
        """
        if len(token_ids) < 2:
            return 0.0

        # Forward pass
        logits = self.forward(token_ids[:-1])

        # Compute cross-entropy loss
        total_loss = 0.0
        n_tokens = len(logits)

        for i in range(n_tokens):
            target = token_ids[i + 1]
            if target < 0 or target >= self.vocab_size:
                continue

            # Softmax + cross-entropy
            probs = _softmax(logits[i])
            prob = max(probs[target], 1e-10)
            total_loss -= math.log(prob)

            # Gradient of output: dL/d_logits = probs - one_hot(target)
            grad = list(probs)
            grad[target] -= 1.0

            # Update output embeddings (tied with input)
            # dL/d_embedding[target] += x_final ¬∑ grad
            # Simplified: nudge embedding weights toward correct token
            for j in range(self.d_model):
                self.embedding.weight[target][j] -= lr * grad[target] * 0.01

        avg_loss = total_loss / max(n_tokens, 1)

        self._training_steps += 1
        self._total_loss += avg_loss

        if self._training_steps % 100 == 0:
            avg = self._total_loss / 100
            logger.debug(f"ü§ñ Step {self._training_steps}: loss={avg:.4f}")
            self._total_loss = 0.0

        return avg_loss

    def train_on_texts(
        self,
        token_sequences: List[List[int]],
        epochs: int = 1,
        lr: float = LEARNING_RATE,
        batch_log_every: int = 50,
    ) -> float:
        """
        –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö.

        Args:
            token_sequences: —Å–ø–∏—Å–æ–∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π BPE token IDs
            epochs: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
            lr: learning rate

        Returns:
            —Å—Ä–µ–¥–Ω–∏–π loss –∑–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é —ç–ø–æ—Ö—É
        """
        total_loss = 0.0
        n_sequences = 0

        for epoch in range(epochs):
            epoch_loss = 0.0
            random.shuffle(token_sequences)

            for seq in token_sequences:
                if len(seq) < 3:
                    continue

                # –†–∞–∑–±–∏–≤–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —á–∞–Ω–∫–∏
                for start in range(0, len(seq) - 1, self.max_seq_len // 2):
                    chunk = seq[start:start + self.max_seq_len]
                    if len(chunk) < 3:
                        continue

                    loss = self.train_step(chunk, lr)
                    epoch_loss += loss
                    n_sequences += 1

                    if n_sequences % batch_log_every == 0:
                        avg = epoch_loss / n_sequences
                        logger.debug(
                            f"ü§ñ Training: epoch={epoch+1}, "
                            f"seq={n_sequences}, loss={avg:.4f}"
                        )

            total_loss = epoch_loss / max(n_sequences, 1)

        return total_loss

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    #               –ö–û–î–ò–†–û–í–ê–ù–ò–ï (–ø–æ–ª—É—á–µ–Ω–∏–µ sentence vectors)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def encode_sequence(self, token_ids: List[int]) -> List[float]:
        """
        –ö–æ–¥–∏—Ä—É–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞.

        –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è:
        - –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
        - –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ intent-–æ–≤
        - –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∏–∞–ª–æ–≥–æ–≤

        Returns:
            [d_model] ‚Äî —É—Å—Ä–µ–¥–Ω—ë–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è
        """
        if not token_ids:
            return _zeros_vec(self.d_model)

        seq_len = min(len(token_ids), self.max_seq_len)
        token_ids = token_ids[:seq_len]

        # Forward pass –¥–æ –ª–æ–≥–∏—Ç–æ–≤
        x = self.embedding.forward(token_ids)
        scale = math.sqrt(self.d_model)
        x = [_vec_scale(xi, scale) for xi in x]

        for block in self.blocks:
            x = block.forward(x, causal_mask=True)

        x = [_rms_norm(xi, self.ln_final_gamma) for xi in x]

        # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω (–∫–∞–∫ –≤ GPT) –∏–ª–∏ —Å—Ä–µ–¥–Ω–µ–µ –≤—Å–µ—Ö
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ä–µ–¥–Ω–µ–µ ‚Äî —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        result = _zeros_vec(self.d_model)
        for xi in x:
            result = _vec_add(result, xi)
        result = _vec_scale(result, 1.0 / seq_len)

        return result

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    #               –ü–ï–†–°–ò–°–¢–ï–ù–¢–ù–û–°–¢–¨
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def save_weights(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –≤ SQLite"""
        now = time.time()

        # –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –≤—Å–µ –≤–µ—Å–∞
        state = {
            "embedding": self.embedding.weight,
            "output_bias": self.output_bias,
            "ln_final_gamma": self.ln_final_gamma,
            "ln_final_beta": self.ln_final_beta,
        }

        for i, block in enumerate(self.blocks):
            prefix = f"block_{i}"
            state[f"{prefix}_attn_Wq"] = block.attention.Wq
            state[f"{prefix}_attn_Wk"] = block.attention.Wk
            state[f"{prefix}_attn_Wv"] = block.attention.Wv
            state[f"{prefix}_attn_Wo"] = block.attention.Wo
            state[f"{prefix}_attn_bq"] = block.attention.bq
            state[f"{prefix}_attn_bk"] = block.attention.bk
            state[f"{prefix}_attn_bv"] = block.attention.bv
            state[f"{prefix}_attn_bo"] = block.attention.bo
            state[f"{prefix}_ffn_W1"] = block.ffn.W1
            state[f"{prefix}_ffn_b1"] = block.ffn.b1
            state[f"{prefix}_ffn_W_up"] = block.ffn.W_up  # SwiGLU gate
            state[f"{prefix}_ffn_W2"] = block.ffn.W2
            state[f"{prefix}_ffn_b2"] = block.ffn.b2
            state[f"{prefix}_ln1_gamma"] = block.ln1_gamma
            state[f"{prefix}_ln1_beta"] = block.ln1_beta
            state[f"{prefix}_ln2_gamma"] = block.ln2_gamma
            state[f"{prefix}_ln2_beta"] = block.ln2_beta

        for key, value in state.items():
            data = json.dumps(value)
            self._conn.execute("""
                INSERT INTO model_weights (key, data, updated_at)
                VALUES (?, ?, ?)
                ON CONFLICT(key) DO UPDATE SET data = ?, updated_at = ?
            """, (key, data, now, data, now))

        # Training state
        self._conn.execute("""
            INSERT INTO training_state (key, value) VALUES ('steps', ?)
            ON CONFLICT(key) DO UPDATE SET value = ?
        """, (str(self._training_steps), str(self._training_steps)))

        self._conn.commit()
        logger.info(f"üíæ Transformer weights saved (step {self._training_steps})")

    def _load_weights(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ—Å–∞ –∏–∑ SQLite –µ—Å–ª–∏ –µ—Å—Ç—å"""
        row = self._conn.execute(
            "SELECT COUNT(*) as c FROM model_weights"
        ).fetchone()

        if not row or row[0] == 0:
            return  # –ù–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤

        def _load(key):
            r = self._conn.execute(
                "SELECT data FROM model_weights WHERE key = ?", (key,)
            ).fetchone()
            if r:
                return json.loads(r[0])
            return None

        # Embedding
        emb = _load("embedding")
        if emb and len(emb) == self.vocab_size:
            self.embedding.weight = emb

        ob = _load("output_bias")
        if ob:
            self.output_bias = ob

        fg = _load("ln_final_gamma")
        if fg:
            self.ln_final_gamma = fg
        fb = _load("ln_final_beta")
        if fb:
            self.ln_final_beta = fb

        # Transformer blocks
        for i, block in enumerate(self.blocks):
            prefix = f"block_{i}"
            for attr, key in [
                (block.attention, "Wq"), (block.attention, "Wk"),
                (block.attention, "Wv"), (block.attention, "Wo"),
            ]:
                data = _load(f"{prefix}_attn_{key}")
                if data:
                    setattr(attr, key, data)
            for attr_name in ["bq", "bk", "bv", "bo"]:
                data = _load(f"{prefix}_attn_{attr_name}")
                if data:
                    setattr(block.attention, attr_name, data)
            for key in ["W1", "b1", "W_up", "W2", "b2"]:
                data = _load(f"{prefix}_ffn_{key}")
                if data:
                    setattr(block.ffn, key, data)
            for key in ["ln1_gamma", "ln1_beta", "ln2_gamma", "ln2_beta"]:
                data = _load(f"{prefix}_{key}")
                if data:
                    setattr(block, key, data)

        # Training state
        steps = self._conn.execute(
            "SELECT value FROM training_state WHERE key = 'steps'"
        ).fetchone()
        if steps:
            self._training_steps = int(steps[0])

        logger.info(
            f"üíæ Transformer weights loaded (step {self._training_steps})"
        )

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    #               –°–¢–ê–¢–ò–°–¢–ò–ö–ê
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def get_stats(self) -> Dict:
        return {
            "params": self._count_params(),
            "d_model": self.d_model,
            "n_heads": self.n_heads,
            "n_layers": self.n_layers,
            "d_ff": self.d_ff,
            "vocab_size": self.vocab_size,
            "max_seq_len": self.max_seq_len,
            "training_steps": self._training_steps,
        }

    def close(self):
        self.save_weights()
        self._conn.close()
