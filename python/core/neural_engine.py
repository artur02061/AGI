"""
–ö—Ä–∏—Å—Ç–∏–Ω–∞ 7.1 ‚Äî NeuralEngine (–ù–µ–π—Ä–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å–ª–æ–≤)

–ö–ê–ö –≠–¢–û –†–ê–ë–û–¢–ê–ï–¢ (–∞–Ω–∞–ª–æ–≥–∏—è —Å —á–µ–ª–æ–≤–µ–∫–æ–º):
  –†–µ–±—ë–Ω–æ–∫ —Å–ª—ã—à–∏—Ç —Å–ª–æ–≤–∞ ‚Üí –∑–∞–ø–æ–º–∏–Ω–∞–µ—Ç –≤ –∫–∞–∫–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ ‚Üí
  –ø–æ–Ω–∏–º–∞–µ—Ç —á—Ç–æ "–æ—Ç–ª–∏—á–Ω–æ" –∏ "–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ" –∑–Ω–∞—á–∞—Ç –æ–¥–Ω–æ –∏ —Ç–æ –∂–µ ‚Üí
  —Å—Ç—Ä–æ–∏—Ç –°–í–û–ò –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ –ø–æ–Ω—è—Ç—ã—Ö —Å–ª–æ–≤.

–ê–†–•–ò–¢–ï–ö–¢–£–†–ê:
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ 1. Word2Vec (Skip-gram)                         ‚îÇ
  ‚îÇ    "–ø—Ä–∏–≤–µ—Ç" ‚Üí [0.12, -0.34, 0.56, ...]          ‚îÇ
  ‚îÇ    "–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π" ‚Üí [0.11, -0.33, 0.55, ...]      ‚îÇ
  ‚îÇ    (–±–ª–∏–∑–∫–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ = –ø–æ—Ö–æ–∂–∏–π —Å–º—ã—Å–ª)             ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚Üì
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ 2. WordKnowledge (–≥—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π)                   ‚îÇ
  ‚îÇ    "—Ñ–∞–π–ª" ‚Üí {pos: "noun", assoc: ["—Å–æ–∑–¥–∞—Ç—å",     ‚îÇ
  ‚îÇ              "–æ—Ç–∫—Ä—ã—Ç—å", "—É–¥–∞–ª–∏—Ç—å"], role: "object"}‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚Üì
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ 3. N-gram Model (–ø–µ—Ä–µ—Ö–æ–¥—ã –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏)         ‚îÇ
  ‚îÇ    "—è" ‚Üí "–º–æ–≥—É"(0.3), "–±—É–¥—É"(0.2), "—Ö–æ—á—É"(0.15) ‚îÇ
  ‚îÇ    "–º–æ–≥—É" ‚Üí "–ø–æ–º–æ—á—å"(0.4), "—Å–¥–µ–ª–∞—Ç—å"(0.3)        ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚Üì
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ 4. SentenceBuilder                               ‚îÇ
  ‚îÇ    intent="offer_help", mood="happy"              ‚îÇ
  ‚îÇ    seed="—Ä–∞–¥–∞" ‚Üí "—Ä–∞–¥–∞ –ø–æ–º–æ—á—å —Ç–µ–±–µ !" (–ù–û–í–û–ï)    ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

–û–ë–£–ß–ï–ù–ò–ï:
  –ö–∞–∂–¥—ã–π LLM-–æ—Ç–≤–µ—Ç –∏ –∫–∞–∂–¥–∞—è —Ñ—Ä–∞–∑–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
  1. –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç—Å—è ‚Üí —Å–ª–æ–≤–∞ –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –≤ —Å–ª–æ–≤–∞—Ä—å
  2. Word2Vec –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –ø–∞—Ä–∞—Ö (skip-gram)
  3. N-gram –º–æ–¥–µ–ª—å –æ–±–Ω–æ–≤–ª—è–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤
  4. WordKnowledge –æ–±–Ω–æ–≤–ª—è–µ—Ç –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–∏ –∏ —Ä–æ–ª–∏ —Å–ª–æ–≤
"""

import sqlite3
import json
import math
import random
import re
import time
from pathlib import Path
from typing import Optional, Dict, List, Tuple, Set

from utils.logging import get_logger
import config

logger = get_logger("neural_engine")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#               –ö–û–ù–°–¢–ê–ù–¢–´
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

EMBEDDING_DIM = 128        # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞ —Å–ª–æ–≤–∞ (v7.3: 64‚Üí128 –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Å–º—ã—Å–ª–æ–≤)
LEARNING_RATE = 0.025      # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
MIN_LEARNING_RATE = 0.001  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
WINDOW_SIZE = 3            # –û–∫–Ω–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (¬±3 —Å–ª–æ–≤–∞)
NEGATIVE_SAMPLES = 5       # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —Å—ç–º–ø–ª–æ–≤ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
MIN_WORD_FREQ = 1          # –ú–∏–Ω. —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤–∞ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
MAX_SENTENCE_LEN = 20      # –ú–∞–∫—Å. –¥–ª–∏–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
MIN_SENTENCE_LEN = 3       # –ú–∏–Ω. –¥–ª–∏–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è

# –†—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ (–Ω–µ —É—á–∏–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, –Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)
STOP_WORDS = {
    "–∏", "–≤", "–Ω–∞", "—Å", "–ø–æ", "–∫", "–æ—Ç", "–∑–∞", "–∏–∑", "—É", "–æ",
    "–∞", "–Ω–æ", "–∂–µ", "–ª–∏", "–±—ã", "–Ω–µ", "–Ω–∏", "–¥–∞", "–Ω–µ—Ç",
}

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–∞—Å—Ç–∏ —Ä–µ—á–∏ (—É–ø—Ä–æ—â—ë–Ω–Ω–∞—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è)
POS_PATTERNS = {
    "verb": re.compile(
        r'(?:–∞—Ç—å|—è—Ç—å|–µ—Ç—å|–∏—Ç—å|—É—Ç—å|—ã—Ç—å|—Ç–∏|—á—å|'
        r'–∞—é|—è—é|—É—é|–µ—à—å|–∏—à—å|–µ—Ç|–∏—Ç|–µ–º|–∏–º|–µ—Ç–µ|–∏—Ç–µ|—É—Ç|—é—Ç|–∞—Ç|—è—Ç|'
        r'–∞–ª|—è–ª|–µ–ª|–∏–ª|–∞–ª–∞|—è–ª–∞|–µ–ª–∞|–∏–ª–∞|–∞–ª–æ|—è–ª–æ|–µ–ª–æ|–∏–ª–æ|–∞–ª–∏|—è–ª–∏|–µ–ª–∏|–∏–ª–∏|'
        r'–∞–π|–µ–π|—É–π|–æ–π|–∞–π—Ç–µ|–µ–π—Ç–µ|—É–π—Ç–µ|–æ–π—Ç–µ)$'
    ),
    "noun": re.compile(
        r'(?:[–∞-—è]+(?:–æ—Å—Ç—å|–µ–Ω–∏–µ|–∞–Ω–∏–µ|—Å—Ç–≤–æ|—Ç–µ–ª—å|–Ω–∏–∫|—á–∏–∫|—â–∏–∫|–∫–∞|—Ü–∏—è|'
        r'–∏–µ|—å–µ|—Ç–∏—è|–∑–∏—è|–∏—è|–µ–π|–æ–≤|–∞–º|–∞–º–∏|–∞—Ö))$'
    ),
    "adjective": re.compile(
        r'(?:[–∞-—è]+(?:—ã–π|–∏–π|–æ–π|–∞—è|—è—è|–æ–µ|–µ–µ|—ã–µ|–∏–µ|–æ–º—É|–µ–º—É|–æ–π|–µ–π|'
        r'—ã–º|–∏–º|—ã–º–∏|–∏–º–∏|—ã—Ö|–∏—Ö))$'
    ),
    "adverb": re.compile(
        r'(?:[–∞-—è]+(?:–Ω–æ|–∫–æ|—Å–∫–∏|—á–µ—Å–∫–∏|—å–Ω–æ|–µ–ª–æ|—Å—Ç–æ|–∂–Ω–æ|—á–Ω–æ|—Ç–Ω–æ))$'
    ),
    "pronoun": re.compile(
        r'^(?:—è|—Ç—ã|–æ–Ω|–æ–Ω–∞|–æ–Ω–æ|–º—ã|–≤—ã|–æ–Ω–∏|–º–µ–Ω—è|—Ç–µ–±—è|–µ–≥–æ|–µ—ë|–Ω–∞—Å|–≤–∞—Å|–∏—Ö|'
        r'–º–Ω–µ|—Ç–µ–±–µ|–µ–º—É|–µ–π|–Ω–∞–º|–≤–∞–º|–∏–º|–º–Ω–æ–π|—Ç–æ–±–æ–π|–Ω–∏–º|–Ω–µ–π|–Ω–∞–º–∏|–≤–∞–º–∏|–Ω–∏–º–∏|'
        r'–º–æ–π|—Ç–≤–æ–π|–Ω–∞—à|–≤–∞—à|–µ–≥–æ|–µ—ë|–∏—Ö|—Å–≤–æ–π|—ç—Ç–æ—Ç|—Ç–æ—Ç|—Ç–∞–∫–æ–π|–∫–∞–∫–æ–π|'
        r'—á—Ç–æ|–∫—Ç–æ|–∫–æ—Ç–æ—Ä—ã–π|—á–µ–π|—Å–∫–æ–ª—å–∫–æ|—Å—Ç–æ–ª—å–∫–æ)$'
    ),
}

# –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å–ª–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (seed-—Å–ª–æ–≤–∞ –ø–æ —Å–∏—Ç—É–∞—Ü–∏—è–º)
SITUATION_SEEDS = {
    "greeting": ["–ø—Ä–∏–≤–µ—Ç", "–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π", "—Ä–∞–¥–∞", "–¥–æ–±—Ä—ã–π"],
    "farewell": ["–ø–æ–∫–∞", "—É–¥–∞—á–∏", "–≤—Å—Ç—Ä–µ—á–∏", "—Å–≤—è–∑–∏"],
    "offer_help": ["–ø–æ–º–æ—á—å", "–ø–æ–º–æ–≥—É", "—Å–¥–µ–ª–∞—Ç—å", "–Ω—É–∂–Ω–æ", "–¥–∞–≤–∞–π"],
    "state_positive": ["—Ö–æ—Ä–æ—à–æ", "–æ—Ç–ª–∏—á–Ω–æ", "–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ", "–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ", "—Ä–∞–¥–∞"],
    "state_neutral": ["–Ω–æ—Ä–º–∞–ª—å–Ω–æ", "—Å—Ç–∞–±–∏–ª—å–Ω–æ", "—Ä–∞–±–æ—Ç–∞—é", "–ø–æ—Ä—è–¥–∫–µ"],
    "state_tired": ["—É—Å—Ç–∞–ª–∞", "—Ç—è–∂–µ–ª–æ", "—Å–ø—Ä–∞–≤–ª—é—Å—å"],
    "gratitude_response": ["–ø–æ–∂–∞–ª—É–π—Å—Ç–∞", "—Ä–∞–¥–∞", "–æ–±—Ä–∞—â–∞–π—Å—è"],
    "self_intro": ["–∫—Ä–∏—Å—Ç–∏–Ω–∞", "–∑–æ–≤—É—Ç", "–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç", "–ø–æ–º–æ–≥–∞—é"],
    "compliment_response": ["—Å–ø–∞—Å–∏–±–æ", "–ø—Ä–∏—è—Ç–Ω–æ", "—Å—Ç–∞—Ä–∞—é—Å—å"],
    "empathy_positive": ["—Ä–∞–¥–∞", "–∑–¥–æ—Ä–æ–≤–æ", "–æ—Ç–ª–∏—á–Ω–æ", "–º–æ–ª–æ–¥–µ—Ü"],
    "empathy_negative": ["–ø–æ–Ω–∏–º–∞—é", "–±—ã–≤–∞–µ—Ç", "–¥–µ—Ä–∂–∏—Å—å", "–∑–¥–µ—Å—å"],
    "complaint_response": ["–∏–∑–≤–∏–Ω–∏", "–ø–æ—Å—Ç–∞—Ä–∞—é—Å—å", "–∏—Å–ø—Ä–∞–≤–ª—é", "–ª—É—á—à–µ"],
}


def _sigmoid(x: float) -> float:
    """–°—Ç–∞–±–∏–ª—å–Ω–∞—è —Å–∏–≥–º–æ–∏–¥–∞"""
    if x >= 0:
        z = math.exp(-x)
        return 1.0 / (1.0 + z)
    else:
        z = math.exp(x)
        return z / (1.0 + z)


def _cosine_similarity(v1: List[float], v2: List[float]) -> float:
    """–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–≤—É—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤"""
    dot = sum(a * b for a, b in zip(v1, v2))
    norm1 = math.sqrt(sum(a * a for a in v1))
    norm2 = math.sqrt(sum(b * b for b in v2))
    if norm1 < 1e-10 or norm2 < 1e-10:
        return 0.0
    return dot / (norm1 * norm2)


def _random_vector(dim: int) -> List[float]:
    """–°–ª—É—á–∞–π–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞"""
    return [(random.random() - 0.5) / dim for _ in range(dim)]


class NeuralEngine:
    """
    –ù–µ–π—Ä–æ–Ω–Ω—ã–π –¥–≤–∏–∂–æ–∫ –ö—Ä–∏—Å—Ç–∏–Ω—ã ‚Äî –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å–ª–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.

    –¢—Ä–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞:
    1. Word2Vec ‚Äî —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ª–æ–≤ (–ø–æ–Ω–∏–º–∞–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è)
    2. N-gram Model ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ (—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π)
    3. WordKnowledge ‚Äî —á–∞—Å—Ç—å —Ä–µ—á–∏, –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–∏ (–∑–Ω–∞–Ω–∏–µ –æ —Å–ª–æ–≤–∞—Ö)

    –û–±—É—á–∞–µ—Ç—Å—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ –∏–∑ –∫–∞–∂–¥–æ–≥–æ –¥–∏–∞–ª–æ–≥–∞.
    """

    def __init__(self, db_path: Path = None):
        self._db_path = db_path or (config.config.data_dir / "neural_engine.db")
        self._db_path.parent.mkdir(parents=True, exist_ok=True)

        self._conn = sqlite3.connect(str(self._db_path))
        self._conn.row_factory = sqlite3.Row
        self._conn.execute("PRAGMA journal_mode=WAL")
        self._conn.execute("PRAGMA synchronous=NORMAL")

        self._create_tables()

        # In-memory –∫–µ—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
        self._embeddings_cache: Dict[str, List[float]] = {}
        self._word_freq: Dict[str, int] = {}
        self._total_words = 0

        # N-gram –∫–µ—à
        self._bigrams: Dict[str, Dict[str, int]] = {}
        self._trigrams: Dict[Tuple[str, str], Dict[str, int]] = {}

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–µ—à –∏–∑ SQLite
        self._load_cache()

        stats = self.get_stats()
        logger.info(
            f"üß† NeuralEngine: {stats['vocabulary']} —Å–ª–æ–≤, "
            f"{stats['embeddings']} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, "
            f"{stats['bigrams']} –±–∏–≥—Ä–∞–º–º, "
            f"{stats['trigrams']} —Ç—Ä–∏–≥—Ä–∞–º–º, "
            f"–æ–±—É—á–µ–Ω–∏–π: {stats['training_steps']}"
        )

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    #               –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ë–î
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _create_tables(self):
        cur = self._conn.cursor()

        # ‚îÄ‚îÄ –°–ª–æ–≤–∞—Ä—å: –≤—Å–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ ‚îÄ‚îÄ
        cur.execute("""
            CREATE TABLE IF NOT EXISTS vocabulary (
                word TEXT PRIMARY KEY,
                frequency INTEGER DEFAULT 1,
                pos TEXT DEFAULT 'unknown',
                embedding TEXT,
                created_at REAL NOT NULL,
                updated_at REAL NOT NULL
            )
        """)

        # ‚îÄ‚îÄ –ë–∏–≥—Ä–∞–º–º—ã: –ø–∞—Ä—ã —Å–ª–æ–≤ (word1 ‚Üí word2) —Å —á–∞—Å—Ç–æ—Ç–æ–π ‚îÄ‚îÄ
        cur.execute("""
            CREATE TABLE IF NOT EXISTS bigrams (
                word1 TEXT NOT NULL,
                word2 TEXT NOT NULL,
                frequency INTEGER DEFAULT 1,
                PRIMARY KEY (word1, word2)
            )
        """)

        # ‚îÄ‚îÄ –¢—Ä–∏–≥—Ä–∞–º–º—ã: —Ç—Ä–æ–π–∫–∏ —Å–ª–æ–≤ ‚îÄ‚îÄ
        cur.execute("""
            CREATE TABLE IF NOT EXISTS trigrams (
                word1 TEXT NOT NULL,
                word2 TEXT NOT NULL,
                word3 TEXT NOT NULL,
                frequency INTEGER DEFAULT 1,
                PRIMARY KEY (word1, word2, word3)
            )
        """)

        # ‚îÄ‚îÄ –ê—Å—Å–æ—Ü–∏–∞—Ü–∏–∏: –∫–∞–∫–∏–µ —Å–ª–æ–≤–∞ —á–∞—Å—Ç–æ —Ä—è–¥–æ–º ‚îÄ‚îÄ
        cur.execute("""
            CREATE TABLE IF NOT EXISTS associations (
                word1 TEXT NOT NULL,
                word2 TEXT NOT NULL,
                strength REAL DEFAULT 1.0,
                context TEXT DEFAULT '',
                PRIMARY KEY (word1, word2)
            )
        """)

        # ‚îÄ‚îÄ –°–ª–æ–≤–æ-—Å–∏—Ç—É–∞—Ü–∏—è: –≤ –∫–∞–∫–∏—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —Å–ª–æ–≤–æ ‚îÄ‚îÄ
        cur.execute("""
            CREATE TABLE IF NOT EXISTS word_situations (
                word TEXT NOT NULL,
                situation TEXT NOT NULL,
                frequency INTEGER DEFAULT 1,
                PRIMARY KEY (word, situation)
            )
        """)

        # ‚îÄ‚îÄ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è ‚îÄ‚îÄ
        cur.execute("""
            CREATE TABLE IF NOT EXISTS training_log (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp REAL NOT NULL,
                source TEXT NOT NULL,
                words_processed INTEGER DEFAULT 0,
                pairs_trained INTEGER DEFAULT 0,
                loss REAL DEFAULT 0.0
            )
        """)

        # –ò–Ω–¥–µ–∫—Å—ã
        cur.execute("CREATE INDEX IF NOT EXISTS idx_vocab_freq ON vocabulary(frequency DESC)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_bigram_w1 ON bigrams(word1)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_trigram_w1w2 ON trigrams(word1, word2)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_assoc_w1 ON associations(word1)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_wordsit_word ON word_situations(word)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_wordsit_sit ON word_situations(situation)")

        self._conn.commit()

    def _load_cache(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ –ø–∞–º—è—Ç—å"""
        # –°–ª–æ–≤–∞—Ä—å –∏ —á–∞—Å—Ç–æ—Ç—ã
        rows = self._conn.execute(
            "SELECT word, frequency, embedding FROM vocabulary"
        ).fetchall()

        migrated = 0
        for row in rows:
            word = row["word"]
            self._word_freq[word] = row["frequency"]
            self._total_words += row["frequency"]
            if row["embedding"]:
                try:
                    emb = json.loads(row["embedding"])
                    # –ú–∏–≥—Ä–∞—Ü–∏—è: —Ä–∞—Å—à–∏—Ä—è–µ–º —Å—Ç–∞—Ä—ã–µ –≤–µ–∫—Ç–æ—Ä–∞ –¥–æ EMBEDDING_DIM
                    if len(emb) < EMBEDDING_DIM:
                        extra = EMBEDDING_DIM - len(emb)
                        emb.extend(
                            (random.random() - 0.5) / EMBEDDING_DIM
                            for _ in range(extra)
                        )
                        migrated += 1
                    self._embeddings_cache[word] = emb
                except (json.JSONDecodeError, TypeError):
                    pass

        if migrated > 0:
            logger.info(f"üìè Migrated {migrated} embeddings to {EMBEDDING_DIM}-dim")
            self._save_all_embeddings()

        # –ë–∏–≥—Ä–∞–º–º—ã
        rows = self._conn.execute(
            "SELECT word1, word2, frequency FROM bigrams"
        ).fetchall()

        for row in rows:
            w1 = row["word1"]
            if w1 not in self._bigrams:
                self._bigrams[w1] = {}
            self._bigrams[w1][row["word2"]] = row["frequency"]

        # –¢—Ä–∏–≥—Ä–∞–º–º—ã (–∑–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è)
        rows = self._conn.execute(
            "SELECT word1, word2, word3, frequency FROM trigrams "
            "WHERE frequency >= 2"
        ).fetchall()

        for row in rows:
            key = (row["word1"], row["word2"])
            if key not in self._trigrams:
                self._trigrams[key] = {}
            self._trigrams[key][row["word3"]] = row["frequency"]

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    #     –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø (—Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–ª–æ–≤–∞)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def tokenize(self, text: str) -> List[str]:
        """
        –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–ª–æ–≤–∞.

        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        (–Ω—É–∂–Ω—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π).
        """
        text = text.lower().strip()
        # –†–∞–∑–¥–µ–ª—è–µ–º —Å–ª–æ–≤–∞ –∏ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        tokens = re.findall(r'[–∞-—è—ëa-z0-9]+|[.!?,;:‚Äî\-]', text)
        return [t for t in tokens if t]

    def _guess_pos(self, word: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —á–∞—Å—Ç—å —Ä–µ—á–∏ –ø–æ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º"""
        w = word.lower()

        for pos, pattern in POS_PATTERNS.items():
            if pattern.search(w):
                return pos

        # –ö–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ ‚Äî —á–∞—Å—Ç–æ –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è –∏–ª–∏ —á–∞—Å—Ç–∏—Ü—ã
        if len(w) <= 2:
            return "particle"

        return "unknown"

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    #     –û–ë–£–ß–ï–ù–ò–ï: WORD2VEC (Skip-gram + Negative Sampling)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def learn_from_text(
        self,
        text: str,
        source: str = "dialogue",
        situations: List[str] = None,
    ):
        """
        –£—á–∏—Ç—Å—è –Ω–∞ —Ç–µ–∫—Å—Ç–µ:
        1. –î–æ–±–∞–≤–ª—è–µ—Ç —Å–ª–æ–≤–∞ –≤ —Å–ª–æ–≤–∞—Ä—å
        2. –û–±–Ω–æ–≤–ª—è–µ—Ç n-gram –º–æ–¥–µ–ª—å
        3. –û–±—É—á–∞–µ—Ç Word2Vec —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        4. –ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–∏

        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –¥–ª—è –ö–ê–ñ–î–û–ì–û LLM-–æ—Ç–≤–µ—Ç–∞ –∏ —Ä–µ–ø–ª–∏–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
        """
        tokens = self.tokenize(text)
        if len(tokens) < 2:
            return

        now = time.time()
        words_only = [t for t in tokens if re.match(r'[–∞-—è—ëa-z]', t)]

        # 1. –û–±–Ω–æ–≤–ª—è–µ–º —Å–ª–æ–≤–∞—Ä—å
        self._update_vocabulary(words_only, now)

        # 2. –û–±–Ω–æ–≤–ª—è–µ–º n-gram –º–æ–¥–µ–ª—å
        self._update_ngrams(tokens)

        # 3. –û–±—É—á–∞–µ–º Word2Vec
        pairs_trained = self._train_word2vec(words_only)

        # 4. –û–±–Ω–æ–≤–ª—è–µ–º –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–∏ (—Å–ª–æ–≤–∞ –≤ –æ–¥–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏)
        self._update_associations(words_only)

        # 5. –ü—Ä–∏–≤—è–∑—ã–≤–∞–µ–º —Å–ª–æ–≤–∞ –∫ —Å–∏—Ç—É–∞—Ü–∏—è–º
        if situations:
            self._update_word_situations(words_only, situations)

        # 6. –õ–æ–≥–∏—Ä—É–µ–º
        self._conn.execute("""
            INSERT INTO training_log (timestamp, source, words_processed, pairs_trained)
            VALUES (?, ?, ?, ?)
        """, (now, source, len(words_only), pairs_trained))
        self._conn.commit()

        logger.debug(
            f"üìñ NeuralEngine: learned {len(words_only)} words, "
            f"{pairs_trained} pairs from '{text[:40]}...'"
        )

    def _update_vocabulary(self, words: List[str], now: float):
        """–î–æ–±–∞–≤–ª—è–µ—Ç/–æ–±–Ω–æ–≤–ª—è–µ—Ç —Å–ª–æ–≤–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ"""
        for word in words:
            if word in STOP_WORDS and len(word) <= 2:
                continue

            if word in self._word_freq:
                self._word_freq[word] += 1
                self._conn.execute("""
                    UPDATE vocabulary SET frequency = frequency + 1, updated_at = ?
                    WHERE word = ?
                """, (now, word))
            else:
                pos = self._guess_pos(word)
                self._word_freq[word] = 1
                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
                emb = _random_vector(EMBEDDING_DIM)
                self._embeddings_cache[word] = emb

                self._conn.execute("""
                    INSERT OR IGNORE INTO vocabulary
                    (word, frequency, pos, embedding, created_at, updated_at)
                    VALUES (?, 1, ?, ?, ?, ?)
                """, (word, pos, json.dumps(emb), now, now))

            self._total_words += 1

    def _update_ngrams(self, tokens: List[str]):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –±–∏–≥—Ä–∞–º–º—ã –∏ —Ç—Ä–∏–≥—Ä–∞–º–º—ã"""
        # –î–æ–±–∞–≤–ª—è–µ–º –º–∞—Ä–∫–µ—Ä—ã –Ω–∞—á–∞–ª–∞/–∫–æ–Ω—Ü–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∑–Ω–∞–∫–∞–º –∫–æ–Ω—Ü–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = []
        current = ["<S>"]
        for t in tokens:
            if t in ".!?":
                current.append(t)
                current.append("</S>")
                sentences.append(current)
                current = ["<S>"]
            else:
                current.append(t)
        if len(current) > 1:
            current.append("</S>")
            sentences.append(current)

        for sent in sentences:
            # –ë–∏–≥—Ä–∞–º–º—ã
            for i in range(len(sent) - 1):
                w1, w2 = sent[i], sent[i + 1]
                if w1 not in self._bigrams:
                    self._bigrams[w1] = {}
                self._bigrams[w1][w2] = self._bigrams[w1].get(w2, 0) + 1

                self._conn.execute("""
                    INSERT INTO bigrams (word1, word2, frequency)
                    VALUES (?, ?, 1)
                    ON CONFLICT(word1, word2)
                    DO UPDATE SET frequency = frequency + 1
                """, (w1, w2))

            # –¢—Ä–∏–≥—Ä–∞–º–º—ã
            for i in range(len(sent) - 2):
                w1, w2, w3 = sent[i], sent[i + 1], sent[i + 2]
                key = (w1, w2)
                if key not in self._trigrams:
                    self._trigrams[key] = {}
                self._trigrams[key][w3] = self._trigrams[key].get(w3, 0) + 1

                self._conn.execute("""
                    INSERT INTO trigrams (word1, word2, word3, frequency)
                    VALUES (?, ?, ?, 1)
                    ON CONFLICT(word1, word2, word3)
                    DO UPDATE SET frequency = frequency + 1
                """, (w1, w2, w3))

    def _train_word2vec(self, words: List[str]) -> int:
        """
        Skip-gram Word2Vec —Å Negative Sampling.

        –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ:
          - –ë–µ—Ä—ë–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ (¬±WINDOW_SIZE)
          - –û–±—É—á–∞–µ–º: –≤–µ–∫—Ç–æ—Ä —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å
            –ë–õ–ò–ó–û–ö –∫ –≤–µ–∫—Ç–æ—Ä–∞–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –î–ê–õ–Å–ö –æ—Ç —Å–ª—É—á–∞–π–Ω—ã—Ö —Å–ª–æ–≤

        –í—Å—ë –Ω–∞ —á–∏—Å—Ç–æ–º Python ‚Äî –±–µ–∑ numpy/torch.
        """
        if len(words) < 3 or len(self._word_freq) < 5:
            return 0

        pairs_trained = 0

        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
        vocab_size = len(self._word_freq)
        lr = max(
            MIN_LEARNING_RATE,
            LEARNING_RATE * (1.0 - self._total_words / max(vocab_size * 1000, 1))
        )

        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è negative sampling (unigram distribution^0.75)
        neg_table = self._build_neg_table()
        if not neg_table:
            return 0

        for i, center_word in enumerate(words):
            if center_word in STOP_WORDS and len(center_word) <= 2:
                continue

            center_emb = self._embeddings_cache.get(center_word)
            if not center_emb:
                continue

            # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞
            actual_window = random.randint(1, WINDOW_SIZE)

            for j in range(max(0, i - actual_window), min(len(words), i + actual_window + 1)):
                if i == j:
                    continue

                context_word = words[j]
                if context_word in STOP_WORDS and len(context_word) <= 2:
                    continue

                context_emb = self._embeddings_cache.get(context_word)
                if not context_emb:
                    continue

                # === Positive sample: center + context ===
                dot = sum(a * b for a, b in zip(center_emb, context_emb))
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                dot = max(-6.0, min(6.0, dot))
                sig = _sigmoid(dot)
                grad = lr * (1.0 - sig)

                # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±–∞ –≤–µ–∫—Ç–æ—Ä–∞
                for k in range(EMBEDDING_DIM):
                    old_center = center_emb[k]
                    center_emb[k] += grad * context_emb[k]
                    context_emb[k] += grad * old_center

                # === Negative samples: center + random words ===
                for _ in range(NEGATIVE_SAMPLES):
                    neg_word = neg_table[random.randint(0, len(neg_table) - 1)]
                    if neg_word == center_word or neg_word == context_word:
                        continue

                    neg_emb = self._embeddings_cache.get(neg_word)
                    if not neg_emb:
                        continue

                    dot = sum(a * b for a, b in zip(center_emb, neg_emb))
                    dot = max(-6.0, min(6.0, dot))
                    sig = _sigmoid(dot)
                    neg_grad = lr * sig  # –û—Ç—Ç–∞–ª–∫–∏–≤–∞–µ–º

                    for k in range(EMBEDDING_DIM):
                        center_emb[k] -= neg_grad * neg_emb[k]
                        neg_emb[k] -= neg_grad * center_emb[k]

                pairs_trained += 1

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ –ë–î
        self._save_embeddings(words)

        return pairs_trained

    def _build_neg_table(self, table_size: int = 1000) -> List[str]:
        """
        –°—Ç—Ä–æ–∏—Ç —Ç–∞–±–ª–∏—Ü—É –¥–ª—è negative sampling.
        –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤—ã–±–æ—Ä–∞ —Å–ª–æ–≤–∞ ‚àù freq^0.75
        """
        if not self._word_freq:
            return []

        # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
        words_with_emb = [
            w for w in self._word_freq
            if w in self._embeddings_cache and w not in STOP_WORDS
        ]
        if not words_with_emb:
            return []

        # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞
        total_pow = sum(
            self._word_freq[w] ** 0.75 for w in words_with_emb
        )
        if total_pow == 0:
            return words_with_emb[:table_size]

        table = []
        for word in words_with_emb:
            weight = self._word_freq[word] ** 0.75
            count = max(1, int(weight / total_pow * table_size))
            table.extend([word] * count)

        return table[:table_size * 2]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä

    def _save_embeddings(self, words: List[str]):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ SQLite"""
        now = time.time()
        for word in set(words):
            emb = self._embeddings_cache.get(word)
            if emb:
                self._conn.execute("""
                    UPDATE vocabulary SET embedding = ?, updated_at = ?
                    WHERE word = ?
                """, (json.dumps(emb), now, word))

    def _save_all_embeddings(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –í–°–ï —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–¥–ª—è –º–∏–≥—Ä–∞—Ü–∏–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏)"""
        now = time.time()
        for word, emb in self._embeddings_cache.items():
            self._conn.execute("""
                UPDATE vocabulary SET embedding = ?, updated_at = ?
                WHERE word = ?
            """, (json.dumps(emb), now, word))
        self._conn.commit()

    def _update_associations(self, words: List[str]):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–∏ –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏ –≤ –æ–¥–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏"""
        # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞
        meaningful = [
            w for w in words
            if w not in STOP_WORDS and len(w) > 2
        ]

        for i, w1 in enumerate(meaningful):
            for j in range(i + 1, min(i + 5, len(meaningful))):
                w2 = meaningful[j]
                if w1 == w2:
                    continue

                # –°–∏–ª–∞ –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–∏ —É–±—ã–≤–∞–µ—Ç —Å —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ–º
                distance = j - i
                strength_delta = 1.0 / distance

                self._conn.execute("""
                    INSERT INTO associations (word1, word2, strength)
                    VALUES (?, ?, ?)
                    ON CONFLICT(word1, word2)
                    DO UPDATE SET strength = strength + ?
                """, (w1, w2, strength_delta, strength_delta))

                # –û–±—Ä–∞—Ç–Ω–∞—è –∞—Å—Å–æ—Ü–∏–∞—Ü–∏—è (—Å–ª–∞–±–µ–µ)
                self._conn.execute("""
                    INSERT INTO associations (word1, word2, strength)
                    VALUES (?, ?, ?)
                    ON CONFLICT(word1, word2)
                    DO UPDATE SET strength = strength + ?
                """, (w2, w1, strength_delta * 0.5, strength_delta * 0.5))

    def _update_word_situations(self, words: List[str], situations: List[str]):
        """–ü—Ä–∏–≤—è–∑—ã–≤–∞–µ—Ç —Å–ª–æ–≤–∞ –∫ —Å–∏—Ç—É–∞—Ü–∏—è–º (–≤ –∫–∞–∫–∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è)"""
        meaningful = [w for w in words if w not in STOP_WORDS and len(w) > 2]

        for word in meaningful:
            for situation in situations:
                self._conn.execute("""
                    INSERT INTO word_situations (word, situation, frequency)
                    VALUES (?, ?, 1)
                    ON CONFLICT(word, situation)
                    DO UPDATE SET frequency = frequency + 1
                """, (word, situation))

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    #     –ü–û–ù–ò–ú–ê–ù–ò–ï –°–õ–û–í
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def word_meaning(self, word: str) -> Optional[Dict]:
        """
        –ß—Ç–æ –ö—Ä–∏—Å—Ç–∏–Ω–∞ –∑–Ω–∞–µ—Ç –æ —Å–ª–æ–≤–µ:
        - –≤–µ–∫—Ç–æ—Ä (—ç–º–±–µ–¥–¥–∏–Ω–≥)
        - —á–∞—Å—Ç—å —Ä–µ—á–∏
        - —á–∞—Å—Ç–æ—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        - –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞
        - –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–∏
        - –≤ –∫–∞–∫–∏—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è
        """
        word = word.lower()

        row = self._conn.execute(
            "SELECT * FROM vocabulary WHERE word = ?", (word,)
        ).fetchone()

        if not row:
            return None

        # –ü–æ—Ö–æ–∂–∏–µ –ø–æ —Å–º—ã—Å–ª—É (—á–µ—Ä–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏)
        similar = self.find_similar_words(word, top_n=5)

        # –ê—Å—Å–æ—Ü–∏–∞—Ü–∏–∏
        assoc_rows = self._conn.execute("""
            SELECT word2, strength FROM associations
            WHERE word1 = ?
            ORDER BY strength DESC
            LIMIT 10
        """, (word,)).fetchall()

        # –°–∏—Ç—É–∞—Ü–∏–∏
        sit_rows = self._conn.execute("""
            SELECT situation, frequency FROM word_situations
            WHERE word = ?
            ORDER BY frequency DESC
            LIMIT 5
        """, (word,)).fetchall()

        return {
            "word": word,
            "pos": row["pos"],
            "frequency": row["frequency"],
            "has_embedding": row["embedding"] is not None,
            "similar_words": similar,
            "associations": [
                {"word": r["word2"], "strength": round(r["strength"], 2)}
                for r in assoc_rows
            ],
            "situations": [
                {"situation": r["situation"], "frequency": r["frequency"]}
                for r in sit_rows
            ],
        }

    def find_similar_words(
        self,
        word: str,
        top_n: int = 10,
        pos_filter: str = None,
    ) -> List[Tuple[str, float]]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç —Å–ª–æ–≤–∞ —Å –ø–æ—Ö–æ–∂–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ.

        "–æ—Ç–ª–∏—á–Ω–æ" ‚Üí [("–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ", 0.92), ("–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ", 0.88), ...]
        """
        word = word.lower()
        emb = self._embeddings_cache.get(word)
        if not emb:
            return []

        similarities = []

        for other_word, other_emb in self._embeddings_cache.items():
            if other_word == word:
                continue
            if pos_filter:
                row = self._conn.execute(
                    "SELECT pos FROM vocabulary WHERE word = ?",
                    (other_word,)
                ).fetchone()
                if row and row["pos"] != pos_filter:
                    continue

            sim = _cosine_similarity(emb, other_emb)
            similarities.append((other_word, sim))

        similarities.sort(key=lambda x: x[1], reverse=True)
        return [(w, round(s, 3)) for w, s in similarities[:top_n]]

    def understand_sentence(self, text: str) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞,
        –æ–±—â–∏–π —Å–º—ã—Å–ª, –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞.
        """
        tokens = self.tokenize(text)
        words = [t for t in tokens if re.match(r'[–∞-—è—ëa-z]', t)]

        analysis = {
            "tokens": tokens,
            "words": [],
            "unknown_words": [],
            "understood_pct": 0.0,
        }

        known = 0
        for word in words:
            info = self._conn.execute(
                "SELECT pos, frequency FROM vocabulary WHERE word = ?",
                (word,)
            ).fetchone()

            if info:
                known += 1
                analysis["words"].append({
                    "word": word,
                    "pos": info["pos"],
                    "frequency": info["frequency"],
                    "known": True,
                })
            else:
                analysis["unknown_words"].append(word)
                analysis["words"].append({
                    "word": word,
                    "pos": self._guess_pos(word),
                    "frequency": 0,
                    "known": False,
                })

        if words:
            analysis["understood_pct"] = round(known / len(words) * 100, 1)

        return analysis

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    #     –ì–ï–ù–ï–†–ê–¶–ò–Ø –ü–†–ï–î–õ–û–ñ–ï–ù–ò–ô
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def generate_sentence(
        self,
        situation: str = None,
        seed_word: str = None,
        mood: str = "neutral",
        max_len: int = MAX_SENTENCE_LEN,
        creativity: float = 0.3,
    ) -> Optional[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ù–û–í–û–ï –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ.

        –ê–ª–≥–æ—Ä–∏—Ç–º:
        1. –í—ã–±–∏—Ä–∞–µ–º seed-—Å–ª–æ–≤–æ (–∏–∑ —Å–∏—Ç—É–∞—Ü–∏–∏ –∏–ª–∏ —Å–ª—É—á–∞–π–Ω–æ–µ)
        2. –°—Ç—Ä–æ–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å–ª–æ–≤–æ-–∑–∞-—Å–ª–æ–≤–æ–º:
           a. –ë–µ—Ä—ë–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∏–∑ n-gram –º–æ–¥–µ–ª–∏ (—Ç—Ä–∏–≥—Ä–∞–º–º—ã > –±–∏–≥—Ä–∞–º–º—ã)
           b. –†–∞–Ω–∂–∏—Ä—É–µ–º –ø–æ: n-gram —á–∞—Å—Ç–æ—Ç–∞ + embedding —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
           c. –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å (creativity) –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
        3. –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ –∑–Ω–∞–∫–µ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏–ª–∏ max_len

        Args:
            situation: —Ç–∏–ø —Å–∏—Ç—É–∞—Ü–∏–∏ ("greeting", "offer_help", ...)
            seed_word: –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ (–µ—Å–ª–∏ None ‚Äî –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –∏–∑ —Å–∏—Ç—É–∞—Ü–∏–∏)
            mood: –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤–ª–∏—è–µ—Ç –Ω–∞ –≤—ã–±–æ—Ä —Å–ª–æ–≤
            max_len: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
            creativity: 0.0=—Å—Ç—Ä–æ–≥–æ –ø–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ, 1.0=–±–æ–ª—å—à–µ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏

        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö.
        """
        if len(self._bigrams) < 10:
            return None  # –°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö

        # 1. –í—ã–±–∏—Ä–∞–µ–º seed-—Å–ª–æ–≤–æ
        start_word = self._choose_seed(situation, seed_word, mood)
        if not start_word:
            return None

        # 2. –°—Ç—Ä–æ–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
        sentence = [start_word]
        prev_word = "<S>"
        curr_word = start_word

        for step in range(max_len - 1):
            next_word = self._predict_next(
                prev_word, curr_word, sentence,
                situation=situation,
                creativity=creativity,
            )

            if not next_word or next_word == "</S>":
                break

            # –ü—É–Ω–∫—Ç—É–∞—Ü–∏—è
            if next_word in ".!?,;:‚Äî":
                sentence.append(next_word)
                if next_word in ".!?":
                    break
                continue

            sentence.append(next_word)
            prev_word = curr_word
            curr_word = next_word

        # 3. –§–∏–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º
        if len(sentence) < MIN_SENTENCE_LEN:
            return None

        result = self._format_sentence(sentence)
        return result

    def generate_response(
        self,
        situations: List[str],
        mood: str = "neutral",
        max_sentences: int = 2,
        creativity: float = 0.3,
    ) -> Optional[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.

        –î–ª—è –∫–∞–∂–¥–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ,
        –ø–æ—Ç–æ–º —Å–æ–µ–¥–∏–Ω—è–µ—Ç –≤ —Å–≤—è–∑–Ω—ã–π –æ—Ç–≤–µ—Ç.
        """
        if len(self._bigrams) < 20:
            return None

        parts = []

        for situation in situations[:max_sentences]:
            sentence = self.generate_sentence(
                situation=situation,
                mood=mood,
                creativity=creativity,
            )
            if sentence:
                parts.append(sentence)

        if not parts:
            return None

        return " ".join(parts)

    def _choose_seed(
        self,
        situation: str = None,
        seed_word: str = None,
        mood: str = "neutral",
    ) -> Optional[str]:
        """–í—ã–±–∏—Ä–∞–µ—Ç –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""

        if seed_word and seed_word in self._word_freq:
            return seed_word

        # –ò–∑ —Å–∏—Ç—É–∞—Ü–∏–∏ ‚Äî –±–µ—Ä—ë–º —Å–ª–æ–≤–∞ –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã–µ –∫ —Å–∏—Ç—É–∞—Ü–∏–∏
        if situation:
            # –°–Ω–∞—á–∞–ª–∞ –∏–∑ –≤—ã—É—á–µ–Ω–Ω—ã—Ö word_situations
            rows = self._conn.execute("""
                SELECT word, frequency FROM word_situations
                WHERE situation = ?
                ORDER BY frequency DESC
                LIMIT 20
            """, (situation,)).fetchall()

            candidates = [
                r["word"] for r in rows
                if r["word"] in self._bigrams  # –°–ª–æ–≤–æ –¥–æ–ª–∂–Ω–æ –∏–º–µ—Ç—å –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è
            ]

            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ seed-—Å–ª–æ–≤–∞
            static_seeds = SITUATION_SEEDS.get(situation, [])
            for sw in static_seeds:
                if sw in self._word_freq and sw not in candidates:
                    candidates.append(sw)

            if candidates:
                # –°–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—â–µ –Ω–∞—á–∏–Ω–∞—é—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                start_candidates = []
                for c in candidates:
                    bigram_freq = self._bigrams.get("<S>", {}).get(c, 0)
                    if bigram_freq > 0:
                        start_candidates.append((c, bigram_freq))

                if start_candidates:
                    # –í–∑–≤–µ—à–µ–Ω–Ω—ã–π –≤—ã–±–æ—Ä
                    total = sum(f for _, f in start_candidates)
                    r = random.random() * total
                    cumulative = 0
                    for word, freq in start_candidates:
                        cumulative += freq
                        if r <= cumulative:
                            return word

                # –ï—Å–ª–∏ –Ω–µ—Ç —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö ‚Äî –ø—Ä–æ—Å—Ç–æ —Å–ª—É—á–∞–π–Ω—ã–π –∏–∑ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
                return random.choice(candidates)

        # Fallback: —Å–ª—É—á–∞–π–Ω–æ–µ —Å–ª–æ–≤–æ, –∫–æ—Ç–æ—Ä–æ–µ –Ω–∞—á–∏–Ω–∞–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        starters = self._bigrams.get("<S>", {})
        if starters:
            words = list(starters.keys())
            freqs = list(starters.values())
            total = sum(freqs)
            r = random.random() * total
            cumulative = 0
            for word, freq in zip(words, freqs):
                cumulative += freq
                if r <= cumulative:
                    if word not in ("</S>",) and re.match(r'[–∞-—è—ëa-z]', word):
                        return word

        return None

    def _predict_next(
        self,
        prev_word: str,
        curr_word: str,
        sentence: List[str],
        situation: str = None,
        creativity: float = 0.3,
    ) -> Optional[str]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ.

        –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç:
        1. –¢—Ä–∏–≥—Ä–∞–º–º—ã (–µ—Å–ª–∏ –µ—Å—Ç—å) ‚Äî —Å–∞–º—ã–µ —Ç–æ—á–Ω—ã–µ
        2. –ë–∏–≥—Ä–∞–º–º—ã ‚Äî –æ—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫
        3. Embedding similarity ‚Äî —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å–≤—è–∑–Ω–æ—Å—Ç—å
        4. –°–∏—Ç—É–∞—Ü–∏–æ–Ω–Ω—ã–π –±–æ–Ω—É—Å ‚Äî —Å–ª–æ–≤–∞ –∏–∑ –Ω—É–∂–Ω–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏
        """
        candidates: Dict[str, float] = {}

        # 1. –¢—Ä–∏–≥—Ä–∞–º–º—ã: (prev, curr) ‚Üí next
        trigram_key = (prev_word, curr_word)
        if trigram_key in self._trigrams:
            tri_total = sum(self._trigrams[trigram_key].values())
            for word, freq in self._trigrams[trigram_key].items():
                candidates[word] = candidates.get(word, 0) + (freq / tri_total) * 3.0

        # 2. –ë–∏–≥—Ä–∞–º–º—ã: curr ‚Üí next
        if curr_word in self._bigrams:
            bi_total = sum(self._bigrams[curr_word].values())
            for word, freq in self._bigrams[curr_word].items():
                candidates[word] = candidates.get(word, 0) + (freq / bi_total) * 1.0

        if not candidates:
            return None

        # 3. Embedding similarity –±–æ–Ω—É—Å
        curr_emb = self._embeddings_cache.get(curr_word)
        if curr_emb:
            for word in list(candidates.keys()):
                word_emb = self._embeddings_cache.get(word)
                if word_emb:
                    sim = _cosine_similarity(curr_emb, word_emb)
                    # –ù–µ–±–æ–ª—å—à–æ–π –±–æ–Ω—É—Å –∑–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–ª–∏–∑–æ—Å—Ç—å
                    candidates[word] += max(0, sim) * 0.3

        # 4. –°–∏—Ç—É–∞—Ü–∏–æ–Ω–Ω—ã–π –±–æ–Ω—É—Å
        if situation:
            sit_words = set()
            rows = self._conn.execute("""
                SELECT word FROM word_situations
                WHERE situation = ?
                AND frequency >= 2
            """, (situation,)).fetchall()
            sit_words = {r["word"] for r in rows}

            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ seed-—Å–ª–æ–≤–∞
            static = SITUATION_SEEDS.get(situation, [])
            sit_words.update(static)

            for word in candidates:
                if word in sit_words:
                    candidates[word] *= 1.5

        # 5. –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ —Å–ª–æ–≤ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏
        for word in candidates:
            if word in sentence and word not in STOP_WORDS:
                candidates[word] *= 0.1

        # 6. –®—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (—Å—Ç–∏–º—É–ª–∏—Ä—É–µ–º –∫–æ–Ω–µ—Ü)
        if len(sentence) > 8:
            for word in candidates:
                if word in (".!?", "</S>"):
                    candidates[word] *= 1.5 + (len(sentence) - 8) * 0.3

        # 7. –í—ã–±–æ—Ä —Å —É—á—ë—Ç–æ–º creativity
        if not candidates:
            return None

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        total = sum(max(0, s) for s in candidates.values())
        if total <= 0:
            return None

        # Temperature sampling
        temperature = 0.5 + creativity  # 0.5-1.5
        scored = []
        for word, score in candidates.items():
            if score > 0:
                adjusted = (score / total) ** (1.0 / temperature)
                scored.append((word, adjusted))

        adj_total = sum(s for _, s in scored)
        if adj_total <= 0:
            return None

        r = random.random() * adj_total
        cumulative = 0
        for word, score in scored:
            cumulative += score
            if r <= cumulative:
                return word

        return scored[0][0] if scored else None

    def _format_sentence(self, tokens: List[str]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω—ã –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ"""
        if not tokens:
            return ""

        # –ü–µ—Ä–≤–∞—è –±—É–∫–≤–∞ ‚Äî –∑–∞–≥–ª–∞–≤–Ω–∞—è
        result = []
        for i, token in enumerate(tokens):
            if token in ".!?,;:‚Äî":
                # –ü—É–Ω–∫—Ç—É–∞—Ü–∏—è –±–µ–∑ –ø—Ä–æ–±–µ–ª–∞ –ø–µ—Ä–µ–¥ –Ω–µ–π
                if result:
                    result[-1] = result[-1] + token
                else:
                    result.append(token)
            else:
                if i == 0:
                    result.append(token.capitalize())
                else:
                    result.append(token)

        text = " ".join(result)

        # –ï—Å–ª–∏ –Ω–µ—Ç –∑–∞–≤–µ—Ä—à–∞—é—â–µ–≥–æ –∑–Ω–∞–∫–∞ ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º
        if text and text[-1] not in ".!?":
            text += "."

        return text

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    #     –£–¢–ò–õ–ò–¢–´
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def get_vocabulary_size(self) -> int:
        return len(self._word_freq)

    def get_stats(self) -> Dict[str, int]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–µ–π—Ä–æ–Ω–Ω–æ–≥–æ –¥–≤–∏–∂–∫–∞"""
        vocabulary = self._conn.execute(
            "SELECT COUNT(*) as c FROM vocabulary"
        ).fetchone()["c"]

        embeddings = len(self._embeddings_cache)

        bigrams = self._conn.execute(
            "SELECT COUNT(*) as c FROM bigrams"
        ).fetchone()["c"]

        trigrams = self._conn.execute(
            "SELECT COUNT(*) as c FROM trigrams"
        ).fetchone()["c"]

        associations = self._conn.execute(
            "SELECT COUNT(*) as c FROM associations"
        ).fetchone()["c"]

        training_steps = self._conn.execute(
            "SELECT COUNT(*) as c FROM training_log"
        ).fetchone()["c"]

        word_situations = self._conn.execute(
            "SELECT COUNT(*) as c FROM word_situations"
        ).fetchone()["c"]

        return {
            "vocabulary": vocabulary,
            "embeddings": embeddings,
            "bigrams": bigrams,
            "trigrams": trigrams,
            "associations": associations,
            "word_situations": word_situations,
            "training_steps": training_steps,
            "total_words_processed": self._total_words,
        }

    def close(self):
        self._conn.commit()
        self._conn.close()
