# ПЛАН: Безлимитная память диалога в контексте

## Проблема
Сейчас Кристина "забывает" начало разговора через 3-15 сообщений.
MicroTransformer видит максимум 256 токенов. Контекст для LLM строится
из последних 3 сообщений треда + 2 результатов из ChromaDB.
Для пользователя это ощущается как потеря памяти внутри одного разговора.

## Цель
Безлимитная память в рамках диалога: Кристина помнит ВСЁ, что было сказано
в текущей сессии, и может ссылаться на любой момент разговора.

---

## АРХИТЕКТУРА: 4 уровня решения

### Уровень 1: Скользящее резюме диалога (Sliding Summary)
**Файл:** `python/core/dialogue_memory.py` (новый)
**Сложность:** Средняя | **Эффект:** Высокий

**Суть:** После каждых N сообщений (например, 6) — автоматически
сжимать старую часть диалога в краткое резюме. Резюме "катится"
вперёд, накапливая контекст.

**Как работает:**
```
Сообщения 1-6:   [полный текст]
Сообщения 7-12:  [резюме 1-6] + [полный текст 7-12]
Сообщения 13-18: [резюме 1-12] + [полный текст 13-18]
...
Сообщение 100:   [резюме 1-94] + [полный текст 95-100]
```

Резюме генерируется через:
- Приоритет A: локальную LLM (gemma3:4b) — качественно
- Приоритет B: extractive summarization (без LLM) — берём ключевые
  фразы по TF-IDF + именованные сущности + числа + решения

**Структура данных:**
```python
class SlidingSummary:
    summary_text: str          # Накопленное резюме
    summary_tokens: int        # Размер резюме в токенах
    key_facts: List[str]       # Извлечённые факты (имена, числа, решения)
    topic_history: List[str]   # Смена тем в разговоре
    full_messages: List[Dict]  # Последние N сообщений (полный текст)
    total_messages: int        # Счётчик всех сообщений сессии
```

**Бюджет токенов:**
- Резюме: макс 500 токенов (растёт логарифмически, не линейно)
- Полный текст: последние 6 сообщений (~600 токенов)
- Итого: ~1100 токенов на историю (вместо текущих ~200)

### Уровень 2: Индексированный архив сессии (Session Index)
**Файл:** `python/core/dialogue_memory.py` (тот же)
**Сложность:** Средняя | **Эффект:** Высокий

**Суть:** Каждое сообщение сессии сохраняется с эмбеддингом в
быстрый in-memory индекс. Когда пользователь ссылается на
что-то из прошлого ("как я говорил раньше", "вернёмся к тому
вопросу"), система находит релевантное сообщение по семантике.

**Как работает:**
```python
class SessionIndex:
    messages: List[SessionMessage]  # Все сообщения сессии
    embeddings: np.ndarray          # Матрица эмбеддингов [N × 128]

    def add(self, role, text):
        vec = sentence_embeddings.encode(text)
        self.messages.append(SessionMessage(role, text, vec, time))
        self.embeddings = np.vstack([self.embeddings, vec])

    def search(self, query, top_k=3) -> List[SessionMessage]:
        q_vec = sentence_embeddings.encode(query)
        scores = cosine_similarity(q_vec, self.embeddings)
        return top_k_messages(scores)
```

**Триггеры поиска по архиву:**
- Анафорические ссылки: "это", "тот", "как раньше", "вернёмся"
- Низкий скор из текущего контекста (непонятно о чём речь)
- Явные ссылки: "в начале разговора", "когда мы обсуждали X"

**Лимит:** Нет. Все сообщения сессии хранятся в RAM.
При 1000 сообщений × 128 dim = ~500 KB — ничтожно.

### Уровень 3: Расширение контекстного окна MicroTransformer
**Файл:** `python/core/micro_transformer.py` (изменение)
**Сложность:** Средняя | **Эффект:** Средний

**Суть:** Увеличить MAX_SEQ_LEN с 256 до 512-1024 токенов.
RoPE (Rotary Position Embeddings) масштабируется без переобучения,
поэтому можно увеличить окно и получить лучшее понимание контекста.

**Изменения:**
```python
# Было:
MAX_SEQ_LEN = 256

# Стало:
MAX_SEQ_LEN = 512  # Этап 1
# или
MAX_SEQ_LEN = 1024  # Этап 2 (после проверки скорости)
```

**Дополнительно — ALiBi (Attention with Linear Biases):**
Добавить ALiBi как альтернативу RoPE для лучшей экстраполяции
за пределы обученной длины. ALiBi позволяет модели, обученной
на 512 токенах, работать с 1024+ без деградации.

**Оценка скорости:**
- 256 → 512: замедление ~2-3x (attention O(n²))
- 512 → 1024: замедление ~4x от базовой
- На CPU это: ~50ms → ~150ms (512) или ~200ms (1024) — приемлемо

### Уровень 4: Улучшение построения контекста в оркестраторе
**Файл:** `python/core/orchestrator.py` (изменение)
**Сложность:** Низкая | **Эффект:** Высокий

**Суть:** Переписать `_build_context()` чтобы использовать
все новые источники памяти.

**Новый `_build_context()`:**
```python
def _build_context(self, user_input: str) -> str:
    parts = []

    # 1. Скользящее резюме сессии (Уровень 1)
    if self.dialogue_memory.summary_text:
        parts.append(f"[Контекст сессии]: {self.dialogue_memory.summary_text}")

    # 2. Релевантные сообщения из архива сессии (Уровень 2)
    relevant = self.dialogue_memory.search(user_input, top_k=3)
    if relevant:
        parts.append("[Из ранее в разговоре]:")
        for msg in relevant:
            parts.append(f"  {msg.role}: {msg.text[:200]}")

    # 3. Последние сообщения (полный текст)
    for msg in self.dialogue_memory.recent_messages(n=6):
        parts.append(f"{msg.role}: {msg.text}")

    # 4. Долгосрочная память (ChromaDB) — если тема новая
    if not relevant or max_score < 0.7:
        long_term = await self.vector_memory.search_async(user_input, n=3)
        for doc in long_term:
            parts.append(f"[Из памяти]: {doc}")

    # 5. Cross-attention обогащение
    enriched = self.memory_attention.enrich(user_input)
    if enriched.gate > 0.3:
        parts.append(f"[Ассоциация]: {enriched.text}")

    return "\n".join(parts)
```

**Бюджет итогового контекста:**
```
Скользящее резюме:     ~500 токенов
Релевантные из архива:  ~300 токенов (3 × 100)
Последние 6 сообщений: ~600 токенов
Долгосрочная память:    ~300 токенов (3 × 100)
Cross-attention:        ~100 токенов
─────────────────────────────────────
ИТОГО:                 ~1800 токенов (из 16384 доступных)
```

Это в 6-9 раз больше контекста чем сейчас (~200-300 токенов),
при этом всего 11% от доступного окна LLM.

---

## ИЗМЕНЕНИЯ В СУЩЕСТВУЮЩИХ ФАЙЛАХ

### config.py — новые параметры:
```python
# Dialogue Memory
sliding_summary_window: int = 6         # Сообщений до сжатия
sliding_summary_max_tokens: int = 500   # Макс размер резюме
session_search_top_k: int = 3           # Результатов из архива сессии
session_search_threshold: float = 0.5   # Мин. косинусное сходство
context_budget_summary: int = 500       # Бюджет токенов на резюме
context_budget_recent: int = 600        # Бюджет на последние сообщения
context_budget_search: int = 300        # Бюджет на поиск по архиву
context_budget_longterm: int = 300      # Бюджет на долгосрочную память
```

### orchestrator.py — интеграция:
- Импорт `DialogueMemory` из `dialogue_memory.py`
- Инициализация в `__init__`: `self.dialogue_memory = DialogueMemory(config)`
- Замена `_build_context()` на новую версию (Уровень 4)
- В `_save_to_memory()`: добавить `self.dialogue_memory.add(role, text)`
- Убрать жёсткую зависимость от `thread_tracker` для контекста
  (thread_tracker остаётся для определения смены темы)

### micro_transformer.py — расширение окна:
- `MAX_SEQ_LEN: 256 → 512`
- Добавить параметр в config: `transformer_max_seq_len: int = 512`
- Опционально: реализация ALiBi для экстраполяции

### thread_tracker.py — адаптация:
- Убрать жёсткий timeout 600s → сделать адаптивным
- Тема меняется по СЕМАНТИКЕ (если новое сообщение далеко от текущей темы),
  а не по таймеру
- Оставить timeout как fallback (30 минут вместо 10)

---

## ПОРЯДОК РЕАЛИЗАЦИИ

```
Шаг 1: Создать dialogue_memory.py
       ├── Класс SlidingSummary (резюмирование)
       ├── Класс SessionIndex (поиск по сессии)
       └── Класс DialogueMemory (объединяющий фасад)

Шаг 2: Добавить параметры в config.py

Шаг 3: Интегрировать в orchestrator.py
       ├── Инициализация DialogueMemory
       ├── Новый _build_context()
       └── Сохранение в _save_to_memory()

Шаг 4: Расширить micro_transformer.py
       └── MAX_SEQ_LEN 256 → 512

Шаг 5: Адаптировать thread_tracker.py
       └── Семантический timeout вместо временного

Шаг 6: Тестирование
       └── Длинный диалог (50+ сообщений) с проверкой
           что Кристина помнит начало разговора
```

---

## РЕЗУЛЬТАТ

**До:**
- Кристина помнит 3 последних сообщения в контексте
- Через 10 минут — принудительный сброс темы
- MicroTransformer видит 256 токенов

**После:**
- Скользящее резюме ВСЕЙ сессии (безлимитно)
- Семантический поиск по ЛЮБОМУ моменту разговора
- Адаптивная смена темы по смыслу, а не по таймеру
- MicroTransformer видит 512 токенов
- Контекст для LLM: ~1800 токенов (было ~200-300)

**Память безлимитна** потому что:
1. Резюме растёт ЛОГАРИФМИЧЕСКИ (не линейно) — 10 сообщений и 1000
   сообщений дают резюме примерно одного размера
2. Все сообщения хранятся в RAM-индексе с эмбеддингами — поиск O(N)
   по косинусному сходству, при 1000 сообщений < 1ms
3. ChromaDB хранит ВСЁ на диске — безлимитная долгосрочная память
4. Иерархическая суммаризация (уже есть) сжимает дни → недели → месяцы
